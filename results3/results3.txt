trainingset size: 100000
validationset size: 10000
input_dims:  (64, 64, 3)
nb_labels:  200
nb_labels: 200
x_train.shape  (100000, 64, 64, 3)
y_train.shape  (100000, 200)
x_test.shape  (10000, 64, 64, 3)
y_test.shape  (10000, 200)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 58, 62, 48)        3072      
_________________________________________________________________
activation_1 (Activation)    (None, 58, 62, 48)        0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 28, 30, 48)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 24, 30, 128)       30848     
_________________________________________________________________
activation_2 (Activation)    (None, 24, 30, 128)       0         
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 11, 14, 128)       0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 9, 14, 192)        73920     
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 7, 14, 192)        110784    
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 5, 14, 128)        73856     
_________________________________________________________________
activation_3 (Activation)    (None, 5, 14, 128)        0         
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 2, 6, 128)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 1536)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 2048)              3147776   
_________________________________________________________________
dense_2 (Dense)              (None, 2048)              4196352   
_________________________________________________________________
dense_3 (Dense)              (None, 200)               409800    
=================================================================
Total params: 8,046,408
Trainable params: 8,046,408
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
 - 37s - loss: 5.0035 - acc: 0.0230 - val_loss: 4.7289 - val_acc: 0.0462
Epoch 2/100
 - 32s - loss: 4.5344 - acc: 0.0716 - val_loss: 4.3762 - val_acc: 0.0876
Epoch 3/100
 - 32s - loss: 4.2680 - acc: 0.1020 - val_loss: 4.1885 - val_acc: 0.1177
Epoch 4/100
 - 32s - loss: 4.0805 - acc: 0.1269 - val_loss: 4.1175 - val_acc: 0.1189
Epoch 5/100
 - 32s - loss: 3.9420 - acc: 0.1457 - val_loss: 3.9670 - val_acc: 0.1414
Epoch 6/100
 - 32s - loss: 3.8298 - acc: 0.1619 - val_loss: 3.8432 - val_acc: 0.1613
Epoch 7/100
 - 32s - loss: 3.7289 - acc: 0.1764 - val_loss: 3.8707 - val_acc: 0.1633
Epoch 8/100
 - 32s - loss: 3.6431 - acc: 0.1902 - val_loss: 3.7892 - val_acc: 0.1761
Epoch 9/100
 - 32s - loss: 3.5772 - acc: 0.1999 - val_loss: 3.8227 - val_acc: 0.1678
Epoch 10/100
 - 32s - loss: 3.5041 - acc: 0.2120 - val_loss: 3.7775 - val_acc: 0.1747
Epoch 11/100
 - 32s - loss: 3.4436 - acc: 0.2192 - val_loss: 3.8521 - val_acc: 0.1741
Epoch 12/100
 - 32s - loss: 3.3823 - acc: 0.2290 - val_loss: 3.7520 - val_acc: 0.1827
Epoch 13/100
 - 32s - loss: 3.3306 - acc: 0.2360 - val_loss: 3.7240 - val_acc: 0.1943
Epoch 14/100
 - 32s - loss: 3.2765 - acc: 0.2444 - val_loss: 3.7806 - val_acc: 0.1913
Epoch 15/100
 - 32s - loss: 3.2197 - acc: 0.2554 - val_loss: 3.7350 - val_acc: 0.1943
Epoch 16/100
 - 32s - loss: 3.1669 - acc: 0.2635 - val_loss: 3.7906 - val_acc: 0.1941
Epoch 17/100
 - 32s - loss: 3.1204 - acc: 0.2711 - val_loss: 3.7939 - val_acc: 0.1965
Epoch 18/100
 - 32s - loss: 3.0670 - acc: 0.2805 - val_loss: 3.8309 - val_acc: 0.1887
Epoch 19/100
 - 32s - loss: 3.0215 - acc: 0.2876 - val_loss: 3.8265 - val_acc: 0.1904
Epoch 20/100
 - 32s - loss: 2.9617 - acc: 0.2981 - val_loss: 3.8051 - val_acc: 0.1961
Epoch 21/100
 - 32s - loss: 2.9178 - acc: 0.3049 - val_loss: 3.9514 - val_acc: 0.1913
Epoch 22/100
 - 32s - loss: 2.8628 - acc: 0.3151 - val_loss: 3.9743 - val_acc: 0.1865
Epoch 23/100
 - 32s - loss: 2.8081 - acc: 0.3238 - val_loss: 4.0201 - val_acc: 0.1890
Epoch 24/100
 - 32s - loss: 2.7692 - acc: 0.3323 - val_loss: 4.0275 - val_acc: 0.1855
Epoch 25/100
 - 32s - loss: 2.7150 - acc: 0.3401 - val_loss: 4.1112 - val_acc: 0.1831
Epoch 26/100
 - 32s - loss: 2.6680 - acc: 0.3501 - val_loss: 4.1015 - val_acc: 0.1886
Epoch 27/100
 - 32s - loss: 2.6217 - acc: 0.3589 - val_loss: 4.2536 - val_acc: 0.1807
Epoch 28/100
 - 32s - loss: 2.5656 - acc: 0.3675 - val_loss: 4.1790 - val_acc: 0.1889
Epoch 29/100
 - 32s - loss: 2.5209 - acc: 0.3776 - val_loss: 4.3606 - val_acc: 0.1766
Epoch 30/100
 - 32s - loss: 2.4670 - acc: 0.3886 - val_loss: 4.3450 - val_acc: 0.1895
Epoch 31/100
 - 32s - loss: 2.4337 - acc: 0.3940 - val_loss: 4.3473 - val_acc: 0.1850
Epoch 32/100
 - 32s - loss: 2.3809 - acc: 0.4062 - val_loss: 4.4938 - val_acc: 0.1778
Epoch 33/100
 - 32s - loss: 2.3384 - acc: 0.4132 - val_loss: 4.6480 - val_acc: 0.1745
Epoch 34/100
 - 32s - loss: 2.2948 - acc: 0.4217 - val_loss: 4.5706 - val_acc: 0.1793
Epoch 35/100
 - 32s - loss: 2.2481 - acc: 0.4296 - val_loss: 4.6704 - val_acc: 0.1776
Epoch 36/100
 - 32s - loss: 2.2096 - acc: 0.4396 - val_loss: 4.7112 - val_acc: 0.1762
Epoch 37/100
 - 32s - loss: 2.1590 - acc: 0.4476 - val_loss: 4.7609 - val_acc: 0.1756
Epoch 38/100
 - 32s - loss: 2.1022 - acc: 0.4611 - val_loss: 4.8172 - val_acc: 0.1719
Epoch 39/100
 - 32s - loss: 2.0671 - acc: 0.4693 - val_loss: 4.9519 - val_acc: 0.1706
Epoch 40/100
 - 32s - loss: 2.0350 - acc: 0.4774 - val_loss: 5.0481 - val_acc: 0.1759
Epoch 41/100
 - 32s - loss: 1.9886 - acc: 0.4865 - val_loss: 5.0444 - val_acc: 0.1700
Epoch 42/100
 - 32s - loss: 1.9469 - acc: 0.4934 - val_loss: 5.0704 - val_acc: 0.1707
Epoch 43/100
 - 32s - loss: 1.9154 - acc: 0.5008 - val_loss: 5.2406 - val_acc: 0.1698
Epoch 44/100
 - 32s - loss: 1.8761 - acc: 0.5090 - val_loss: 5.2323 - val_acc: 0.1680
Epoch 45/100
 - 32s - loss: 1.8346 - acc: 0.5181 - val_loss: 5.3852 - val_acc: 0.1618
Epoch 46/100
 - 32s - loss: 1.8077 - acc: 0.5227 - val_loss: 5.4945 - val_acc: 0.1685
Epoch 47/100
 - 32s - loss: 1.7709 - acc: 0.5324 - val_loss: 5.5852 - val_acc: 0.1679
Epoch 48/100
 - 32s - loss: 1.7292 - acc: 0.5389 - val_loss: 5.4467 - val_acc: 0.1625
Epoch 49/100
 - 32s - loss: 1.6991 - acc: 0.5480 - val_loss: 5.8110 - val_acc: 0.1611
Epoch 50/100
 - 32s - loss: 1.6739 - acc: 0.5545 - val_loss: 5.6950 - val_acc: 0.1622
Epoch 51/100
 - 32s - loss: 1.6471 - acc: 0.5603 - val_loss: 5.6848 - val_acc: 0.1597
Epoch 52/100
 - 32s - loss: 1.6049 - acc: 0.5701 - val_loss: 5.9848 - val_acc: 0.1588
Epoch 53/100
 - 32s - loss: 1.5887 - acc: 0.5726 - val_loss: 5.9133 - val_acc: 0.1619
Epoch 54/100
 - 32s - loss: 1.5498 - acc: 0.5836 - val_loss: 6.0833 - val_acc: 0.1597
Epoch 55/100
 - 32s - loss: 1.5259 - acc: 0.5879 - val_loss: 6.1421 - val_acc: 0.1628
Epoch 56/100
 - 32s - loss: 1.4953 - acc: 0.5946 - val_loss: 6.1018 - val_acc: 0.1557
Epoch 57/100
 - 32s - loss: 1.4670 - acc: 0.5994 - val_loss: 6.2852 - val_acc: 0.1617
Epoch 58/100
 - 32s - loss: 1.4433 - acc: 0.6075 - val_loss: 6.2255 - val_acc: 0.1556
Epoch 59/100
 - 32s - loss: 1.4305 - acc: 0.6099 - val_loss: 6.2912 - val_acc: 0.1599
Epoch 60/100
 - 32s - loss: 1.4031 - acc: 0.6159 - val_loss: 6.3751 - val_acc: 0.1528
Epoch 61/100
 - 32s - loss: 1.3575 - acc: 0.6290 - val_loss: 6.4900 - val_acc: 0.1545
Epoch 62/100
 - 32s - loss: 1.3769 - acc: 0.6217 - val_loss: 6.4690 - val_acc: 0.1598
Epoch 63/100
 - 32s - loss: 1.3256 - acc: 0.6341 - val_loss: 6.8637 - val_acc: 0.1520
Epoch 64/100
 - 32s - loss: 1.3225 - acc: 0.6352 - val_loss: 6.6921 - val_acc: 0.1540
Epoch 65/100
 - 32s - loss: 1.2900 - acc: 0.6424 - val_loss: 6.7676 - val_acc: 0.1509
Epoch 66/100
 - 32s - loss: 1.2713 - acc: 0.6461 - val_loss: 6.5441 - val_acc: 0.1582
Epoch 67/100
 - 32s - loss: 1.2612 - acc: 0.6516 - val_loss: 6.9191 - val_acc: 0.1530
Epoch 68/100
 - 32s - loss: 1.2283 - acc: 0.6592 - val_loss: 6.9254 - val_acc: 0.1606
Epoch 69/100
 - 32s - loss: 1.2224 - acc: 0.6605 - val_loss: 6.8780 - val_acc: 0.1500
Epoch 70/100
 - 32s - loss: 1.2086 - acc: 0.6633 - val_loss: 7.1720 - val_acc: 0.1557
Epoch 71/100
 - 32s - loss: 1.1931 - acc: 0.6661 - val_loss: 7.0388 - val_acc: 0.1491
Epoch 72/100
 - 32s - loss: 1.1771 - acc: 0.6705 - val_loss: 7.2024 - val_acc: 0.1511
Epoch 73/100
 - 32s - loss: 1.1744 - acc: 0.6712 - val_loss: 7.2629 - val_acc: 0.1525
Epoch 74/100
 - 32s - loss: 1.1492 - acc: 0.6789 - val_loss: 7.2175 - val_acc: 0.1566
Epoch 75/100
 - 32s - loss: 1.1230 - acc: 0.6849 - val_loss: 7.2671 - val_acc: 0.1539
Epoch 76/100
 - 32s - loss: 1.1442 - acc: 0.6794 - val_loss: 7.3389 - val_acc: 0.1544
Epoch 77/100
 - 32s - loss: 1.0988 - acc: 0.6882 - val_loss: 7.4716 - val_acc: 0.1572
Epoch 78/100
 - 32s - loss: 1.1046 - acc: 0.6909 - val_loss: 7.3260 - val_acc: 0.1489
Epoch 79/100
 - 32s - loss: 1.0754 - acc: 0.6976 - val_loss: 7.4343 - val_acc: 0.1480
Epoch 80/100
 - 32s - loss: 1.0731 - acc: 0.6985 - val_loss: 7.4560 - val_acc: 0.1498
Epoch 81/100
 - 32s - loss: 1.0611 - acc: 0.7026 - val_loss: 7.6146 - val_acc: 0.1479
Epoch 82/100
 - 32s - loss: 1.0539 - acc: 0.7041 - val_loss: 7.5330 - val_acc: 0.1482
Epoch 83/100
 - 32s - loss: 1.0354 - acc: 0.7077 - val_loss: 7.7251 - val_acc: 0.1571
Epoch 84/100
 - 32s - loss: 1.0358 - acc: 0.7076 - val_loss: 7.8137 - val_acc: 0.1497
Epoch 85/100
 - 32s - loss: 1.0091 - acc: 0.7147 - val_loss: 7.5570 - val_acc: 0.1446
Epoch 86/100
 - 32s - loss: 1.0038 - acc: 0.7166 - val_loss: 7.8046 - val_acc: 0.1484
Epoch 87/100
 - 32s - loss: 0.9921 - acc: 0.7195 - val_loss: 7.7744 - val_acc: 0.1546
Epoch 88/100
 - 32s - loss: 0.9904 - acc: 0.7190 - val_loss: 7.9418 - val_acc: 0.1553
Epoch 89/100
 - 32s - loss: 0.9887 - acc: 0.7202 - val_loss: 7.7137 - val_acc: 0.1485
Epoch 90/100
 - 32s - loss: 0.9723 - acc: 0.7241 - val_loss: 7.8378 - val_acc: 0.1539
Epoch 91/100
 - 32s - loss: 0.9579 - acc: 0.7303 - val_loss: 7.9227 - val_acc: 0.1471
Epoch 92/100
 - 32s - loss: 0.9508 - acc: 0.7304 - val_loss: 8.0754 - val_acc: 0.1422
Epoch 93/100
 - 32s - loss: 0.9430 - acc: 0.7314 - val_loss: 8.2807 - val_acc: 0.1459
Epoch 94/100
 - 32s - loss: 0.9522 - acc: 0.7292 - val_loss: 7.9115 - val_acc: 0.1507
Epoch 95/100
 - 32s - loss: 0.9538 - acc: 0.7316 - val_loss: 8.1275 - val_acc: 0.1460
Epoch 96/100
 - 32s - loss: 0.9099 - acc: 0.7414 - val_loss: 8.1952 - val_acc: 0.1485
Epoch 97/100
 - 32s - loss: 0.9160 - acc: 0.7412 - val_loss: 8.1933 - val_acc: 0.1518
Epoch 98/100
 - 32s - loss: 0.9026 - acc: 0.7444 - val_loss: 8.2146 - val_acc: 0.1499
Epoch 99/100
 - 32s - loss: 0.9171 - acc: 0.7411 - val_loss: 8.2763 - val_acc: 0.1485
Epoch 100/100
 - 32s - loss: 0.8999 - acc: 0.7439 - val_loss: 8.3150 - val_acc: 0.1429
training history:
{'val_loss': [4.728889900970459, 4.3761884841918945, 4.188470881652832, 4.117450954437256, 3.9669528495788575, 3.843161057281494, 3.870735587310791, 3.7892396194458007, 3.822741785430908, 3.777535503387451, 3.8520839111328127, 3.7519710494995118, 3.7239860633850097, 3.7805515716552733, 3.7350390701293947, 3.7905503128051756, 3.793896662902832, 3.83091841506958, 3.826523712158203, 3.8051001319885254, 3.9513947029113767, 3.974256231689453, 4.020066851043701, 4.027474545288086, 4.111245280456543, 4.1015312789917, 4.253590762329101, 4.1790111572265625, 4.360616680145264, 4.344996060943603, 4.3472801849365235, 4.493797764587402, 4.647962892150879, 4.570552612304687, 4.670365103149414, 4.71118600692749, 4.760870861816406, 4.817209451293945, 4.951931832885742, 5.048123355865479, 5.04442752532959, 5.070365913391114, 5.240638375854492, 5.2322567970275875, 5.385224057006836, 5.4945425659179685, 5.585241748046875, 5.4467326942443846, 5.81095180053711, 5.695000309753418, 5.684799772644043, 5.984770700073242, 5.913267082214356, 6.083307007598877, 6.142057968139649, 6.101795803070068, 6.285161126708984, 6.225532846069336, 6.2912225288391115, 6.375145324707031, 6.489983630371094, 6.4689693359375, 6.863730456542969, 6.692076463317871, 6.767626170349121, 6.5441346313476565, 6.919068935394287, 6.925424626159668, 6.878010025024414, 7.172039025878906, 7.038819241333008, 7.2023985443115235, 7.262924089050293, 7.2174984741210935, 7.267090682983398, 7.338879463195801, 7.4715887100219724, 7.326046315765381, 7.434291668701172, 7.456009928894043, 7.614586227416992, 7.532999697875977, 7.7250791381835935, 7.813686886596679, 7.557003616333008, 7.804551623535156, 7.774431497192383, 7.941830108642578, 7.71368939819336, 7.837847131347656, 7.922682931518555, 8.075409996032715, 8.280650802612305, 7.911514971923828, 8.12751957397461, 8.195185098266602, 8.193301109313964, 8.214598188781737, 8.27628239440918, 8.31500551147461], 'val_acc': [0.0462, 0.0876, 0.1177, 0.1189, 0.1414, 0.1613, 0.1633, 0.1761, 0.1678, 0.1747, 0.1741, 0.1827, 0.1943, 0.1913, 0.1943, 0.1941, 0.1965, 0.1887, 0.1904, 0.1961, 0.1913, 0.1865, 0.189, 0.1855, 0.1831, 0.1886, 0.1807, 0.1889, 0.1766, 0.1895, 0.185, 0.1778, 0.1745, 0.1793, 0.1776, 0.1762, 0.1756, 0.1719, 0.1706, 0.1759, 0.17, 0.1707, 0.1698, 0.168, 0.1618, 0.1685, 0.1679, 0.1625, 0.1611, 0.1622, 0.1597, 0.1588, 0.1619, 0.1597, 0.1628, 0.1557, 0.1617, 0.1556, 0.1599, 0.1528, 0.1545, 0.1598, 0.152, 0.154, 0.1509, 0.1582, 0.153, 0.1606, 0.15, 0.1557, 0.1491, 0.1511, 0.1525, 0.1566, 0.1539, 0.1544, 0.1572, 0.1489, 0.148, 0.1498, 0.1479, 0.1482, 0.1571, 0.1497, 0.1446, 0.1484, 0.1546, 0.1553, 0.1485, 0.1539, 0.1471, 0.1422, 0.1459, 0.1507, 0.146, 0.1485, 0.1518, 0.1499, 0.1485, 0.1429], 'loss': [5.003488151550293, 4.534529754333496, 4.268048865509034, 4.080470369415283, 3.942097969207764, 3.8298259434509276, 3.7290117948913575, 3.64311384475708, 3.5772643070983885, 3.5041588093566896, 3.443411760406494, 3.3821367443847654, 3.3307568705749513, 3.276577000579834, 3.2197519503784178, 3.166868864212036, 3.1204820333862306, 3.06697568939209, 3.0213638957214357, 2.961608374633789, 2.917616858215332, 2.862858240356445, 2.8080810891723633, 2.7693598021697996, 2.714994287490845, 2.6677239532470702, 2.6218681172180176, 2.5656181201171875, 2.5209315925598146, 2.4670264669036865, 2.4335798101806643, 2.3810086698150634, 2.338625329208374, 2.2945853021240232, 2.2478997731018064, 2.209630106048584, 2.1590440046691897, 2.102481953201294, 2.067186537246704, 2.035135319061279, 1.9886737948989868, 1.9469665972518921, 1.9153731132507323, 1.876067568283081, 1.8346050830459595, 1.8076461587524415, 1.7711413173866273, 1.7293247365951538, 1.6992208557510375, 1.6739743244934082, 1.6470202807998657, 1.604855718345642, 1.588757353439331, 1.5499709487915039, 1.5259288262557984, 1.4953678520584106, 1.4668000855636596, 1.4435240287017823, 1.4305052114486694, 1.4029560633468627, 1.357350053100586, 1.376966431503296, 1.3258318234062194, 1.3225011952781678, 1.2898821802139282, 1.2709762601089478, 1.261384373474121, 1.228221241836548, 1.2225127726364136, 1.2085759882354736, 1.1930291038894654, 1.1771374203109741, 1.174563196144104, 1.149157709236145, 1.1230182511901856, 1.1441498263549805, 1.0987831893920899, 1.1046178629684449, 1.075532741909027, 1.072982439994812, 1.0611231832504273, 1.053724537563324, 1.0353882277679443, 1.0358398996353149, 1.008978120212555, 1.003686250629425, 0.9920943690109253, 0.9904077217674255, 0.9885626265716553, 0.972388498954773, 0.9578830504417419, 0.9509001699066162, 0.9431113330841064, 0.9523691354751587, 0.9538133340644837, 0.9098682910919189, 0.9159513669013977, 0.9027206384658814, 0.9171348069763183, 0.9000029846954346], 'acc': [0.02303, 0.07158, 0.10201, 0.12695, 0.14563, 0.1619, 0.17639, 0.19025, 0.19985, 0.212, 0.21928, 0.22904, 0.23601, 0.24441, 0.25539, 0.26347, 0.27106, 0.28055, 0.28757, 0.29817, 0.30493, 0.31512, 0.32378, 0.33226, 0.34008, 0.35012, 0.35889, 0.36755, 0.37763, 0.38856, 0.39404, 0.40618, 0.41318, 0.42172, 0.42963, 0.4396, 0.44764, 0.46102, 0.46925, 0.47736, 0.48652, 0.49337, 0.50078, 0.50903, 0.51812, 0.52268, 0.53237, 0.5388, 0.54796, 0.55445, 0.56033, 0.57012, 0.57265, 0.58349, 0.58784, 0.59455, 0.59936, 0.60743, 0.6099, 0.61593, 0.62898, 0.62173, 0.63407, 0.63519, 0.64237, 0.64619, 0.65152, 0.65929, 0.66046, 0.66331, 0.66617, 0.67046, 0.67122, 0.67891, 0.68482, 0.67945, 0.68823, 0.69091, 0.69752, 0.69854, 0.70262, 0.70409, 0.70774, 0.70762, 0.71477, 0.71665, 0.71949, 0.719, 0.72017, 0.72409, 0.73025, 0.73034, 0.73136, 0.72919, 0.73157, 0.74138, 0.74124, 0.7444, 0.74106, 0.74385]}
Training time: 
3235.32248210907
Evaluation results:  [8.32690863647461, 0.1443]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_6 (Conv2D)            (None, 58, 62, 48)        3072      
_________________________________________________________________
batch_normalization_1 (Batch (None, 58, 62, 48)        192       
_________________________________________________________________
activation_4 (Activation)    (None, 58, 62, 48)        0         
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 28, 30, 48)        0         
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 24, 30, 128)       30848     
_________________________________________________________________
batch_normalization_2 (Batch (None, 24, 30, 128)       512       
_________________________________________________________________
activation_5 (Activation)    (None, 24, 30, 128)       0         
_________________________________________________________________
max_pooling2d_5 (MaxPooling2 (None, 11, 14, 128)       0         
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 9, 14, 192)        73920     
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 7, 14, 192)        110784    
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 5, 14, 128)        73856     
_________________________________________________________________
activation_6 (Activation)    (None, 5, 14, 128)        0         
_________________________________________________________________
max_pooling2d_6 (MaxPooling2 (None, 2, 6, 128)         0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 1536)              0         
_________________________________________________________________
dense_4 (Dense)              (None, 2048)              3147776   
_________________________________________________________________
dense_5 (Dense)              (None, 2048)              4196352   
_________________________________________________________________
dense_6 (Dense)              (None, 200)               409800    
=================================================================
Total params: 8,047,112
Trainable params: 8,046,760
Non-trainable params: 352
_________________________________________________________________
Epoch 1/100
 - 42s - loss: 5.3003 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 2/100
 - 42s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 3/100
 - 42s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 4/100
 - 42s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 5/100
 - 42s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 6/100
 - 42s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 7/100
 - 42s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 8/100
 - 42s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 9/100
 - 42s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 10/100
 - 42s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 11/100
 - 42s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 12/100
 - 42s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 13/100
 - 42s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 14/100
 - 42s - loss: 5.2991 - acc: 0.0041 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 15/100
 - 42s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 16/100
 - 42s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 17/100
 - 42s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 18/100
 - 42s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 19/100
 - 42s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 20/100
 - 42s - loss: 5.2991 - acc: 0.0048 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 21/100
 - 42s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 22/100
 - 42s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 23/100
 - 42s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 24/100
 - 42s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 25/100
 - 42s - loss: 5.2991 - acc: 0.0041 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 26/100
 - 42s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 27/100
 - 42s - loss: 5.2991 - acc: 0.0041 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 28/100
 - 42s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 29/100
 - 42s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 30/100
 - 42s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 31/100
 - 42s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 32/100
 - 42s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 33/100
 - 42s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 34/100
 - 42s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 35/100
 - 42s - loss: 5.2991 - acc: 0.0040 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 36/100
 - 42s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 37/100
 - 42s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 38/100
 - 42s - loss: 5.2991 - acc: 0.0041 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 39/100
 - 42s - loss: 5.2991 - acc: 0.0041 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 40/100
 - 42s - loss: 5.2991 - acc: 0.0047 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 41/100
 - 42s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 42/100
 - 42s - loss: 5.2991 - acc: 0.0041 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 43/100
 - 42s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 44/100
 - 42s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 45/100
 - 42s - loss: 5.2991 - acc: 0.0047 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 46/100
 - 42s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 47/100
 - 42s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 48/100
 - 42s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 49/100
 - 42s - loss: 5.2991 - acc: 0.0046 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 50/100
 - 42s - loss: 5.2991 - acc: 0.0041 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 51/100
 - 42s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2984 - val_acc: 0.0050
Epoch 52/100
 - 42s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 53/100
 - 42s - loss: 5.2991 - acc: 0.0040 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 54/100
 - 42s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 55/100
 - 42s - loss: 5.2991 - acc: 0.0041 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 56/100
 - 42s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 57/100
 - 42s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 58/100
 - 42s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 59/100
 - 42s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 60/100
 - 42s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 61/100
 - 42s - loss: 5.2991 - acc: 0.0038 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 62/100
 - 42s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 63/100
 - 42s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 64/100
 - 42s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 65/100
 - 42s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 66/100
 - 42s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 67/100
 - 42s - loss: 5.2991 - acc: 0.0041 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 68/100
 - 42s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 69/100
 - 42s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 70/100
 - 42s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 71/100
 - 42s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 72/100
 - 42s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 73/100
 - 42s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 74/100
 - 42s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 75/100
 - 42s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 76/100
 - 42s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 77/100
 - 42s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 78/100
 - 42s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 79/100
 - 42s - loss: 5.2991 - acc: 0.0041 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 80/100
 - 42s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 81/100
 - 42s - loss: 5.2991 - acc: 0.0041 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 82/100
 - 42s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 83/100
 - 42s - loss: 5.2991 - acc: 0.0041 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 84/100
 - 42s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 85/100
 - 42s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 86/100
 - 42s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 87/100
 - 42s - loss: 5.2991 - acc: 0.0040 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 88/100
 - 42s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 89/100
 - 42s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 90/100
 - 42s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 91/100
 - 42s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2984 - val_acc: 0.0050
Epoch 92/100
 - 42s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 93/100
 - 42s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 94/100
 - 42s - loss: 5.2991 - acc: 0.0040 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 95/100
 - 42s - loss: 5.2991 - acc: 0.0041 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 96/100
 - 42s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 97/100
 - 42s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 98/100
 - 42s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 99/100
 - 42s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 100/100
 - 42s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
training history:
{'val_loss': [5.298340776062012, 5.298344921112061, 5.298342199707031, 5.298343708801269, 5.298348460388183, 5.298349391174316, 5.298345757293701, 5.298343023681641, 5.298344987487793, 5.298339991760254, 5.298341780853272, 5.298346208190918, 5.2983460662841795, 5.298347428894043, 5.298345149230957, 5.298345890808106, 5.298343516540528, 5.298344867706299, 5.298344549560547, 5.298345516967774, 5.298344240570068, 5.29833971862793, 5.298343495941162, 5.29834428100586, 5.298344194030761, 5.298346000671387, 5.2983466918945314, 5.2983471908569335, 5.298344871520996, 5.298345524597168, 5.298341751098633, 5.298345144653321, 5.298345266723633, 5.298343344116211, 5.298341149139405, 5.298345013427735, 5.298347709655761, 5.298343572235107, 5.298345753479004, 5.298344914245606, 5.29834677810669, 5.298348262786865, 5.298344473266601, 5.298344076538086, 5.298343312835693, 5.298344483184814, 5.2983407905578614, 5.29834224319458, 5.29834649810791, 5.298344338989258, 5.298350038146973, 5.2983450485229495, 5.29834220123291, 5.298339112091065, 5.2983443069458005, 5.298343578338623, 5.298348561096192, 5.2983446807861325, 5.298340716552734, 5.298342367553711, 5.298341167449951, 5.29834409790039, 5.29834001159668, 5.298344506072998, 5.298347727966308, 5.298347277069092, 5.298342097473144, 5.298344599151611, 5.2983392700195315, 5.298341967010498, 5.298349346923828, 5.298343187713623, 5.298341320037842, 5.2983422927856445, 5.29834380569458, 5.2983459815979, 5.298341508483887, 5.298338757324219, 5.2983424049377446, 5.298341272735596, 5.298339323425293, 5.298341926574707, 5.298342231750488, 5.298344523620606, 5.298340198516846, 5.2983445571899415, 5.29834192199707, 5.29834119720459, 5.298342147827149, 5.298344621276856, 5.298352052307129, 5.298343737030029, 5.298341834259033, 5.298341570281982, 5.298341943359375, 5.298348468017578, 5.298343717956543, 5.298341883850098, 5.298343627929688, 5.298346087646484], 'val_acc': [0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005], 'loss': [5.300341107635498, 5.299073949584961, 5.299085897521973, 5.299088384094238, 5.299095986175537, 5.299076948699951, 5.2990947367858885, 5.299079780273438, 5.299086530914306, 5.299105848693848, 5.299083276977539, 5.299091390533447, 5.299084652252197, 5.299078479919434, 5.299094800109863, 5.299078837280273, 5.299099098052978, 5.2990963679504395, 5.2990872442626955, 5.2990782525634765, 5.299098134460449, 5.299092821807862, 5.299085333557129, 5.299092662963867, 5.299095249328613, 5.299083076477051, 5.299085826568604, 5.299092509460449, 5.299085353240967, 5.299073904724121, 5.299102796325683, 5.299092664489746, 5.299078685302734, 5.299090950012207, 5.2990919473266604, 5.299080919799804, 5.299087395019531, 5.299085597229004, 5.299085911407471, 5.299084482116699, 5.29908182144165, 5.2990880264282225, 5.299098124389649, 5.29908945968628, 5.299084945068359, 5.299067627563477, 5.299080279693603, 5.299078814086914, 5.299066640930175, 5.299091343078613, 5.2990680035400395, 5.299097294921875, 5.299083908996582, 5.299093486328125, 5.29908643157959, 5.299087323455811, 5.299084801330566, 5.299099502258301, 5.299087514038086, 5.299098967590332, 5.2990945840454104, 5.2990791836547855, 5.299089044189453, 5.299071428985596, 5.299086405487061, 5.299072870788574, 5.299082840881348, 5.299070253753662, 5.299085074615479, 5.299086858825683, 5.299062263183594, 5.2990941825866695, 5.2990873962402345, 5.2991011563110355, 5.299078731384277, 5.299080816040039, 5.299084873962403, 5.299086006164551, 5.299089384155273, 5.299082711791992, 5.299102191314697, 5.299078038940429, 5.299094091796875, 5.299085731658936, 5.299091026306153, 5.29909788696289, 5.299097633361816, 5.299096534118652, 5.299081554565429, 5.299097060546875, 5.299080768890381, 5.299104589233399, 5.299078806915283, 5.299102387695313, 5.299090646057129, 5.299089651184082, 5.299091891174316, 5.299086199951172, 5.299092101440429, 5.29907975189209], 'acc': [0.00444, 0.00449, 0.0044, 0.00423, 0.00441, 0.00446, 0.00442, 0.00436, 0.00449, 0.00451, 0.00428, 0.00424, 0.0044, 0.00411, 0.00431, 0.00437, 0.0042, 0.00432, 0.00441, 0.00478, 0.00427, 0.00428, 0.00444, 0.00439, 0.00412, 0.0044, 0.0041, 0.00419, 0.00424, 0.00427, 0.00426, 0.00433, 0.00441, 0.00437, 0.00405, 0.0042, 0.00433, 0.00409, 0.00411, 0.00473, 0.00439, 0.00414, 0.00435, 0.00435, 0.00466, 0.00429, 0.00442, 0.00437, 0.00459, 0.00414, 0.00452, 0.00429, 0.00402, 0.00437, 0.00413, 0.00427, 0.00438, 0.00419, 0.00433, 0.00418, 0.00384, 0.0043, 0.00425, 0.00452, 0.00433, 0.00441, 0.00414, 0.00443, 0.00453, 0.00449, 0.00429, 0.00438, 0.00446, 0.00454, 0.00447, 0.00446, 0.00453, 0.0042, 0.0041, 0.00451, 0.00412, 0.00439, 0.00413, 0.00422, 0.00433, 0.00434, 0.00397, 0.00437, 0.00448, 0.00441, 0.00418, 0.00444, 0.00438, 0.00396, 0.00414, 0.00443, 0.00417, 0.00439, 0.00442, 0.00425]}
Training time: 
4186.0297684669495
Evaluation results:  [5.298346069335937, 0.005]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_11 (Conv2D)           (None, 58, 62, 48)        3072      
_________________________________________________________________
activation_7 (Activation)    (None, 58, 62, 48)        0         
_________________________________________________________________
max_pooling2d_7 (MaxPooling2 (None, 28, 30, 48)        0         
_________________________________________________________________
conv2d_12 (Conv2D)           (None, 24, 30, 128)       30848     
_________________________________________________________________
activation_8 (Activation)    (None, 24, 30, 128)       0         
_________________________________________________________________
max_pooling2d_8 (MaxPooling2 (None, 11, 14, 128)       0         
_________________________________________________________________
conv2d_13 (Conv2D)           (None, 9, 14, 192)        73920     
_________________________________________________________________
conv2d_14 (Conv2D)           (None, 7, 14, 192)        110784    
_________________________________________________________________
conv2d_15 (Conv2D)           (None, 5, 14, 128)        73856     
_________________________________________________________________
activation_9 (Activation)    (None, 5, 14, 128)        0         
_________________________________________________________________
max_pooling2d_9 (MaxPooling2 (None, 2, 6, 128)         0         
_________________________________________________________________
flatten_3 (Flatten)          (None, 1536)              0         
_________________________________________________________________
dense_7 (Dense)              (None, 2048)              3147776   
_________________________________________________________________
dense_8 (Dense)              (None, 2048)              4196352   
_________________________________________________________________
dense_9 (Dense)              (None, 200)               409800    
=================================================================
Total params: 8,046,408
Trainable params: 8,046,408
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
 - 33s - loss: 5.3115 - acc: 0.0047 - val_loss: 5.3004 - val_acc: 0.0050
Epoch 2/100
 - 32s - loss: 5.3027 - acc: 0.0046 - val_loss: 5.3004 - val_acc: 0.0050
Epoch 3/100
 - 32s - loss: 5.3019 - acc: 0.0047 - val_loss: 5.3001 - val_acc: 0.0050
Epoch 4/100
 - 33s - loss: 5.3018 - acc: 0.0049 - val_loss: 5.2998 - val_acc: 0.0050
Epoch 5/100
 - 33s - loss: 5.3020 - acc: 0.0049 - val_loss: 5.3019 - val_acc: 0.0050
Epoch 6/100
 - 33s - loss: 5.3023 - acc: 0.0045 - val_loss: 5.2998 - val_acc: 0.0050
Epoch 7/100
 - 33s - loss: 5.3018 - acc: 0.0045 - val_loss: 5.3000 - val_acc: 0.0050
Epoch 8/100
 - 33s - loss: 5.3018 - acc: 0.0044 - val_loss: 5.3001 - val_acc: 0.0050
Epoch 9/100
 - 33s - loss: 5.3017 - acc: 0.0049 - val_loss: 5.3000 - val_acc: 0.0050
Epoch 10/100
 - 33s - loss: 5.3016 - acc: 0.0046 - val_loss: 5.2999 - val_acc: 0.0050
Epoch 11/100
 - 33s - loss: 5.3018 - acc: 0.0042 - val_loss: 5.2999 - val_acc: 0.0050
Epoch 12/100
 - 33s - loss: 5.3018 - acc: 0.0049 - val_loss: 5.3000 - val_acc: 0.0050
Epoch 13/100
 - 33s - loss: 5.3017 - acc: 0.0045 - val_loss: 5.3001 - val_acc: 0.0050
Epoch 14/100
 - 33s - loss: 5.3018 - acc: 0.0046 - val_loss: 5.3001 - val_acc: 0.0050
Epoch 15/100
 - 33s - loss: 5.3018 - acc: 0.0043 - val_loss: 5.2997 - val_acc: 0.0050
Epoch 16/100
 - 33s - loss: 5.3018 - acc: 0.0048 - val_loss: 5.2998 - val_acc: 0.0050
Epoch 17/100
 - 33s - loss: 5.3018 - acc: 0.0046 - val_loss: 5.3000 - val_acc: 0.0050
Epoch 18/100
 - 33s - loss: 5.3018 - acc: 0.0045 - val_loss: 5.2999 - val_acc: 0.0050
Epoch 19/100
 - 33s - loss: 5.3017 - acc: 0.0044 - val_loss: 5.3002 - val_acc: 0.0050
Epoch 20/100
 - 33s - loss: 5.3019 - acc: 0.0048 - val_loss: 5.2998 - val_acc: 0.0050
Epoch 21/100
 - 33s - loss: 5.3018 - acc: 0.0046 - val_loss: 5.2997 - val_acc: 0.0050
Epoch 22/100
 - 33s - loss: 5.3018 - acc: 0.0045 - val_loss: 5.2998 - val_acc: 0.0050
Epoch 23/100
 - 33s - loss: 5.3017 - acc: 0.0047 - val_loss: 5.2998 - val_acc: 0.0050
Epoch 24/100
 - 33s - loss: 5.3016 - acc: 0.0045 - val_loss: 5.3002 - val_acc: 0.0050
Epoch 25/100
 - 33s - loss: 5.3018 - acc: 0.0046 - val_loss: 5.2998 - val_acc: 0.0050
Epoch 26/100
 - 33s - loss: 5.3017 - acc: 0.0041 - val_loss: 5.3001 - val_acc: 0.0050
Epoch 27/100
 - 33s - loss: 5.3017 - acc: 0.0049 - val_loss: 5.3001 - val_acc: 0.0050
Epoch 28/100
 - 33s - loss: 5.3019 - acc: 0.0043 - val_loss: 5.2999 - val_acc: 0.0050
Epoch 29/100
 - 33s - loss: 5.3017 - acc: 0.0048 - val_loss: 5.3000 - val_acc: 0.0050
Epoch 30/100
 - 33s - loss: 5.3018 - acc: 0.0044 - val_loss: 5.3000 - val_acc: 0.0050
Epoch 31/100
 - 33s - loss: 5.3018 - acc: 0.0047 - val_loss: 5.2999 - val_acc: 0.0050
Epoch 32/100
 - 33s - loss: 5.3018 - acc: 0.0044 - val_loss: 5.2999 - val_acc: 0.0050
Epoch 33/100
 - 33s - loss: 5.3020 - acc: 0.0048 - val_loss: 5.2999 - val_acc: 0.0050
Epoch 34/100
 - 33s - loss: 5.3018 - acc: 0.0048 - val_loss: 5.3000 - val_acc: 0.0050
Epoch 35/100
 - 33s - loss: 5.3017 - acc: 0.0049 - val_loss: 5.3001 - val_acc: 0.0050
Epoch 36/100
 - 33s - loss: 5.3020 - acc: 0.0039 - val_loss: 5.2998 - val_acc: 0.0050
Epoch 37/100
 - 33s - loss: 5.3018 - acc: 0.0048 - val_loss: 5.2999 - val_acc: 0.0050
Epoch 38/100
 - 33s - loss: 5.3018 - acc: 0.0047 - val_loss: 5.3000 - val_acc: 0.0050
Epoch 39/100
 - 33s - loss: 5.3019 - acc: 0.0047 - val_loss: 5.2998 - val_acc: 0.0050
Epoch 40/100
 - 33s - loss: 5.3018 - acc: 0.0045 - val_loss: 5.3000 - val_acc: 0.0050
Epoch 41/100
 - 33s - loss: 5.3018 - acc: 0.0046 - val_loss: 5.2998 - val_acc: 0.0050
Epoch 42/100
 - 33s - loss: 5.3018 - acc: 0.0046 - val_loss: 5.2999 - val_acc: 0.0050
Epoch 43/100
 - 33s - loss: 5.3017 - acc: 0.0045 - val_loss: 5.3000 - val_acc: 0.0050
Epoch 44/100
 - 33s - loss: 5.3019 - acc: 0.0045 - val_loss: 5.2996 - val_acc: 0.0050
Epoch 45/100
 - 33s - loss: 5.3016 - acc: 0.0046 - val_loss: 5.3003 - val_acc: 0.0050
Epoch 46/100
 - 33s - loss: 5.3018 - acc: 0.0042 - val_loss: 5.3000 - val_acc: 0.0050
Epoch 47/100
 - 33s - loss: 5.3019 - acc: 0.0049 - val_loss: 5.2997 - val_acc: 0.0050
Epoch 48/100
 - 33s - loss: 5.3017 - acc: 0.0047 - val_loss: 5.2999 - val_acc: 0.0050
Epoch 49/100
 - 33s - loss: 5.3018 - acc: 0.0047 - val_loss: 5.2998 - val_acc: 0.0050
Epoch 50/100
 - 33s - loss: 5.3017 - acc: 0.0044 - val_loss: 5.3002 - val_acc: 0.0050
Epoch 51/100
 - 33s - loss: 5.3019 - acc: 0.0045 - val_loss: 5.2997 - val_acc: 0.0050
Epoch 52/100
 - 33s - loss: 5.3018 - acc: 0.0044 - val_loss: 5.2999 - val_acc: 0.0050
Epoch 53/100
 - 33s - loss: 5.3019 - acc: 0.0047 - val_loss: 5.3001 - val_acc: 0.0050
Epoch 54/100
 - 33s - loss: 5.3018 - acc: 0.0047 - val_loss: 5.2998 - val_acc: 0.0050
Epoch 55/100
 - 33s - loss: 5.3018 - acc: 0.0045 - val_loss: 5.3000 - val_acc: 0.0050
Epoch 56/100
 - 33s - loss: 5.3015 - acc: 0.0047 - val_loss: 5.3005 - val_acc: 0.0050
Epoch 57/100
 - 33s - loss: 5.3018 - acc: 0.0046 - val_loss: 5.3000 - val_acc: 0.0050
Epoch 58/100
 - 33s - loss: 5.3018 - acc: 0.0045 - val_loss: 5.3000 - val_acc: 0.0050
Epoch 59/100
 - 33s - loss: 5.3017 - acc: 0.0049 - val_loss: 5.3001 - val_acc: 0.0050
Epoch 60/100
 - 33s - loss: 5.3019 - acc: 0.0047 - val_loss: 5.2999 - val_acc: 0.0050
Epoch 61/100
 - 33s - loss: 5.3017 - acc: 0.0048 - val_loss: 5.3002 - val_acc: 0.0050
Epoch 62/100
 - 33s - loss: 5.3017 - acc: 0.0047 - val_loss: 5.3002 - val_acc: 0.0050
Epoch 63/100
 - 33s - loss: 5.3017 - acc: 0.0048 - val_loss: 5.3003 - val_acc: 0.0050
Epoch 64/100
 - 33s - loss: 5.3019 - acc: 0.0046 - val_loss: 5.2998 - val_acc: 0.0050
Epoch 65/100
 - 33s - loss: 5.3018 - acc: 0.0047 - val_loss: 5.2998 - val_acc: 0.0050
Epoch 66/100
 - 33s - loss: 5.3018 - acc: 0.0044 - val_loss: 5.2999 - val_acc: 0.0050
Epoch 67/100
 - 33s - loss: 5.3017 - acc: 0.0048 - val_loss: 5.2999 - val_acc: 0.0050
Epoch 68/100
 - 33s - loss: 5.3017 - acc: 0.0047 - val_loss: 5.2998 - val_acc: 0.0050
Epoch 69/100
 - 33s - loss: 5.3017 - acc: 0.0046 - val_loss: 5.2999 - val_acc: 0.0050
Epoch 70/100
 - 33s - loss: 5.3017 - acc: 0.0047 - val_loss: 5.2999 - val_acc: 0.0050
Epoch 71/100
 - 33s - loss: 5.3017 - acc: 0.0048 - val_loss: 5.3000 - val_acc: 0.0050
Epoch 72/100
 - 33s - loss: 5.3018 - acc: 0.0047 - val_loss: 5.3000 - val_acc: 0.0050
Epoch 73/100
 - 33s - loss: 5.3020 - acc: 0.0046 - val_loss: 5.2998 - val_acc: 0.0050
Epoch 74/100
 - 33s - loss: 5.3018 - acc: 0.0045 - val_loss: 5.3000 - val_acc: 0.0050
Epoch 75/100
 - 33s - loss: 5.3018 - acc: 0.0044 - val_loss: 5.3001 - val_acc: 0.0050
Epoch 76/100
 - 33s - loss: 5.3017 - acc: 0.0050 - val_loss: 5.2999 - val_acc: 0.0050
Epoch 77/100
 - 33s - loss: 5.3018 - acc: 0.0044 - val_loss: 5.3000 - val_acc: 0.0050
Epoch 78/100
 - 33s - loss: 5.3019 - acc: 0.0046 - val_loss: 5.2998 - val_acc: 0.0050
Epoch 79/100
 - 33s - loss: 5.3017 - acc: 0.0045 - val_loss: 5.3002 - val_acc: 0.0050
Epoch 80/100
 - 33s - loss: 5.3019 - acc: 0.0046 - val_loss: 5.2999 - val_acc: 0.0050
Epoch 81/100
 - 33s - loss: 5.3018 - acc: 0.0048 - val_loss: 5.3000 - val_acc: 0.0050
Epoch 82/100
 - 33s - loss: 5.3017 - acc: 0.0048 - val_loss: 5.3001 - val_acc: 0.0050
Epoch 83/100
 - 33s - loss: 5.3017 - acc: 0.0047 - val_loss: 5.3003 - val_acc: 0.0050
Epoch 84/100
 - 33s - loss: 5.3019 - acc: 0.0048 - val_loss: 5.2998 - val_acc: 0.0050
Epoch 85/100
 - 33s - loss: 5.3017 - acc: 0.0048 - val_loss: 5.3000 - val_acc: 0.0050
Epoch 86/100
 - 33s - loss: 5.3017 - acc: 0.0046 - val_loss: 5.3000 - val_acc: 0.0050
Epoch 87/100
 - 33s - loss: 5.3018 - acc: 0.0046 - val_loss: 5.3000 - val_acc: 0.0050
Epoch 88/100
 - 33s - loss: 5.3019 - acc: 0.0046 - val_loss: 5.2999 - val_acc: 0.0050
Epoch 89/100
 - 33s - loss: 5.3018 - acc: 0.0049 - val_loss: 5.2998 - val_acc: 0.0050
Epoch 90/100
 - 33s - loss: 5.3016 - acc: 0.0048 - val_loss: 5.3001 - val_acc: 0.0050
Epoch 91/100
 - 33s - loss: 5.3016 - acc: 0.0048 - val_loss: 5.3003 - val_acc: 0.0050
Epoch 92/100
 - 33s - loss: 5.3020 - acc: 0.0045 - val_loss: 5.2998 - val_acc: 0.0050
Epoch 93/100
 - 33s - loss: 5.3017 - acc: 0.0045 - val_loss: 5.2999 - val_acc: 0.0050
Epoch 94/100
 - 33s - loss: 5.3017 - acc: 0.0044 - val_loss: 5.2998 - val_acc: 0.0050
Epoch 95/100
 - 33s - loss: 5.3016 - acc: 0.0046 - val_loss: 5.3001 - val_acc: 0.0050
Epoch 96/100
 - 33s - loss: 5.3018 - acc: 0.0046 - val_loss: 5.3000 - val_acc: 0.0050
Epoch 97/100
 - 33s - loss: 5.3016 - acc: 0.0043 - val_loss: 5.3002 - val_acc: 0.0050
Epoch 98/100
 - 33s - loss: 5.3017 - acc: 0.0043 - val_loss: 5.3001 - val_acc: 0.0050
Epoch 99/100
 - 33s - loss: 5.3020 - acc: 0.0045 - val_loss: 5.2999 - val_acc: 0.0050
Epoch 100/100
 - 33s - loss: 5.3018 - acc: 0.0049 - val_loss: 5.2999 - val_acc: 0.0050
training history:
{'val_loss': [5.300415620422363, 5.300383516693115, 5.30005870513916, 5.299826152801514, 5.301945176696777, 5.299801943206787, 5.30002668762207, 5.300105244445801, 5.300038747406006, 5.299850134277344, 5.299885745239258, 5.299966159057617, 5.300091516113281, 5.300132057189941, 5.299749087524414, 5.299774382781982, 5.3000022850036625, 5.299924208831787, 5.300240249633789, 5.2998062545776365, 5.299686655426026, 5.299820591735839, 5.29984023513794, 5.300165655517578, 5.299845925903321, 5.30013010635376, 5.300076476287842, 5.299937517547607, 5.300023596191406, 5.300025769042969, 5.299858330535889, 5.2999401268005375, 5.299906600189209, 5.2999678909301755, 5.300085215759277, 5.299784231567383, 5.299940414428711, 5.299959576416016, 5.299768384552002, 5.2999629959106445, 5.299825836181641, 5.299949456787109, 5.2999820755004885, 5.299646728515625, 5.300289988708496, 5.3000303085327145, 5.299726850128174, 5.299873375701904, 5.299784937286377, 5.30018663482666, 5.299732009887696, 5.299923112487793, 5.300123879241943, 5.299760136413574, 5.300049255371094, 5.3005491073608395, 5.300040724182129, 5.300039715576172, 5.300059526062012, 5.299900164794922, 5.300169116210937, 5.300192617034912, 5.3003423904418945, 5.299821152496338, 5.299780121612549, 5.299909046936035, 5.29985792388916, 5.299848434448243, 5.2999101257324215, 5.299929768371582, 5.300038920593262, 5.300038242340088, 5.299766484069824, 5.299993765258789, 5.300107099151611, 5.2998704254150395, 5.299981362915039, 5.299797528839111, 5.300176824951172, 5.29986706085205, 5.299976011657715, 5.300061032104492, 5.30028839263916, 5.299767007446289, 5.299951628112793, 5.299950485229492, 5.299990274047851, 5.29987587890625, 5.299769557189942, 5.300107981109619, 5.300272747039795, 5.299791284179688, 5.299923573303222, 5.299798825836182, 5.300144255065918, 5.299979031372071, 5.30017777557373, 5.300133193206787, 5.299855537414551, 5.299903154754639], 'val_acc': [0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005], 'loss': [5.311495703735352, 5.302705372924804, 5.301947562561035, 5.301799181518555, 5.30202308883667, 5.302277563171387, 5.301774351196289, 5.301771599731445, 5.301743335571289, 5.301643689270019, 5.301826831054687, 5.301806649780273, 5.301691315917969, 5.301757016296387, 5.301814428710937, 5.301804404449463, 5.301788017578125, 5.301767274780273, 5.301733372802734, 5.301874838256836, 5.301751465759278, 5.301821802368164, 5.301698584594726, 5.301615957336426, 5.301792540893555, 5.301737142944336, 5.301719689025879, 5.301858831329346, 5.301688084716797, 5.301784035949707, 5.301780213623047, 5.301836280822754, 5.301956825561524, 5.301851905059815, 5.3017276399230955, 5.3019738883972165, 5.301770368041992, 5.301771994781494, 5.301881776275635, 5.301836441650391, 5.301827225952149, 5.301751243133545, 5.301741933898926, 5.301911757354737, 5.301589414367676, 5.301767795715332, 5.301856486206055, 5.301670786132813, 5.30182271850586, 5.301735933074951, 5.301919911193847, 5.301779566192627, 5.301858168029785, 5.301803323364258, 5.301825166778564, 5.3015235766601565, 5.301757563934326, 5.301784583740234, 5.301743768463135, 5.30186239868164, 5.30167501663208, 5.3016882803344725, 5.301689316711426, 5.301909294891358, 5.301790138092041, 5.301764000854492, 5.301707309265137, 5.301747211914062, 5.3016876417541505, 5.301726368255615, 5.301740500183105, 5.301790041351318, 5.301988713531494, 5.3017746125793455, 5.301834993286133, 5.301725728149414, 5.301848550109863, 5.301871470947265, 5.3017542665100095, 5.301856288452148, 5.301756876678467, 5.301734023742676, 5.301661463012695, 5.301923051452636, 5.301716693115234, 5.301737713775635, 5.301825556488037, 5.301851423034668, 5.3018106193542485, 5.301630645294189, 5.301635981445313, 5.302030772094726, 5.301702090759277, 5.301721431121826, 5.3016367304992675, 5.301797752227783, 5.301637010192871, 5.301726174926758, 5.301955566711426, 5.301812614135742], 'acc': [0.00474, 0.00457, 0.00471, 0.00486, 0.00486, 0.00455, 0.00449, 0.00445, 0.0049, 0.00462, 0.00422, 0.00493, 0.00449, 0.00456, 0.0043, 0.00484, 0.00456, 0.0045, 0.00438, 0.00477, 0.00462, 0.00454, 0.00474, 0.00452, 0.00458, 0.00412, 0.00494, 0.00431, 0.00476, 0.00438, 0.00467, 0.00444, 0.00481, 0.00481, 0.00492, 0.00394, 0.00483, 0.00472, 0.00466, 0.0045, 0.00458, 0.00459, 0.00453, 0.00451, 0.00464, 0.00422, 0.00486, 0.00471, 0.00472, 0.00437, 0.00447, 0.00437, 0.00467, 0.0047, 0.00452, 0.00475, 0.0046, 0.00448, 0.00488, 0.0047, 0.00476, 0.00472, 0.00477, 0.00461, 0.00465, 0.00442, 0.00477, 0.00468, 0.00459, 0.00474, 0.00477, 0.00474, 0.00465, 0.00454, 0.00441, 0.00504, 0.00438, 0.00457, 0.00447, 0.00457, 0.00483, 0.0048, 0.00467, 0.00476, 0.00478, 0.00458, 0.00457, 0.00456, 0.00486, 0.00482, 0.00482, 0.00449, 0.00452, 0.00445, 0.00457, 0.00463, 0.00434, 0.00429, 0.00448, 0.00486]}
Training time: 
3256.986827135086
Evaluation results:  [5.299903184509278, 0.005]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_16 (Conv2D)           (None, 58, 62, 48)        3072      
_________________________________________________________________
activation_10 (Activation)   (None, 58, 62, 48)        0         
_________________________________________________________________
max_pooling2d_10 (MaxPooling (None, 28, 30, 48)        0         
_________________________________________________________________
conv2d_17 (Conv2D)           (None, 24, 30, 128)       30848     
_________________________________________________________________
activation_11 (Activation)   (None, 24, 30, 128)       0         
_________________________________________________________________
max_pooling2d_11 (MaxPooling (None, 11, 14, 128)       0         
_________________________________________________________________
conv2d_18 (Conv2D)           (None, 9, 14, 192)        73920     
_________________________________________________________________
conv2d_19 (Conv2D)           (None, 7, 14, 192)        110784    
_________________________________________________________________
conv2d_20 (Conv2D)           (None, 5, 14, 128)        73856     
_________________________________________________________________
activation_12 (Activation)   (None, 5, 14, 128)        0         
_________________________________________________________________
max_pooling2d_12 (MaxPooling (None, 2, 6, 128)         0         
_________________________________________________________________
flatten_4 (Flatten)          (None, 1536)              0         
_________________________________________________________________
dense_10 (Dense)             (None, 2048)              3147776   
_________________________________________________________________
dense_11 (Dense)             (None, 2048)              4196352   
_________________________________________________________________
dense_12 (Dense)             (None, 200)               409800    
=================================================================
Total params: 8,046,408
Trainable params: 8,046,408
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
 - 31s - loss: 4.9009 - acc: 0.0427 - val_loss: 4.3724 - val_acc: 0.0899
Epoch 2/100
 - 30s - loss: 4.2842 - acc: 0.1095 - val_loss: 4.0594 - val_acc: 0.1269
Epoch 3/100
 - 30s - loss: 4.0525 - acc: 0.1395 - val_loss: 4.0710 - val_acc: 0.1397
Epoch 4/100
 - 30s - loss: 3.9296 - acc: 0.1573 - val_loss: 3.9744 - val_acc: 0.1476
Epoch 5/100
 - 30s - loss: 3.8553 - acc: 0.1695 - val_loss: 4.0975 - val_acc: 0.1487
Epoch 6/100
 - 30s - loss: 3.8156 - acc: 0.1775 - val_loss: 3.8832 - val_acc: 0.1668
Epoch 7/100
 - 30s - loss: 3.7768 - acc: 0.1832 - val_loss: 3.8221 - val_acc: 0.1732
Epoch 8/100
 - 30s - loss: 3.7574 - acc: 0.1869 - val_loss: 3.9146 - val_acc: 0.1678
Epoch 9/100
 - 30s - loss: 3.7363 - acc: 0.1900 - val_loss: 3.7723 - val_acc: 0.1858
Epoch 10/100
 - 30s - loss: 3.7261 - acc: 0.1939 - val_loss: 3.9046 - val_acc: 0.1817
Epoch 11/100
 - 30s - loss: 3.7113 - acc: 0.1967 - val_loss: 3.9029 - val_acc: 0.1702
Epoch 12/100
 - 30s - loss: 3.7091 - acc: 0.1948 - val_loss: 3.7841 - val_acc: 0.1831
Epoch 13/100
 - 30s - loss: 3.7040 - acc: 0.1959 - val_loss: 4.0906 - val_acc: 0.1445
Epoch 14/100
 - 30s - loss: 3.6978 - acc: 0.1996 - val_loss: 3.8949 - val_acc: 0.1679
Epoch 15/100
 - 30s - loss: 3.6971 - acc: 0.1985 - val_loss: 3.9317 - val_acc: 0.1645
Epoch 16/100
 - 30s - loss: 3.6940 - acc: 0.2001 - val_loss: 3.8244 - val_acc: 0.1802
Epoch 17/100
 - 30s - loss: 3.6971 - acc: 0.1989 - val_loss: 4.1334 - val_acc: 0.1535
Epoch 18/100
 - 30s - loss: 3.6954 - acc: 0.1997 - val_loss: 3.9442 - val_acc: 0.1750
Epoch 19/100
 - 30s - loss: 3.7056 - acc: 0.1970 - val_loss: 3.7856 - val_acc: 0.1900
Epoch 20/100
 - 30s - loss: 3.7009 - acc: 0.1973 - val_loss: 4.0021 - val_acc: 0.1639
Epoch 21/100
 - 30s - loss: 3.6943 - acc: 0.2004 - val_loss: 3.7959 - val_acc: 0.1869
Epoch 22/100
 - 30s - loss: 3.6943 - acc: 0.2006 - val_loss: 3.8541 - val_acc: 0.1754
Epoch 23/100
 - 30s - loss: 3.6911 - acc: 0.2007 - val_loss: 3.7172 - val_acc: 0.1971
Epoch 24/100
 - 30s - loss: 3.7022 - acc: 0.1998 - val_loss: 3.9454 - val_acc: 0.1729
Epoch 25/100
 - 30s - loss: 3.6948 - acc: 0.2008 - val_loss: 3.8204 - val_acc: 0.1825
Epoch 26/100
 - 30s - loss: 3.7057 - acc: 0.2013 - val_loss: 4.0457 - val_acc: 0.1490
Epoch 27/100
 - 30s - loss: 3.7099 - acc: 0.1986 - val_loss: 4.0977 - val_acc: 0.1432
Epoch 28/100
 - 30s - loss: 3.7159 - acc: 0.1999 - val_loss: 4.0574 - val_acc: 0.1602
Epoch 29/100
 - 30s - loss: 3.7236 - acc: 0.1988 - val_loss: 3.7929 - val_acc: 0.1882
Epoch 30/100
 - 30s - loss: 3.7238 - acc: 0.1975 - val_loss: 3.9934 - val_acc: 0.1606
Epoch 31/100
 - 30s - loss: 3.7330 - acc: 0.1958 - val_loss: 4.1781 - val_acc: 0.1617
Epoch 32/100
 - 30s - loss: 3.7354 - acc: 0.1962 - val_loss: 3.9634 - val_acc: 0.1663
Epoch 33/100
 - 30s - loss: 3.7375 - acc: 0.1944 - val_loss: 3.9332 - val_acc: 0.1705
Epoch 34/100
 - 30s - loss: 3.7513 - acc: 0.1945 - val_loss: 3.9075 - val_acc: 0.1769
Epoch 35/100
 - 30s - loss: 3.7587 - acc: 0.1938 - val_loss: 3.9233 - val_acc: 0.1788
Epoch 36/100
 - 30s - loss: 3.7570 - acc: 0.1935 - val_loss: 3.8250 - val_acc: 0.1896
Epoch 37/100
 - 30s - loss: 3.7750 - acc: 0.1900 - val_loss: 3.9277 - val_acc: 0.1657
Epoch 38/100
 - 30s - loss: 3.7831 - acc: 0.1897 - val_loss: 3.9835 - val_acc: 0.1611
Epoch 39/100
 - 30s - loss: 3.7831 - acc: 0.1902 - val_loss: 3.8784 - val_acc: 0.1772
Epoch 40/100
 - 30s - loss: 3.7770 - acc: 0.1893 - val_loss: 3.8701 - val_acc: 0.1693
Epoch 41/100
 - 30s - loss: 3.7846 - acc: 0.1897 - val_loss: 3.9518 - val_acc: 0.1728
Epoch 42/100
 - 30s - loss: 3.7939 - acc: 0.1882 - val_loss: 3.9662 - val_acc: 0.1621
Epoch 43/100
 - 30s - loss: 3.7917 - acc: 0.1883 - val_loss: 4.1513 - val_acc: 0.1590
Epoch 44/100
 - 30s - loss: 3.7997 - acc: 0.1869 - val_loss: 4.1038 - val_acc: 0.1563
Epoch 45/100
 - 30s - loss: 3.7883 - acc: 0.1900 - val_loss: 4.1159 - val_acc: 0.1504
Epoch 46/100
 - 30s - loss: 3.7980 - acc: 0.1864 - val_loss: 3.8307 - val_acc: 0.1846
Epoch 47/100
 - 30s - loss: 3.8086 - acc: 0.1860 - val_loss: 3.9260 - val_acc: 0.1612
Epoch 48/100
 - 30s - loss: 3.8094 - acc: 0.1855 - val_loss: 4.0558 - val_acc: 0.1468
Epoch 49/100
 - 30s - loss: 3.8258 - acc: 0.1824 - val_loss: 4.0549 - val_acc: 0.1584
Epoch 50/100
 - 30s - loss: 3.8323 - acc: 0.1824 - val_loss: 4.0183 - val_acc: 0.1581
Epoch 51/100
 - 30s - loss: 3.8402 - acc: 0.1800 - val_loss: 3.9311 - val_acc: 0.1613
Epoch 52/100
 - 30s - loss: 3.8438 - acc: 0.1819 - val_loss: 3.9831 - val_acc: 0.1613
Epoch 53/100
 - 30s - loss: 3.8514 - acc: 0.1786 - val_loss: 4.1615 - val_acc: 0.1481
Epoch 54/100
 - 30s - loss: 3.8747 - acc: 0.1765 - val_loss: 4.1132 - val_acc: 0.1419
Epoch 55/100
 - 30s - loss: 3.8833 - acc: 0.1759 - val_loss: 4.0240 - val_acc: 0.1706
Epoch 56/100
 - 30s - loss: 3.8915 - acc: 0.1717 - val_loss: 3.9766 - val_acc: 0.1611
Epoch 57/100
 - 30s - loss: 3.9133 - acc: 0.1713 - val_loss: 4.0328 - val_acc: 0.1459
Epoch 58/100
 - 30s - loss: 3.9350 - acc: 0.1681 - val_loss: 3.9746 - val_acc: 0.1551
Epoch 59/100
 - 30s - loss: 3.9546 - acc: 0.1644 - val_loss: 4.2069 - val_acc: 0.1292
Epoch 60/100
 - 30s - loss: 3.9599 - acc: 0.1636 - val_loss: 3.9200 - val_acc: 0.1652
Epoch 61/100
 - 30s - loss: 3.9812 - acc: 0.1610 - val_loss: 4.0618 - val_acc: 0.1436
Epoch 62/100
 - 30s - loss: 3.9961 - acc: 0.1585 - val_loss: 4.1119 - val_acc: 0.1500
Epoch 63/100
 - 30s - loss: 4.0150 - acc: 0.1568 - val_loss: 4.0819 - val_acc: 0.1397
Epoch 64/100
 - 30s - loss: 4.0114 - acc: 0.1563 - val_loss: 4.2011 - val_acc: 0.1418
Epoch 65/100
 - 30s - loss: 4.0372 - acc: 0.1523 - val_loss: 3.9860 - val_acc: 0.1534
Epoch 66/100
 - 30s - loss: 4.0323 - acc: 0.1526 - val_loss: 4.0346 - val_acc: 0.1470
Epoch 67/100
 - 30s - loss: 4.0595 - acc: 0.1483 - val_loss: 4.2992 - val_acc: 0.1427
Epoch 68/100
 - 30s - loss: 4.0342 - acc: 0.1494 - val_loss: 4.1355 - val_acc: 0.1390
Epoch 69/100
 - 30s - loss: 4.0271 - acc: 0.1516 - val_loss: 4.0932 - val_acc: 0.1395
Epoch 70/100
 - 30s - loss: 4.0041 - acc: 0.1541 - val_loss: 4.0612 - val_acc: 0.1589
Epoch 71/100
 - 30s - loss: 3.9991 - acc: 0.1546 - val_loss: 3.9451 - val_acc: 0.1579
Epoch 72/100
 - 30s - loss: 3.9881 - acc: 0.1571 - val_loss: 4.0478 - val_acc: 0.1449
Epoch 73/100
 - 30s - loss: 3.9834 - acc: 0.1554 - val_loss: 4.2132 - val_acc: 0.1382
Epoch 74/100
 - 30s - loss: 3.9869 - acc: 0.1559 - val_loss: 4.0317 - val_acc: 0.1482
Epoch 75/100
 - 30s - loss: 3.9872 - acc: 0.1558 - val_loss: 4.0783 - val_acc: 0.1487
Epoch 76/100
 - 30s - loss: 3.9845 - acc: 0.1569 - val_loss: 4.0788 - val_acc: 0.1389
Epoch 77/100
 - 30s - loss: 3.9887 - acc: 0.1564 - val_loss: 4.1075 - val_acc: 0.1456
Epoch 78/100
 - 30s - loss: 3.9988 - acc: 0.1561 - val_loss: 3.9533 - val_acc: 0.1577
Epoch 79/100
 - 30s - loss: 3.9857 - acc: 0.1572 - val_loss: 4.1179 - val_acc: 0.1558
Epoch 80/100
 - 30s - loss: 3.9918 - acc: 0.1548 - val_loss: 3.9366 - val_acc: 0.1546
Epoch 81/100
 - 30s - loss: 3.9959 - acc: 0.1556 - val_loss: 4.1718 - val_acc: 0.1319
Epoch 82/100
 - 30s - loss: 3.9922 - acc: 0.1563 - val_loss: 4.0877 - val_acc: 0.1457
Epoch 83/100
 - 30s - loss: 3.9896 - acc: 0.1557 - val_loss: 3.9623 - val_acc: 0.1540
Epoch 84/100
 - 30s - loss: 3.9979 - acc: 0.1534 - val_loss: 3.9666 - val_acc: 0.1624
Epoch 85/100
 - 30s - loss: 4.0096 - acc: 0.1518 - val_loss: 4.0547 - val_acc: 0.1461
Epoch 86/100
 - 30s - loss: 4.0104 - acc: 0.1535 - val_loss: 4.3176 - val_acc: 0.1406
Epoch 87/100
 - 30s - loss: 4.0041 - acc: 0.1523 - val_loss: 4.1327 - val_acc: 0.1336
Epoch 88/100
 - 30s - loss: 4.0057 - acc: 0.1540 - val_loss: 4.3024 - val_acc: 0.1319
Epoch 89/100
 - 30s - loss: 4.0182 - acc: 0.1514 - val_loss: 4.1664 - val_acc: 0.1291
Epoch 90/100
 - 30s - loss: 4.0265 - acc: 0.1519 - val_loss: 4.2116 - val_acc: 0.1406
Epoch 91/100
 - 30s - loss: 4.0305 - acc: 0.1494 - val_loss: 4.1566 - val_acc: 0.1335
Epoch 92/100
 - 30s - loss: 4.0333 - acc: 0.1478 - val_loss: 3.9571 - val_acc: 0.1518
Epoch 93/100
 - 30s - loss: 4.0430 - acc: 0.1486 - val_loss: 4.2421 - val_acc: 0.1113
Epoch 94/100
 - 30s - loss: 4.0501 - acc: 0.1471 - val_loss: 4.1550 - val_acc: 0.1318
Epoch 95/100
 - 30s - loss: 4.0581 - acc: 0.1471 - val_loss: 4.3188 - val_acc: 0.1136
Epoch 96/100
 - 30s - loss: 4.0482 - acc: 0.1465 - val_loss: 4.1610 - val_acc: 0.1339
Epoch 97/100
 - 30s - loss: 4.0624 - acc: 0.1464 - val_loss: 4.1080 - val_acc: 0.1537
Epoch 98/100
 - 30s - loss: 4.0580 - acc: 0.1443 - val_loss: 4.1127 - val_acc: 0.1357
Epoch 99/100
 - 30s - loss: 4.0685 - acc: 0.1432 - val_loss: 3.9667 - val_acc: 0.1481
Epoch 100/100
 - 30s - loss: 4.0647 - acc: 0.1420 - val_loss: 4.1242 - val_acc: 0.1361
training history:
{'val_loss': [4.372403938293457, 4.059356848144532, 4.071027815246582, 3.9744111122131347, 4.0974910507202145, 3.883178828048706, 3.822115325164795, 3.9146255966186523, 3.772340114593506, 3.9046195281982423, 3.9028535423278807, 3.784101866149902, 4.090625768661499, 3.894921347808838, 3.931666846847534, 3.8244038288116453, 4.133366815185547, 3.9442455024719236, 3.7856084941864014, 4.0021161895751955, 3.7958650115966797, 3.854100439453125, 3.71718462638855, 3.9453905220031737, 3.820384708404541, 4.045745596313477, 4.097747699737549, 4.0574453247070315, 3.7929400051116944, 3.993399419403076, 4.178121810913086, 3.963413362121582, 3.933165265274048, 3.9075338775634765, 3.9232712017059326, 3.8249745822906496, 3.9276818771362305, 3.9834689453125, 3.878412180709839, 3.87005849609375, 3.9517956672668455, 3.9661819160461427, 4.1512861141204835, 4.1037605846405025, 4.115856897735596, 3.830698275375366, 3.926048504638672, 4.055804753112793, 4.054922982025147, 4.018256165313721, 3.9310662925720217, 3.9830935279846194, 4.161454288864136, 4.113209194946289, 4.023951818084717, 3.976558491897583, 4.032800956726074, 3.9746411888122557, 4.206873036193848, 3.9200081615448, 4.061821127319336, 4.111861096191406, 4.081882012176513, 4.201138484954834, 3.9859950996398927, 4.034593607711792, 4.299190452957153, 4.1355028747558595, 4.093204098510742, 4.061157234954834, 3.9451093643188475, 4.0477635910034175, 4.213202450561523, 4.031664895629882, 4.078292329406739, 4.07879185256958, 4.107497644042969, 3.953279601287842, 4.1178750350952145, 3.93655647354126, 4.1717671905517575, 4.087675141906738, 3.96228934173584, 3.966599377441406, 4.054678399658203, 4.317628917694091, 4.1327202629089355, 4.302443696212769, 4.16636915435791, 4.211550178527832, 4.156648904418946, 3.957134468078613, 4.24206291809082, 4.155011456298828, 4.318750602722168, 4.160967656707764, 4.108036262512207, 4.112704364013672, 3.9667205764770506, 4.124192386245728], 'val_acc': [0.0899, 0.1269, 0.1397, 0.1476, 0.1487, 0.1668, 0.1732, 0.1678, 0.1858, 0.1817, 0.1702, 0.1831, 0.1445, 0.1679, 0.1645, 0.1802, 0.1535, 0.175, 0.19, 0.1639, 0.1869, 0.1754, 0.1971, 0.1729, 0.1825, 0.149, 0.1432, 0.1602, 0.1882, 0.1606, 0.1617, 0.1663, 0.1705, 0.1769, 0.1788, 0.1896, 0.1657, 0.1611, 0.1772, 0.1693, 0.1728, 0.1621, 0.159, 0.1563, 0.1504, 0.1846, 0.1612, 0.1468, 0.1584, 0.1581, 0.1613, 0.1613, 0.1481, 0.1419, 0.1706, 0.1611, 0.1459, 0.1551, 0.1292, 0.1652, 0.1436, 0.15, 0.1397, 0.1418, 0.1534, 0.147, 0.1427, 0.139, 0.1395, 0.1589, 0.1579, 0.1449, 0.1382, 0.1482, 0.1487, 0.1389, 0.1456, 0.1577, 0.1558, 0.1546, 0.1319, 0.1457, 0.154, 0.1624, 0.1461, 0.1406, 0.1336, 0.1319, 0.1291, 0.1406, 0.1335, 0.1518, 0.1113, 0.1318, 0.1136, 0.1339, 0.1537, 0.1357, 0.1481, 0.1361], 'loss': [4.900737463684082, 4.284291427307129, 4.052644371566773, 3.9293835260009766, 3.8553153993225098, 3.815531201324463, 3.776673065032959, 3.7574375619506837, 3.7363446810913086, 3.7260887641906737, 3.7112671453094483, 3.709199093322754, 3.7040836695098878, 3.6978789627075197, 3.6972669330596926, 3.694130205230713, 3.697148899459839, 3.695343579940796, 3.7053684466552737, 3.7010100872802734, 3.694360603790283, 3.6942781872558594, 3.6911443228149414, 3.702366006011963, 3.6946977793884277, 3.7055467652893066, 3.7098815090179444, 3.715812583770752, 3.723403420562744, 3.7238145238494873, 3.7329599647521974, 3.7355175564575194, 3.7373963987731935, 3.7512126750183103, 3.758731939086914, 3.7568484284973143, 3.7750724788665773, 3.7830833181762697, 3.783058091430664, 3.777053688964844, 3.784553046875, 3.793755053253174, 3.7917283003234865, 3.799644641647339, 3.7882064018249513, 3.7982341983032226, 3.8085254664611816, 3.809208599700928, 3.8260395140838623, 3.832349821243286, 3.8401421475219726, 3.8437648331451415, 3.8514346854400636, 3.874762204360962, 3.8833602738952635, 3.8913676179504395, 3.9133333883666994, 3.934912918395996, 3.9544887185668944, 3.959613791656494, 3.9812485538482667, 3.9961909727478027, 4.014917467651367, 4.0114003587341305, 4.037203939666748, 4.032413106536866, 4.05943768157959, 4.034167398681641, 4.026928530883789, 4.00414593536377, 3.9993229399871826, 3.988207144088745, 3.9832868745422365, 3.9869371031188963, 3.9872553984069823, 3.984573513946533, 3.9887021868896486, 3.998695219116211, 3.985796818084717, 3.9918129377746583, 3.9960048054504393, 3.9921704153442383, 3.989566602020264, 3.9979528215026856, 4.009578335876465, 4.010225928039551, 4.004038829040527, 4.005848010787964, 4.018099397888183, 4.026477888641358, 4.030829591522217, 4.033144900665283, 4.043026951751709, 4.049994372711182, 4.058024862518311, 4.048234218826294, 4.062493131713867, 4.0579897093200685, 4.068424370117188, 4.064629223937988], 'acc': [0.04272, 0.10953, 0.13953, 0.15732, 0.16945, 0.17758, 0.18318, 0.18692, 0.18998, 0.19391, 0.19668, 0.19473, 0.19592, 0.19962, 0.19853, 0.20012, 0.1989, 0.19967, 0.19697, 0.19724, 0.2004, 0.20065, 0.20066, 0.19978, 0.20081, 0.20136, 0.19867, 0.19989, 0.1988, 0.19749, 0.19579, 0.19618, 0.19444, 0.19451, 0.19381, 0.19355, 0.19001, 0.18975, 0.19019, 0.18934, 0.18971, 0.18819, 0.18834, 0.18687, 0.19004, 0.18632, 0.18596, 0.18553, 0.18238, 0.18241, 0.18005, 0.18191, 0.17856, 0.17647, 0.1759, 0.17174, 0.17125, 0.16816, 0.16436, 0.16361, 0.16098, 0.15848, 0.15683, 0.15632, 0.15233, 0.15257, 0.14825, 0.14941, 0.15159, 0.15409, 0.15454, 0.15705, 0.15537, 0.15592, 0.15574, 0.15691, 0.15639, 0.15606, 0.15721, 0.15483, 0.15559, 0.15628, 0.15572, 0.15338, 0.15182, 0.1535, 0.15233, 0.15399, 0.1514, 0.15194, 0.14936, 0.1478, 0.14859, 0.14705, 0.14715, 0.14649, 0.14635, 0.14428, 0.14318, 0.14199]}
Training time: 
3017.5502457618713
Evaluation results:  [4.128967700958252, 0.1353]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_21 (Conv2D)           (None, 58, 62, 48)        3072      
_________________________________________________________________
activation_13 (Activation)   (None, 58, 62, 48)        0         
_________________________________________________________________
max_pooling2d_13 (MaxPooling (None, 28, 30, 48)        0         
_________________________________________________________________
conv2d_22 (Conv2D)           (None, 24, 30, 128)       30848     
_________________________________________________________________
activation_14 (Activation)   (None, 24, 30, 128)       0         
_________________________________________________________________
max_pooling2d_14 (MaxPooling (None, 11, 14, 128)       0         
_________________________________________________________________
conv2d_23 (Conv2D)           (None, 9, 14, 192)        73920     
_________________________________________________________________
conv2d_24 (Conv2D)           (None, 7, 14, 192)        110784    
_________________________________________________________________
conv2d_25 (Conv2D)           (None, 5, 14, 128)        73856     
_________________________________________________________________
activation_15 (Activation)   (None, 5, 14, 128)        0         
_________________________________________________________________
max_pooling2d_15 (MaxPooling (None, 2, 6, 128)         0         
_________________________________________________________________
flatten_5 (Flatten)          (None, 1536)              0         
_________________________________________________________________
dense_13 (Dense)             (None, 2048)              3147776   
_________________________________________________________________
dense_14 (Dense)             (None, 2048)              4196352   
_________________________________________________________________
dense_15 (Dense)             (None, 200)               409800    
=================================================================
Total params: 8,046,408
Trainable params: 8,046,408
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
 - 28s - loss: 5.0500 - acc: 0.0267 - val_loss: 4.8200 - val_acc: 0.0432
Epoch 2/100
 - 27s - loss: 4.5811 - acc: 0.0754 - val_loss: 4.5463 - val_acc: 0.0761
Epoch 3/100
 - 27s - loss: 4.2258 - acc: 0.1216 - val_loss: 4.2348 - val_acc: 0.1171
Epoch 4/100
 - 27s - loss: 3.9528 - acc: 0.1606 - val_loss: 3.9946 - val_acc: 0.1546
Epoch 5/100
 - 27s - loss: 3.7308 - acc: 0.1934 - val_loss: 3.7011 - val_acc: 0.1971
Epoch 6/100
 - 27s - loss: 3.5483 - acc: 0.2228 - val_loss: 3.5827 - val_acc: 0.2156
Epoch 7/100
 - 27s - loss: 3.3909 - acc: 0.2470 - val_loss: 3.5634 - val_acc: 0.2206
Epoch 8/100
 - 27s - loss: 3.2459 - acc: 0.2728 - val_loss: 3.5348 - val_acc: 0.2266
Epoch 9/100
 - 27s - loss: 3.1027 - acc: 0.2976 - val_loss: 3.4402 - val_acc: 0.2419
Epoch 10/100
 - 27s - loss: 2.9717 - acc: 0.3182 - val_loss: 3.4867 - val_acc: 0.2363
Epoch 11/100
 - 27s - loss: 2.8346 - acc: 0.3441 - val_loss: 3.3534 - val_acc: 0.2629
Epoch 12/100
 - 27s - loss: 2.7007 - acc: 0.3682 - val_loss: 3.4040 - val_acc: 0.2660
Epoch 13/100
 - 27s - loss: 2.5583 - acc: 0.3943 - val_loss: 3.4191 - val_acc: 0.2603
Epoch 14/100
 - 27s - loss: 2.4177 - acc: 0.4211 - val_loss: 3.5078 - val_acc: 0.2613
Epoch 15/100
 - 27s - loss: 2.2667 - acc: 0.4503 - val_loss: 3.5605 - val_acc: 0.2657
Epoch 16/100
 - 27s - loss: 2.1145 - acc: 0.4792 - val_loss: 3.5878 - val_acc: 0.2663
Epoch 17/100
 - 27s - loss: 1.9558 - acc: 0.5103 - val_loss: 3.7279 - val_acc: 0.2599
Epoch 18/100
 - 27s - loss: 1.7987 - acc: 0.5439 - val_loss: 3.8446 - val_acc: 0.2529
Epoch 19/100
 - 27s - loss: 1.6308 - acc: 0.5796 - val_loss: 4.0805 - val_acc: 0.2429
Epoch 20/100
 - 27s - loss: 1.4776 - acc: 0.6128 - val_loss: 4.3854 - val_acc: 0.2334
Epoch 21/100
 - 27s - loss: 1.3248 - acc: 0.6484 - val_loss: 4.4600 - val_acc: 0.2507
Epoch 22/100
 - 27s - loss: 1.1775 - acc: 0.6814 - val_loss: 4.6223 - val_acc: 0.2385
Epoch 23/100
 - 27s - loss: 1.0382 - acc: 0.7129 - val_loss: 4.7534 - val_acc: 0.2489
Epoch 24/100
 - 27s - loss: 0.9194 - acc: 0.7447 - val_loss: 4.9632 - val_acc: 0.2516
Epoch 25/100
 - 27s - loss: 0.8089 - acc: 0.7697 - val_loss: 5.2042 - val_acc: 0.2376
Epoch 26/100
 - 27s - loss: 0.7060 - acc: 0.7971 - val_loss: 5.4516 - val_acc: 0.2427
Epoch 27/100
 - 27s - loss: 0.6275 - acc: 0.8181 - val_loss: 5.8512 - val_acc: 0.2345
Epoch 28/100
 - 27s - loss: 0.5510 - acc: 0.8385 - val_loss: 5.8736 - val_acc: 0.2325
Epoch 29/100
 - 27s - loss: 0.4984 - acc: 0.8518 - val_loss: 6.1389 - val_acc: 0.2357
Epoch 30/100
 - 27s - loss: 0.4409 - acc: 0.8685 - val_loss: 6.1303 - val_acc: 0.2340
Epoch 31/100
 - 27s - loss: 0.4015 - acc: 0.8797 - val_loss: 6.4097 - val_acc: 0.2335
Epoch 32/100
 - 27s - loss: 0.3613 - acc: 0.8905 - val_loss: 6.4183 - val_acc: 0.2411
Epoch 33/100
 - 27s - loss: 0.3199 - acc: 0.9028 - val_loss: 6.6608 - val_acc: 0.2371
Epoch 34/100
 - 27s - loss: 0.3125 - acc: 0.9050 - val_loss: 6.8770 - val_acc: 0.2352
Epoch 35/100
 - 27s - loss: 0.2757 - acc: 0.9157 - val_loss: 6.9201 - val_acc: 0.2384
Epoch 36/100
 - 27s - loss: 0.2614 - acc: 0.9201 - val_loss: 6.9581 - val_acc: 0.2366
Epoch 37/100
 - 27s - loss: 0.2282 - acc: 0.9293 - val_loss: 7.0673 - val_acc: 0.2392
Epoch 38/100
 - 27s - loss: 0.2274 - acc: 0.9311 - val_loss: 7.2668 - val_acc: 0.2399
Epoch 39/100
 - 27s - loss: 0.2040 - acc: 0.9366 - val_loss: 7.3135 - val_acc: 0.2409
Epoch 40/100
 - 27s - loss: 0.1910 - acc: 0.9416 - val_loss: 7.2633 - val_acc: 0.2397
Epoch 41/100
 - 27s - loss: 0.1849 - acc: 0.9437 - val_loss: 7.4872 - val_acc: 0.2422
Epoch 42/100
 - 27s - loss: 0.1719 - acc: 0.9471 - val_loss: 7.5879 - val_acc: 0.2308
Epoch 43/100
 - 27s - loss: 0.1690 - acc: 0.9481 - val_loss: 7.4776 - val_acc: 0.2455
Epoch 44/100
 - 27s - loss: 0.1665 - acc: 0.9490 - val_loss: 7.5783 - val_acc: 0.2359
Epoch 45/100
 - 27s - loss: 0.1471 - acc: 0.9549 - val_loss: 7.6123 - val_acc: 0.2452
Epoch 46/100
 - 27s - loss: 0.1389 - acc: 0.9575 - val_loss: 7.7413 - val_acc: 0.2310
Epoch 47/100
 - 27s - loss: 0.1249 - acc: 0.9613 - val_loss: 7.7535 - val_acc: 0.2401
Epoch 48/100
 - 27s - loss: 0.1193 - acc: 0.9635 - val_loss: 7.8710 - val_acc: 0.2417
Epoch 49/100
 - 27s - loss: 0.1151 - acc: 0.9654 - val_loss: 7.7928 - val_acc: 0.2434
Epoch 50/100
 - 27s - loss: 0.1193 - acc: 0.9631 - val_loss: 7.7673 - val_acc: 0.2403
Epoch 51/100
 - 27s - loss: 0.1087 - acc: 0.9674 - val_loss: 7.9901 - val_acc: 0.2462
Epoch 52/100
 - 27s - loss: 0.1102 - acc: 0.9667 - val_loss: 7.9248 - val_acc: 0.2412
Epoch 53/100
 - 27s - loss: 0.1044 - acc: 0.9679 - val_loss: 7.9774 - val_acc: 0.2423
Epoch 54/100
 - 27s - loss: 0.1167 - acc: 0.9647 - val_loss: 7.9157 - val_acc: 0.2438
Epoch 55/100
 - 27s - loss: 0.0873 - acc: 0.9727 - val_loss: 7.9821 - val_acc: 0.2453
Epoch 56/100
 - 27s - loss: 0.0960 - acc: 0.9705 - val_loss: 8.2257 - val_acc: 0.2366
Epoch 57/100
 - 27s - loss: 0.0955 - acc: 0.9708 - val_loss: 8.0856 - val_acc: 0.2405
Epoch 58/100
 - 27s - loss: 0.0876 - acc: 0.9733 - val_loss: 8.1708 - val_acc: 0.2387
Epoch 59/100
 - 27s - loss: 0.0923 - acc: 0.9720 - val_loss: 8.2210 - val_acc: 0.2461
Epoch 60/100
 - 27s - loss: 0.0855 - acc: 0.9745 - val_loss: 8.1571 - val_acc: 0.2404
Epoch 61/100
 - 27s - loss: 0.0750 - acc: 0.9777 - val_loss: 8.2459 - val_acc: 0.2469
Epoch 62/100
 - 27s - loss: 0.0804 - acc: 0.9757 - val_loss: 8.2351 - val_acc: 0.2411
Epoch 63/100
 - 27s - loss: 0.0714 - acc: 0.9784 - val_loss: 8.1984 - val_acc: 0.2412
Epoch 64/100
 - 27s - loss: 0.0720 - acc: 0.9788 - val_loss: 8.4143 - val_acc: 0.2399
Epoch 65/100
 - 27s - loss: 0.0792 - acc: 0.9763 - val_loss: 8.3478 - val_acc: 0.2399
Epoch 66/100
 - 27s - loss: 0.0859 - acc: 0.9743 - val_loss: 8.3333 - val_acc: 0.2442
Epoch 67/100
 - 27s - loss: 0.0805 - acc: 0.9760 - val_loss: 8.2946 - val_acc: 0.2450
Epoch 68/100
 - 27s - loss: 0.0675 - acc: 0.9794 - val_loss: 8.3430 - val_acc: 0.2399
Epoch 69/100
 - 27s - loss: 0.0609 - acc: 0.9821 - val_loss: 8.5276 - val_acc: 0.2445
Epoch 70/100
 - 27s - loss: 0.0694 - acc: 0.9793 - val_loss: 8.5073 - val_acc: 0.2377
Epoch 71/100
 - 27s - loss: 0.0697 - acc: 0.9787 - val_loss: 8.4993 - val_acc: 0.2403
Epoch 72/100
 - 27s - loss: 0.0652 - acc: 0.9806 - val_loss: 8.4726 - val_acc: 0.2457
Epoch 73/100
 - 27s - loss: 0.0732 - acc: 0.9777 - val_loss: 8.3635 - val_acc: 0.2382
Epoch 74/100
 - 27s - loss: 0.0608 - acc: 0.9818 - val_loss: 8.6416 - val_acc: 0.2438
Epoch 75/100
 - 27s - loss: 0.0565 - acc: 0.9835 - val_loss: 8.6239 - val_acc: 0.2378
Epoch 76/100
 - 27s - loss: 0.0548 - acc: 0.9838 - val_loss: 8.6756 - val_acc: 0.2487
Epoch 77/100
 - 27s - loss: 0.0635 - acc: 0.9808 - val_loss: 8.6822 - val_acc: 0.2344
Epoch 78/100
 - 27s - loss: 0.0651 - acc: 0.9801 - val_loss: 8.5820 - val_acc: 0.2347
Epoch 79/100
 - 27s - loss: 0.0585 - acc: 0.9824 - val_loss: 8.7321 - val_acc: 0.2394
Epoch 80/100
 - 27s - loss: 0.0663 - acc: 0.9807 - val_loss: 8.5906 - val_acc: 0.2359
Epoch 81/100
 - 27s - loss: 0.0534 - acc: 0.9843 - val_loss: 8.5525 - val_acc: 0.2455
Epoch 82/100
 - 27s - loss: 0.0623 - acc: 0.9814 - val_loss: 8.6622 - val_acc: 0.2395
Epoch 83/100
 - 27s - loss: 0.0563 - acc: 0.9832 - val_loss: 8.7063 - val_acc: 0.2401
Epoch 84/100
 - 27s - loss: 0.0573 - acc: 0.9829 - val_loss: 8.7256 - val_acc: 0.2397
Epoch 85/100
 - 27s - loss: 0.0523 - acc: 0.9847 - val_loss: 8.6250 - val_acc: 0.2418
Epoch 86/100
 - 27s - loss: 0.0490 - acc: 0.9852 - val_loss: 8.6270 - val_acc: 0.2493
Epoch 87/100
 - 27s - loss: 0.0619 - acc: 0.9822 - val_loss: 8.6765 - val_acc: 0.2357
Epoch 88/100
 - 27s - loss: 0.0581 - acc: 0.9829 - val_loss: 8.8093 - val_acc: 0.2375
Epoch 89/100
 - 27s - loss: 0.0535 - acc: 0.9839 - val_loss: 8.9305 - val_acc: 0.2343
Epoch 90/100
 - 27s - loss: 0.0527 - acc: 0.9844 - val_loss: 8.7253 - val_acc: 0.2410
Epoch 91/100
 - 27s - loss: 0.0465 - acc: 0.9862 - val_loss: 8.7894 - val_acc: 0.2419
Epoch 92/100
 - 27s - loss: 0.0545 - acc: 0.9834 - val_loss: 8.7271 - val_acc: 0.2421
Epoch 93/100
 - 27s - loss: 0.0485 - acc: 0.9857 - val_loss: 8.7045 - val_acc: 0.2393
Epoch 94/100
 - 27s - loss: 0.0521 - acc: 0.9847 - val_loss: 8.7383 - val_acc: 0.2466
Epoch 95/100
 - 27s - loss: 0.0471 - acc: 0.9865 - val_loss: 8.8072 - val_acc: 0.2395
Epoch 96/100
 - 27s - loss: 0.0380 - acc: 0.9887 - val_loss: 8.8696 - val_acc: 0.2396
Epoch 97/100
 - 27s - loss: 0.0458 - acc: 0.9868 - val_loss: 8.8228 - val_acc: 0.2482
Epoch 98/100
 - 27s - loss: 0.0377 - acc: 0.9894 - val_loss: 8.9962 - val_acc: 0.2420
Epoch 99/100
 - 27s - loss: 0.0447 - acc: 0.9869 - val_loss: 8.8826 - val_acc: 0.2418
Epoch 100/100
 - 27s - loss: 0.0449 - acc: 0.9869 - val_loss: 8.7250 - val_acc: 0.2473
training history:
{'val_loss': [4.820010177612304, 4.546293937683106, 4.234794962310791, 3.9946328201293944, 3.701051197433472, 3.582655167388916, 3.5634176956176757, 3.534814028930664, 3.4402376335144043, 3.486700937652588, 3.353419945526123, 3.4039518241882325, 3.419052380371094, 3.507848142623901, 3.5604503356933592, 3.5878218856811523, 3.727852201080322, 3.8446303108215334, 4.080490505981445, 4.385433839416504, 4.459952153015137, 4.622289131164551, 4.753383723449707, 4.963162421417237, 5.204236224365235, 5.451607302856445, 5.851233706665039, 5.873612712097168, 6.138912892150879, 6.130323896789551, 6.409668992614746, 6.418253854370117, 6.660825088500976, 6.876961874389648, 6.920111912536621, 6.95807522277832, 7.067348236083984, 7.266848147583008, 7.313459872436524, 7.263262940979004, 7.487175779724121, 7.587859938049316, 7.477649163818359, 7.578279562377929, 7.612285302734375, 7.741309063720703, 7.753466622924805, 7.871017518615723, 7.792776870727539, 7.767305464172363, 7.990090832519531, 7.924757699584961, 7.977370359802246, 7.915715863037109, 7.98209193572998, 8.22570598449707, 8.085631535339356, 8.170807630920411, 8.221038414001464, 8.157058389282227, 8.245924639892579, 8.235055477905274, 8.19841093597412, 8.414257699584962, 8.347791146850586, 8.33329213256836, 8.294568103027343, 8.343029244995117, 8.527589666748048, 8.50727825012207, 8.49926304321289, 8.472584674072266, 8.363545530700684, 8.64162396850586, 8.623867291259765, 8.675616487121582, 8.68220998840332, 8.581954919433594, 8.732078569030762, 8.590633697509766, 8.552475057983399, 8.66221531677246, 8.706328756713868, 8.725619702148437, 8.624979763793945, 8.626997857666016, 8.676473944091796, 8.809297145080567, 8.930516833496094, 8.725291018676758, 8.789448754882812, 8.727119696044921, 8.704504290771485, 8.738255403137208, 8.80724796447754, 8.869573484802245, 8.822813806152343, 8.996213006591796, 8.882588796997071, 8.725011857604981], 'val_acc': [0.0432, 0.0761, 0.1171, 0.1546, 0.1971, 0.2156, 0.2206, 0.2266, 0.2419, 0.2363, 0.2629, 0.266, 0.2603, 0.2613, 0.2657, 0.2663, 0.2599, 0.2529, 0.2429, 0.2334, 0.2507, 0.2385, 0.2489, 0.2516, 0.2376, 0.2427, 0.2345, 0.2325, 0.2357, 0.234, 0.2335, 0.2411, 0.2371, 0.2352, 0.2384, 0.2366, 0.2392, 0.2399, 0.2409, 0.2397, 0.2422, 0.2308, 0.2455, 0.2359, 0.2452, 0.231, 0.2401, 0.2417, 0.2434, 0.2403, 0.2462, 0.2412, 0.2423, 0.2438, 0.2453, 0.2366, 0.2405, 0.2387, 0.2461, 0.2404, 0.2469, 0.2411, 0.2412, 0.2399, 0.2399, 0.2442, 0.245, 0.2399, 0.2445, 0.2377, 0.2403, 0.2457, 0.2382, 0.2438, 0.2378, 0.2487, 0.2344, 0.2347, 0.2394, 0.2359, 0.2455, 0.2395, 0.2401, 0.2397, 0.2418, 0.2493, 0.2357, 0.2375, 0.2343, 0.241, 0.2419, 0.2421, 0.2393, 0.2466, 0.2395, 0.2396, 0.2482, 0.242, 0.2418, 0.2473], 'loss': [5.050014463806153, 4.581159519195556, 4.22597594581604, 3.952882698440552, 3.7308442636108397, 3.5480776354980468, 3.391077943572998, 3.245789020614624, 3.102585838317871, 2.9717808488464357, 2.834801053161621, 2.7005615967559815, 2.5586414461898803, 2.417734557723999, 2.2666382536315917, 2.1142142667388915, 1.955812167930603, 1.7985842599487305, 1.630885578918457, 1.4776533011627198, 1.324825284461975, 1.1775530318641663, 1.0381685151481628, 0.9195284003829957, 0.808810517578125, 0.7058961174583435, 0.6273440730142593, 0.5511097353744507, 0.4984319267272949, 0.4410085389089584, 0.40158930606365206, 0.3613391192626953, 0.3199523986577988, 0.3124494399905205, 0.275732317135334, 0.2613191637349129, 0.2282053527355194, 0.227433147033453, 0.20402683379173278, 0.1910372203040123, 0.1848094558775425, 0.17190136188089847, 0.16895624439567328, 0.16649014739096166, 0.14716768506646155, 0.13880986469626427, 0.1248788512301445, 0.11909151905477047, 0.11504242246806622, 0.11930113132983446, 0.10871854084819556, 0.11022531975328922, 0.1044254875138402, 0.11666533132970333, 0.08733756604582071, 0.09606079208314419, 0.0954849253590405, 0.08760577741995454, 0.09231817321270704, 0.0853734642509371, 0.07505182310588658, 0.08036434510990978, 0.07137632206540555, 0.0719704375289753, 0.07919171504613012, 0.08586396508619189, 0.0805130120421946, 0.06753761355433613, 0.060838310830704866, 0.06936871636487543, 0.0697245097450912, 0.06522505505613983, 0.07317677714854479, 0.06077723051019013, 0.05651521029762924, 0.054776169482916595, 0.06350833133727313, 0.06509508392170071, 0.05848496714618057, 0.06635355624899268, 0.05330249275103211, 0.062261828288137915, 0.0563273037141189, 0.05722641794409603, 0.052292361374441536, 0.04902498396020383, 0.061785359522849324, 0.058042483079489324, 0.0535210088533815, 0.0527579152671434, 0.04655361126487143, 0.05445299408569001, 0.04851220794053748, 0.05213660632283427, 0.047159015735061836, 0.037975541889993476, 0.045805953872194514, 0.03774758590206504, 0.044668134921565655, 0.044955228495551273], 'acc': [0.02669, 0.07535, 0.12163, 0.16059, 0.19342, 0.22285, 0.24703, 0.27283, 0.29758, 0.31815, 0.34411, 0.36818, 0.39427, 0.42107, 0.45033, 0.4793, 0.5103, 0.54398, 0.57956, 0.61281, 0.64838, 0.68142, 0.71289, 0.74471, 0.76975, 0.79715, 0.81812, 0.83852, 0.85175, 0.86852, 0.87964, 0.89052, 0.90273, 0.90502, 0.91566, 0.92012, 0.92932, 0.93106, 0.93662, 0.94161, 0.94369, 0.94705, 0.94806, 0.94901, 0.9549, 0.95751, 0.96131, 0.96352, 0.96542, 0.96311, 0.96734, 0.96665, 0.96794, 0.96473, 0.97273, 0.97045, 0.97079, 0.97328, 0.97201, 0.9745, 0.9777, 0.97575, 0.97838, 0.97881, 0.97631, 0.9743, 0.97597, 0.97944, 0.98208, 0.97935, 0.97872, 0.98062, 0.97768, 0.98177, 0.98349, 0.98384, 0.98083, 0.98015, 0.98236, 0.98065, 0.98434, 0.98139, 0.98324, 0.98292, 0.98466, 0.98521, 0.9822, 0.98289, 0.98386, 0.98438, 0.98622, 0.98337, 0.98568, 0.98465, 0.98653, 0.98869, 0.9868, 0.98936, 0.98695, 0.98689]}
Training time: 
2681.4110913276672
Evaluation results:  [8.73494294128418, 0.2448]
