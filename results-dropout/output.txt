trainingset size: 100000
validationset size: 10000
input_dims:  (64, 64, 3)
nb_labels:  200
nb_labels: 200
x_train.shape  (100000, 64, 64, 3)
y_train.shape  (100000, 200)
x_test.shape  (10000, 64, 64, 3)
y_test.shape  (10000, 200)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 58, 61, 48)        4080      
_________________________________________________________________
activation_1 (Activation)    (None, 58, 61, 48)        0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 28, 30, 48)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 24, 30, 128)       30848     
_________________________________________________________________
activation_2 (Activation)    (None, 24, 30, 128)       0         
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 11, 14, 128)       0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 9, 14, 192)        73920     
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 7, 14, 192)        110784    
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 5, 14, 128)        73856     
_________________________________________________________________
activation_3 (Activation)    (None, 5, 14, 128)        0         
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 2, 6, 128)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 1536)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 2048)              3147776   
_________________________________________________________________
dense_2 (Dense)              (None, 2048)              4196352   
_________________________________________________________________
dense_3 (Dense)              (None, 200)               409800    
=================================================================
Total params: 8,047,416
Trainable params: 8,047,416
Non-trainable params: 0
_________________________________________________________________
Epoch 1/50
 - 37s - loss: 5.0444 - acc: 0.0280 - val_loss: 4.7450 - val_acc: 0.0601
Epoch 2/50
 - 28s - loss: 4.5631 - acc: 0.0788 - val_loss: 4.4844 - val_acc: 0.0851
Epoch 3/50
 - 28s - loss: 4.1999 - acc: 0.1269 - val_loss: 4.1210 - val_acc: 0.1377
Epoch 4/50
 - 28s - loss: 3.9190 - acc: 0.1662 - val_loss: 3.9304 - val_acc: 0.1629
Epoch 5/50
 - 28s - loss: 3.6709 - acc: 0.2033 - val_loss: 3.8414 - val_acc: 0.1840
Epoch 6/50
 - 28s - loss: 3.4455 - acc: 0.2390 - val_loss: 3.6806 - val_acc: 0.2073
Epoch 7/50
 - 28s - loss: 3.2287 - acc: 0.2753 - val_loss: 3.7290 - val_acc: 0.2097
Epoch 8/50
 - 28s - loss: 3.0026 - acc: 0.3128 - val_loss: 3.6316 - val_acc: 0.2218
Epoch 9/50
 - 28s - loss: 2.7662 - acc: 0.3533 - val_loss: 3.6903 - val_acc: 0.2241
Epoch 10/50
 - 28s - loss: 2.5083 - acc: 0.3997 - val_loss: 3.9005 - val_acc: 0.2124
Epoch 11/50
 - 28s - loss: 2.2125 - acc: 0.4571 - val_loss: 4.0117 - val_acc: 0.2122
Epoch 12/50
 - 28s - loss: 1.8937 - acc: 0.5225 - val_loss: 4.2506 - val_acc: 0.2029
Epoch 13/50
 - 28s - loss: 1.5749 - acc: 0.5876 - val_loss: 4.6034 - val_acc: 0.2078
Epoch 14/50
 - 28s - loss: 1.2460 - acc: 0.6610 - val_loss: 5.0318 - val_acc: 0.2007
Epoch 15/50
 - 28s - loss: 0.9691 - acc: 0.7290 - val_loss: 5.5516 - val_acc: 0.1954
Epoch 16/50
 - 28s - loss: 0.7528 - acc: 0.7826 - val_loss: 5.9791 - val_acc: 0.1921
Epoch 17/50
 - 28s - loss: 0.5878 - acc: 0.8266 - val_loss: 6.4920 - val_acc: 0.1871
Epoch 18/50
 - 28s - loss: 0.4486 - acc: 0.8668 - val_loss: 6.8531 - val_acc: 0.1956
Epoch 19/50
 - 28s - loss: 0.3743 - acc: 0.8874 - val_loss: 7.0287 - val_acc: 0.1896
Epoch 20/50
 - 28s - loss: 0.3011 - acc: 0.9100 - val_loss: 7.3830 - val_acc: 0.1832
Epoch 21/50
 - 28s - loss: 0.2620 - acc: 0.9214 - val_loss: 7.7511 - val_acc: 0.1807
Epoch 22/50
 - 28s - loss: 0.2309 - acc: 0.9303 - val_loss: 7.5649 - val_acc: 0.1975
Epoch 23/50
 - 28s - loss: 0.1849 - acc: 0.9453 - val_loss: 7.8919 - val_acc: 0.1869
Epoch 24/50
 - 27s - loss: 0.1735 - acc: 0.9482 - val_loss: 7.8943 - val_acc: 0.1938
Epoch 25/50
 - 28s - loss: 0.1467 - acc: 0.9554 - val_loss: 8.0331 - val_acc: 0.1875
Epoch 26/50
 - 28s - loss: 0.1409 - acc: 0.9575 - val_loss: 8.3775 - val_acc: 0.1887
Epoch 27/50
 - 28s - loss: 0.1295 - acc: 0.9606 - val_loss: 8.2918 - val_acc: 0.1942
Epoch 28/50
 - 28s - loss: 0.1209 - acc: 0.9642 - val_loss: 8.1522 - val_acc: 0.1928
Epoch 29/50
 - 28s - loss: 0.1123 - acc: 0.9665 - val_loss: 8.3941 - val_acc: 0.1887
Epoch 30/50
 - 28s - loss: 0.0880 - acc: 0.9735 - val_loss: 8.6416 - val_acc: 0.1966
Epoch 31/50
 - 28s - loss: 0.0786 - acc: 0.9761 - val_loss: 8.6007 - val_acc: 0.1935
Epoch 32/50
 - 28s - loss: 0.0843 - acc: 0.9746 - val_loss: 8.5857 - val_acc: 0.1950
Epoch 33/50
 - 28s - loss: 0.0926 - acc: 0.9728 - val_loss: 8.5610 - val_acc: 0.1948
Epoch 34/50
 - 28s - loss: 0.0783 - acc: 0.9763 - val_loss: 8.6957 - val_acc: 0.1952
Epoch 35/50
 - 28s - loss: 0.0694 - acc: 0.9798 - val_loss: 8.7396 - val_acc: 0.1972
Epoch 36/50
 - 28s - loss: 0.0702 - acc: 0.9794 - val_loss: 8.8602 - val_acc: 0.1999
Epoch 37/50
 - 28s - loss: 0.0567 - acc: 0.9833 - val_loss: 8.8775 - val_acc: 0.1944
Epoch 38/50
 - 28s - loss: 0.0491 - acc: 0.9859 - val_loss: 8.8974 - val_acc: 0.1939
Epoch 39/50
 - 28s - loss: 0.0560 - acc: 0.9837 - val_loss: 8.8178 - val_acc: 0.1951
Epoch 40/50
 - 28s - loss: 0.0526 - acc: 0.9848 - val_loss: 8.9206 - val_acc: 0.1982
Epoch 41/50
 - 28s - loss: 0.0471 - acc: 0.9865 - val_loss: 9.0105 - val_acc: 0.1956
Epoch 42/50
 - 28s - loss: 0.0687 - acc: 0.9800 - val_loss: 8.9111 - val_acc: 0.1996
Epoch 43/50
 - 28s - loss: 0.0555 - acc: 0.9841 - val_loss: 9.4021 - val_acc: 0.1843
Epoch 44/50
 - 28s - loss: 0.0627 - acc: 0.9818 - val_loss: 9.0661 - val_acc: 0.1988
Epoch 45/50
 - 28s - loss: 0.0380 - acc: 0.9895 - val_loss: 9.1572 - val_acc: 0.2024
Epoch 46/50
 - 28s - loss: 0.0360 - acc: 0.9897 - val_loss: 9.1665 - val_acc: 0.2010
Epoch 47/50
 - 28s - loss: 0.0391 - acc: 0.9889 - val_loss: 9.2123 - val_acc: 0.2019
Epoch 48/50
 - 28s - loss: 0.0447 - acc: 0.9872 - val_loss: 9.2335 - val_acc: 0.1985
Epoch 49/50
 - 28s - loss: 0.0478 - acc: 0.9865 - val_loss: 9.3629 - val_acc: 0.1940
Epoch 50/50
 - 28s - loss: 0.0431 - acc: 0.9874 - val_loss: 9.2881 - val_acc: 0.1924
training history:
{'val_loss': [4.744990445709228, 4.484352420043946, 4.120990644073486, 3.9304207023620608, 3.8414363357543944, 3.6805571495056153, 3.7289746719360353, 3.631604591369629, 3.6903263191223146, 3.9004774269104003, 4.011737605285645, 4.2505895675659175, 4.6034272720336915, 5.031834729003906, 5.551550379943848, 5.979129779052735, 6.491974739074707, 6.8531199607849125, 7.028681021118164, 7.3830209129333495, 7.751129412841797, 7.5649321746826175, 7.891903189086914, 7.894304525756836, 8.033091720581055, 8.377459490966796, 8.291839147949219, 8.152176853179931, 8.394106643676757, 8.641550915527343, 8.600689471435548, 8.585673217773438, 8.56100660095215, 8.695734396362305, 8.739566604614257, 8.860174392700195, 8.877494328308105, 8.897381567382812, 8.817750814819336, 8.920574124145508, 9.01050163269043, 8.91107073059082, 9.402136672973633, 9.066107977294921, 9.157168566894532, 9.166513531494141, 9.212329278564454, 9.233548092651366, 9.362897799682617, 9.28814942626953], 'val_acc': [0.0601, 0.0851, 0.1377, 0.1629, 0.184, 0.2073, 0.2097, 0.2218, 0.2241, 0.2124, 0.2122, 0.2029, 0.2078, 0.2007, 0.1954, 0.1921, 0.1871, 0.1956, 0.1896, 0.1832, 0.1807, 0.1975, 0.1869, 0.1938, 0.1875, 0.1887, 0.1942, 0.1928, 0.1887, 0.1966, 0.1935, 0.195, 0.1948, 0.1952, 0.1972, 0.1999, 0.1944, 0.1939, 0.1951, 0.1982, 0.1956, 0.1996, 0.1843, 0.1988, 0.2024, 0.201, 0.2019, 0.1985, 0.194, 0.1924], 'loss': [5.044303400726318, 4.563054614868164, 4.1999502780151365, 3.918996164703369, 3.6709158601379395, 3.4454870230865478, 3.228903501281738, 3.002653437042236, 2.7659072761535644, 2.5081572016906737, 2.2125372357940676, 1.8935414996337891, 1.5749884206771851, 1.2460419483947753, 0.969123281917572, 0.7528253473472595, 0.5877788356399536, 0.44869226566314696, 0.37415169835567474, 0.30112521856188773, 0.2619787637209892, 0.23094270106077194, 0.1848077799117565, 0.17348666516542435, 0.1466568186557293, 0.14087634134441615, 0.12946385282218456, 0.12087156922668219, 0.11232192593872548, 0.08799414605230094, 0.07857709907054901, 0.08427353545352817, 0.09262498566262424, 0.07827285606320947, 0.06942400874257088, 0.0701926336311549, 0.05671448396362364, 0.049136960841454566, 0.05603411765182391, 0.052617228915207086, 0.04710558472154662, 0.06875077110227197, 0.055560795745775104, 0.06267999440483749, 0.03796871492665261, 0.03600010566540062, 0.03908734440516215, 0.044722164582721886, 0.04783237766360864, 0.043063173610325904], 'acc': [0.028, 0.07883, 0.1269, 0.16627, 0.20332, 0.23905, 0.27526, 0.31276, 0.35331, 0.39977, 0.45705, 0.52263, 0.58753, 0.661, 0.72906, 0.78256, 0.82657, 0.86678, 0.88746, 0.91002, 0.92141, 0.93031, 0.94529, 0.94819, 0.95539, 0.95747, 0.96058, 0.96415, 0.96645, 0.97351, 0.97613, 0.97462, 0.97282, 0.97627, 0.97979, 0.97937, 0.98329, 0.9859, 0.98371, 0.98481, 0.98649, 0.97996, 0.98406, 0.9818, 0.98945, 0.98974, 0.98892, 0.9872, 0.9865, 0.98736]}
Training time: 
1390.0757818222046
Evaluation results:  [9.288149301147461, 0.1924]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_6 (Conv2D)            (None, 58, 61, 48)        4080      
_________________________________________________________________
activation_4 (Activation)    (None, 58, 61, 48)        0         
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 28, 30, 48)        0         
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 24, 30, 128)       30848     
_________________________________________________________________
activation_5 (Activation)    (None, 24, 30, 128)       0         
_________________________________________________________________
max_pooling2d_5 (MaxPooling2 (None, 11, 14, 128)       0         
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 9, 14, 192)        73920     
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 7, 14, 192)        110784    
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 5, 14, 128)        73856     
_________________________________________________________________
activation_6 (Activation)    (None, 5, 14, 128)        0         
_________________________________________________________________
max_pooling2d_6 (MaxPooling2 (None, 2, 6, 128)         0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 1536)              0         
_________________________________________________________________
dense_4 (Dense)              (None, 2048)              3147776   
_________________________________________________________________
dropout_1 (Dropout)          (None, 2048)              0         
_________________________________________________________________
dense_5 (Dense)              (None, 2048)              4196352   
_________________________________________________________________
dropout_2 (Dropout)          (None, 2048)              0         
_________________________________________________________________
dense_6 (Dense)              (None, 200)               409800    
=================================================================
Total params: 8,047,416
Trainable params: 8,047,416
Non-trainable params: 0
_________________________________________________________________
Epoch 1/50
 - 28s - loss: 5.1492 - acc: 0.0167 - val_loss: 4.9066 - val_acc: 0.0414
Epoch 2/50
 - 28s - loss: 4.7949 - acc: 0.0491 - val_loss: 4.5330 - val_acc: 0.0763
Epoch 3/50
 - 28s - loss: 4.4833 - acc: 0.0851 - val_loss: 4.2267 - val_acc: 0.1168
Epoch 4/50
 - 28s - loss: 4.2167 - acc: 0.1191 - val_loss: 4.0549 - val_acc: 0.1420
Epoch 5/50
 - 28s - loss: 4.0050 - acc: 0.1482 - val_loss: 3.8604 - val_acc: 0.1677
Epoch 6/50
 - 28s - loss: 3.8253 - acc: 0.1745 - val_loss: 3.7326 - val_acc: 0.1909
Epoch 7/50
 - 28s - loss: 3.6768 - acc: 0.1989 - val_loss: 3.6142 - val_acc: 0.2100
Epoch 8/50
 - 28s - loss: 3.5306 - acc: 0.2202 - val_loss: 3.6399 - val_acc: 0.2106
Epoch 9/50
 - 28s - loss: 3.4039 - acc: 0.2419 - val_loss: 3.5609 - val_acc: 0.2262
Epoch 10/50
 - 28s - loss: 3.2787 - acc: 0.2629 - val_loss: 3.4322 - val_acc: 0.2453
Epoch 11/50
 - 28s - loss: 3.1574 - acc: 0.2829 - val_loss: 3.4681 - val_acc: 0.2369
Epoch 12/50
 - 28s - loss: 3.0361 - acc: 0.3023 - val_loss: 3.4153 - val_acc: 0.2468
Epoch 13/50
 - 28s - loss: 2.9168 - acc: 0.3236 - val_loss: 3.4748 - val_acc: 0.2467
Epoch 14/50
 - 28s - loss: 2.7943 - acc: 0.3448 - val_loss: 3.3310 - val_acc: 0.2676
Epoch 15/50
 - 28s - loss: 2.6670 - acc: 0.3683 - val_loss: 3.4114 - val_acc: 0.2539
Epoch 16/50
 - 28s - loss: 2.5343 - acc: 0.3891 - val_loss: 3.3526 - val_acc: 0.2666
Epoch 17/50
 - 28s - loss: 2.4047 - acc: 0.4159 - val_loss: 3.4541 - val_acc: 0.2591
Epoch 18/50
 - 28s - loss: 2.2626 - acc: 0.4402 - val_loss: 3.4407 - val_acc: 0.2691
Epoch 19/50
 - 28s - loss: 2.1267 - acc: 0.4678 - val_loss: 3.4919 - val_acc: 0.2590
Epoch 20/50
 - 28s - loss: 1.9809 - acc: 0.4963 - val_loss: 3.5047 - val_acc: 0.2710
Epoch 21/50
 - 28s - loss: 1.8480 - acc: 0.5220 - val_loss: 3.6610 - val_acc: 0.2581
Epoch 22/50
 - 28s - loss: 1.7024 - acc: 0.5534 - val_loss: 3.6563 - val_acc: 0.2571
Epoch 23/50
 - 28s - loss: 1.5640 - acc: 0.5823 - val_loss: 3.8394 - val_acc: 0.2501
Epoch 24/50
 - 28s - loss: 1.4320 - acc: 0.6118 - val_loss: 3.8582 - val_acc: 0.2557
Epoch 25/50
 - 28s - loss: 1.3123 - acc: 0.6405 - val_loss: 3.9230 - val_acc: 0.2543
Epoch 26/50
 - 28s - loss: 1.1930 - acc: 0.6674 - val_loss: 4.0758 - val_acc: 0.2538
Epoch 27/50
 - 28s - loss: 1.0849 - acc: 0.6938 - val_loss: 4.2590 - val_acc: 0.2583
Epoch 28/50
 - 28s - loss: 1.0011 - acc: 0.7142 - val_loss: 4.2388 - val_acc: 0.2553
Epoch 29/50
 - 28s - loss: 0.9085 - acc: 0.7375 - val_loss: 4.4032 - val_acc: 0.2485
Epoch 30/50
 - 28s - loss: 0.8368 - acc: 0.7574 - val_loss: 4.5637 - val_acc: 0.2533
Epoch 31/50
 - 28s - loss: 0.7625 - acc: 0.7778 - val_loss: 4.5968 - val_acc: 0.2508
Epoch 32/50
 - 28s - loss: 0.7084 - acc: 0.7904 - val_loss: 4.7202 - val_acc: 0.2472
Epoch 33/50
 - 28s - loss: 0.6508 - acc: 0.8074 - val_loss: 4.8592 - val_acc: 0.2481
Epoch 34/50
 - 28s - loss: 0.6003 - acc: 0.8210 - val_loss: 4.9326 - val_acc: 0.2543
Epoch 35/50
 - 28s - loss: 0.5692 - acc: 0.8298 - val_loss: 5.2054 - val_acc: 0.2510
Epoch 36/50
 - 28s - loss: 0.5285 - acc: 0.8417 - val_loss: 5.2585 - val_acc: 0.2458
Epoch 37/50
 - 28s - loss: 0.4899 - acc: 0.8518 - val_loss: 5.0142 - val_acc: 0.2510
Epoch 38/50
 - 28s - loss: 0.4604 - acc: 0.8616 - val_loss: 5.2032 - val_acc: 0.2471
Epoch 39/50
 - 28s - loss: 0.4343 - acc: 0.8685 - val_loss: 5.3177 - val_acc: 0.2520
Epoch 40/50
 - 28s - loss: 0.4155 - acc: 0.8747 - val_loss: 5.3660 - val_acc: 0.2499
Epoch 41/50
 - 28s - loss: 0.3866 - acc: 0.8836 - val_loss: 5.3548 - val_acc: 0.2428
Epoch 42/50
 - 28s - loss: 0.3676 - acc: 0.8896 - val_loss: 5.5521 - val_acc: 0.2503
Epoch 43/50
 - 28s - loss: 0.3559 - acc: 0.8912 - val_loss: 5.5958 - val_acc: 0.2469
Epoch 44/50
 - 28s - loss: 0.3276 - acc: 0.9007 - val_loss: 5.4828 - val_acc: 0.2496
Epoch 45/50
 - 28s - loss: 0.3175 - acc: 0.9034 - val_loss: 5.6077 - val_acc: 0.2450
Epoch 46/50
 - 28s - loss: 0.3076 - acc: 0.9067 - val_loss: 5.6990 - val_acc: 0.2466
Epoch 47/50
 - 28s - loss: 0.2911 - acc: 0.9110 - val_loss: 5.7039 - val_acc: 0.2468
Epoch 48/50
 - 28s - loss: 0.2833 - acc: 0.9148 - val_loss: 5.6749 - val_acc: 0.2453
Epoch 49/50
 - 28s - loss: 0.2724 - acc: 0.9167 - val_loss: 5.7132 - val_acc: 0.2466
Epoch 50/50
 - 28s - loss: 0.2530 - acc: 0.9232 - val_loss: 5.7230 - val_acc: 0.2442
training history:
{'val_loss': [4.906600219726562, 4.533005661010742, 4.226707633972168, 4.054917583847046, 3.8604050918579103, 3.732624485015869, 3.6141569606781005, 3.639873072052002, 3.5608626296997072, 3.432166024017334, 3.4681469528198243, 3.4153087326049807, 3.47480002784729, 3.330991047668457, 3.411353807449341, 3.352629180908203, 3.4541305953979493, 3.4407497665405273, 3.4918965492248537, 3.504666455078125, 3.661035729598999, 3.656281413650513, 3.8393602180480957, 3.858201443481445, 3.9229789199829104, 4.075826405334473, 4.258974308013916, 4.238767692565918, 4.403153495025634, 4.5637139770507815, 4.59679423904419, 4.720238702392578, 4.8592296241760256, 4.932586734008789, 5.205363465881348, 5.258469743347168, 5.014242640686035, 5.203203388977051, 5.317709790039062, 5.366031211853027, 5.354791711425781, 5.552138642120362, 5.59579783782959, 5.482826692199707, 5.607678799438476, 5.699046086120606, 5.703899850463867, 5.674892802429199, 5.713198303222656, 5.723045947265625], 'val_acc': [0.0414, 0.0763, 0.1168, 0.142, 0.1677, 0.1909, 0.21, 0.2106, 0.2262, 0.2453, 0.2369, 0.2468, 0.2467, 0.2676, 0.2539, 0.2666, 0.2591, 0.2691, 0.259, 0.271, 0.2581, 0.2571, 0.2501, 0.2557, 0.2543, 0.2538, 0.2583, 0.2553, 0.2485, 0.2533, 0.2508, 0.2472, 0.2481, 0.2543, 0.251, 0.2458, 0.251, 0.2471, 0.252, 0.2499, 0.2428, 0.2503, 0.2469, 0.2496, 0.245, 0.2466, 0.2468, 0.2453, 0.2466, 0.2442], 'loss': [5.149211637420654, 4.79496301437378, 4.483095095825195, 4.21665776473999, 4.005117006759644, 3.8253276225280763, 3.6767209840393065, 3.5306076428222655, 3.4037601238250734, 3.2789054621887206, 3.157347223815918, 3.036244554443359, 2.9167637580871584, 2.7942642120361327, 2.667031385116577, 2.5341090097808836, 2.404802100563049, 2.2626783601379397, 2.1267391695404054, 1.9809574098968505, 1.8479409145355226, 1.7024133294677735, 1.5640045435333252, 1.4322066138076783, 1.3123264979934692, 1.19308444480896, 1.08478516746521, 1.0011920307159423, 0.9084304926300049, 0.8367502774429322, 0.7624634700965881, 0.7083491660118103, 0.6507178224182129, 0.6003073090839386, 0.5691630850982666, 0.5285821060180664, 0.4899280825901032, 0.4604465315008163, 0.4342853071117401, 0.41544127048015594, 0.3865775014400482, 0.3676270718002319, 0.3558817158460617, 0.3276069137620926, 0.31745716861963275, 0.3076722081923485, 0.2911384100985527, 0.28326945722579955, 0.2725195603340864, 0.25303042004823684], 'acc': [0.01671, 0.04912, 0.08512, 0.11912, 0.14819, 0.17445, 0.19894, 0.22019, 0.24193, 0.26284, 0.2829, 0.30228, 0.32361, 0.34484, 0.36828, 0.38914, 0.4159, 0.44016, 0.46777, 0.49628, 0.52196, 0.5534, 0.58224, 0.6118, 0.64051, 0.66735, 0.69385, 0.71423, 0.73751, 0.7574, 0.7778, 0.79039, 0.80744, 0.82106, 0.8298, 0.84173, 0.85175, 0.86157, 0.86847, 0.87468, 0.88365, 0.88957, 0.8912, 0.90068, 0.9034, 0.90666, 0.91097, 0.91479, 0.91664, 0.92318]}
Training time: 
1394.9719655513763
Evaluation results:  [5.723045971679688, 0.2442]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_11 (Conv2D)           (None, 58, 61, 48)        4080      
_________________________________________________________________
activation_7 (Activation)    (None, 58, 61, 48)        0         
_________________________________________________________________
max_pooling2d_7 (MaxPooling2 (None, 28, 30, 48)        0         
_________________________________________________________________
conv2d_12 (Conv2D)           (None, 24, 30, 128)       30848     
_________________________________________________________________
activation_8 (Activation)    (None, 24, 30, 128)       0         
_________________________________________________________________
max_pooling2d_8 (MaxPooling2 (None, 11, 14, 128)       0         
_________________________________________________________________
conv2d_13 (Conv2D)           (None, 9, 14, 192)        73920     
_________________________________________________________________
conv2d_14 (Conv2D)           (None, 7, 14, 192)        110784    
_________________________________________________________________
conv2d_15 (Conv2D)           (None, 5, 14, 128)        73856     
_________________________________________________________________
activation_9 (Activation)    (None, 5, 14, 128)        0         
_________________________________________________________________
max_pooling2d_9 (MaxPooling2 (None, 2, 6, 128)         0         
_________________________________________________________________
flatten_3 (Flatten)          (None, 1536)              0         
_________________________________________________________________
dense_7 (Dense)              (None, 2048)              3147776   
_________________________________________________________________
dropout_3 (Dropout)          (None, 2048)              0         
_________________________________________________________________
dense_8 (Dense)              (None, 2048)              4196352   
_________________________________________________________________
dropout_4 (Dropout)          (None, 2048)              0         
_________________________________________________________________
dense_9 (Dense)              (None, 200)               409800    
=================================================================
Total params: 8,047,416
Trainable params: 8,047,416
Non-trainable params: 0
_________________________________________________________________
Epoch 1/50
 - 29s - loss: 5.2850 - acc: 0.0065 - val_loss: 5.1874 - val_acc: 0.0180
Epoch 2/50
 - 28s - loss: 5.1096 - acc: 0.0191 - val_loss: 4.9347 - val_acc: 0.0385
Epoch 3/50
 - 28s - loss: 4.9171 - acc: 0.0361 - val_loss: 4.6918 - val_acc: 0.0626
Epoch 4/50
 - 28s - loss: 4.7261 - acc: 0.0550 - val_loss: 4.4730 - val_acc: 0.0869
Epoch 5/50
 - 28s - loss: 4.5220 - acc: 0.0751 - val_loss: 4.2440 - val_acc: 0.1158
Epoch 6/50
 - 28s - loss: 4.3416 - acc: 0.0953 - val_loss: 4.0577 - val_acc: 0.1393
Epoch 7/50
 - 28s - loss: 4.1886 - acc: 0.1142 - val_loss: 3.9428 - val_acc: 0.1567
Epoch 8/50
 - 28s - loss: 4.0633 - acc: 0.1326 - val_loss: 3.9330 - val_acc: 0.1577
Epoch 9/50
 - 28s - loss: 3.9555 - acc: 0.1472 - val_loss: 3.7410 - val_acc: 0.1832
Epoch 10/50
 - 28s - loss: 3.8632 - acc: 0.1609 - val_loss: 3.6494 - val_acc: 0.1970
Epoch 11/50
 - 28s - loss: 3.7746 - acc: 0.1740 - val_loss: 3.5993 - val_acc: 0.2054
Epoch 12/50
 - 28s - loss: 3.6968 - acc: 0.1872 - val_loss: 3.5433 - val_acc: 0.2125
Epoch 13/50
 - 28s - loss: 3.6292 - acc: 0.1985 - val_loss: 3.5883 - val_acc: 0.2088
Epoch 14/50
 - 28s - loss: 3.5662 - acc: 0.2070 - val_loss: 3.4497 - val_acc: 0.2337
Epoch 15/50
 - 28s - loss: 3.5031 - acc: 0.2176 - val_loss: 3.4460 - val_acc: 0.2326
Epoch 16/50
 - 28s - loss: 3.4558 - acc: 0.2260 - val_loss: 3.3536 - val_acc: 0.2501
Epoch 17/50
 - 28s - loss: 3.3929 - acc: 0.2350 - val_loss: 3.3351 - val_acc: 0.2491
Epoch 18/50
 - 28s - loss: 3.3394 - acc: 0.2455 - val_loss: 3.3051 - val_acc: 0.2556
Epoch 19/50
 - 28s - loss: 3.2922 - acc: 0.2530 - val_loss: 3.3711 - val_acc: 0.2463
Epoch 20/50
 - 28s - loss: 3.2436 - acc: 0.2614 - val_loss: 3.3028 - val_acc: 0.2609
Epoch 21/50
 - 28s - loss: 3.1928 - acc: 0.2701 - val_loss: 3.2262 - val_acc: 0.2742
Epoch 22/50
 - 28s - loss: 3.1490 - acc: 0.2775 - val_loss: 3.2547 - val_acc: 0.2691
Epoch 23/50
 - 28s - loss: 3.1032 - acc: 0.2851 - val_loss: 3.1878 - val_acc: 0.2837
Epoch 24/50
 - 28s - loss: 3.0624 - acc: 0.2911 - val_loss: 3.1479 - val_acc: 0.2846
Epoch 25/50
 - 28s - loss: 3.0176 - acc: 0.2983 - val_loss: 3.2393 - val_acc: 0.2703
Epoch 26/50
 - 28s - loss: 2.9749 - acc: 0.3057 - val_loss: 3.1756 - val_acc: 0.2859
Epoch 27/50
 - 28s - loss: 2.9277 - acc: 0.3158 - val_loss: 3.1371 - val_acc: 0.2850
Epoch 28/50
 - 28s - loss: 2.8906 - acc: 0.3211 - val_loss: 3.1519 - val_acc: 0.2851
Epoch 29/50
 - 28s - loss: 2.8377 - acc: 0.3304 - val_loss: 3.1588 - val_acc: 0.2834
Epoch 30/50
 - 28s - loss: 2.8026 - acc: 0.3369 - val_loss: 3.0965 - val_acc: 0.2965
Epoch 31/50
 - 28s - loss: 2.7674 - acc: 0.3443 - val_loss: 3.0793 - val_acc: 0.3023
Epoch 32/50
 - 28s - loss: 2.7181 - acc: 0.3526 - val_loss: 3.0743 - val_acc: 0.3005
Epoch 33/50
 - 28s - loss: 2.6843 - acc: 0.3580 - val_loss: 3.1104 - val_acc: 0.2973
Epoch 34/50
 - 28s - loss: 2.6403 - acc: 0.3663 - val_loss: 3.0590 - val_acc: 0.3049
Epoch 35/50
 - 28s - loss: 2.5981 - acc: 0.3726 - val_loss: 3.1215 - val_acc: 0.2902
Epoch 36/50
 - 28s - loss: 2.5612 - acc: 0.3800 - val_loss: 3.0697 - val_acc: 0.3017
Epoch 37/50
 - 28s - loss: 2.5238 - acc: 0.3854 - val_loss: 3.0777 - val_acc: 0.3075
Epoch 38/50
 - 28s - loss: 2.4827 - acc: 0.3954 - val_loss: 3.0352 - val_acc: 0.3115
Epoch 39/50
 - 28s - loss: 2.4464 - acc: 0.4013 - val_loss: 3.0767 - val_acc: 0.3025
Epoch 40/50
 - 28s - loss: 2.4040 - acc: 0.4099 - val_loss: 3.1393 - val_acc: 0.2935
Epoch 41/50
 - 28s - loss: 2.3584 - acc: 0.4173 - val_loss: 3.0498 - val_acc: 0.3049
Epoch 42/50
 - 28s - loss: 2.3214 - acc: 0.4259 - val_loss: 3.0686 - val_acc: 0.3040
Epoch 43/50
 - 28s - loss: 2.2792 - acc: 0.4328 - val_loss: 3.0716 - val_acc: 0.3073
Epoch 44/50
 - 28s - loss: 2.2456 - acc: 0.4370 - val_loss: 3.0411 - val_acc: 0.3090
Epoch 45/50
 - 28s - loss: 2.2052 - acc: 0.4469 - val_loss: 3.1004 - val_acc: 0.3080
Epoch 46/50
 - 28s - loss: 2.1679 - acc: 0.4537 - val_loss: 3.0515 - val_acc: 0.3099
Epoch 47/50
 - 28s - loss: 2.1376 - acc: 0.4584 - val_loss: 3.0611 - val_acc: 0.3075
Epoch 48/50
 - 28s - loss: 2.0953 - acc: 0.4686 - val_loss: 3.0807 - val_acc: 0.3079
Epoch 49/50
 - 28s - loss: 2.0536 - acc: 0.4752 - val_loss: 3.0718 - val_acc: 0.3118
Epoch 50/50
 - 28s - loss: 2.0178 - acc: 0.4807 - val_loss: 3.1003 - val_acc: 0.3119
training history:
{'val_loss': [5.187426020812988, 4.93471162185669, 4.69181203918457, 4.473019068908691, 4.244022955322266, 4.057713513946533, 3.9428043449401855, 3.9330294174194336, 3.7409913208007812, 3.649432944488525, 3.599321640777588, 3.543284997558594, 3.5882668285369874, 3.449686431121826, 3.4459553287506104, 3.3536337005615233, 3.3351275718688966, 3.3050931869506837, 3.3710895545959474, 3.3028354507446287, 3.2262445907592774, 3.254680910873413, 3.1878410846710206, 3.1479459396362306, 3.2392589485168455, 3.175613787460327, 3.1371378623962403, 3.151941321563721, 3.1587523281097414, 3.0965289722442626, 3.079322162628174, 3.0743216117858885, 3.110419268798828, 3.0590461769104005, 3.12154069442749, 3.069735478591919, 3.077703826522827, 3.035195927810669, 3.0766912723541258, 3.1393018272399904, 3.0497640686035155, 3.068600048828125, 3.0715685943603517, 3.041108563232422, 3.100432367706299, 3.0515372505187988, 3.0610978485107423, 3.080653764343262, 3.071786661529541, 3.1002543567657472], 'val_acc': [0.018, 0.0385, 0.0626, 0.0869, 0.1158, 0.1393, 0.1567, 0.1577, 0.1832, 0.197, 0.2054, 0.2125, 0.2088, 0.2337, 0.2326, 0.2501, 0.2491, 0.2556, 0.2463, 0.2609, 0.2742, 0.2691, 0.2837, 0.2846, 0.2703, 0.2859, 0.285, 0.2851, 0.2834, 0.2965, 0.3023, 0.3005, 0.2973, 0.3049, 0.2902, 0.3017, 0.3075, 0.3115, 0.3025, 0.2935, 0.3049, 0.304, 0.3073, 0.309, 0.308, 0.3099, 0.3075, 0.3079, 0.3118, 0.3119], 'loss': [5.284977554321289, 5.109566186676026, 4.917107265625, 4.726216509857178, 4.52192991973877, 4.3416019058227535, 4.188687691345215, 4.063248023223877, 3.9555945283508303, 3.8631563236999513, 3.774731987762451, 3.6968551928710935, 3.6294068731689455, 3.5659789459228515, 3.502987560043335, 3.4558831776428223, 3.3931627027893065, 3.3394121947479247, 3.292047971496582, 3.2436139908599855, 3.1927899490356446, 3.148878673400879, 3.1031548582458495, 3.0626737072753905, 3.0175424742126467, 2.974885226364136, 2.927660000305176, 2.8906788932800294, 2.837722829589844, 2.8026637538146972, 2.7675254432678225, 2.71811184967041, 2.6842175282287597, 2.6401533753204345, 2.5979919272613525, 2.561057167892456, 2.5237931581878663, 2.4828147077178957, 2.4464711101531984, 2.404024044189453, 2.358472399597168, 2.3215900671005247, 2.279311570854187, 2.2456683612060546, 2.2050537538909913, 2.167875486679077, 2.137627152862549, 2.095316422653198, 2.053771176147461, 2.0179014937210082], 'acc': [0.00652, 0.01911, 0.03609, 0.05495, 0.0751, 0.09526, 0.11423, 0.13257, 0.14718, 0.16089, 0.17398, 0.18722, 0.19852, 0.20705, 0.21757, 0.22597, 0.23497, 0.2455, 0.25299, 0.26143, 0.27002, 0.27748, 0.2851, 0.29109, 0.29836, 0.30571, 0.31576, 0.32105, 0.33041, 0.33688, 0.34426, 0.35254, 0.35805, 0.36635, 0.37265, 0.37998, 0.38541, 0.39534, 0.40124, 0.40991, 0.41727, 0.42586, 0.43282, 0.43694, 0.44697, 0.45366, 0.45835, 0.46857, 0.47514, 0.48074]}
Training time: 
1398.0375833511353
Evaluation results:  [3.1002543575286867, 0.3119]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_16 (Conv2D)           (None, 58, 61, 48)        4080      
_________________________________________________________________
activation_10 (Activation)   (None, 58, 61, 48)        0         
_________________________________________________________________
max_pooling2d_10 (MaxPooling (None, 28, 30, 48)        0         
_________________________________________________________________
conv2d_17 (Conv2D)           (None, 24, 30, 128)       30848     
_________________________________________________________________
activation_11 (Activation)   (None, 24, 30, 128)       0         
_________________________________________________________________
max_pooling2d_11 (MaxPooling (None, 11, 14, 128)       0         
_________________________________________________________________
conv2d_18 (Conv2D)           (None, 9, 14, 192)        73920     
_________________________________________________________________
conv2d_19 (Conv2D)           (None, 7, 14, 192)        110784    
_________________________________________________________________
conv2d_20 (Conv2D)           (None, 5, 14, 128)        73856     
_________________________________________________________________
activation_12 (Activation)   (None, 5, 14, 128)        0         
_________________________________________________________________
max_pooling2d_12 (MaxPooling (None, 2, 6, 128)         0         
_________________________________________________________________
flatten_4 (Flatten)          (None, 1536)              0         
_________________________________________________________________
dense_10 (Dense)             (None, 2048)              3147776   
_________________________________________________________________
dense_11 (Dense)             (None, 2048)              4196352   
_________________________________________________________________
dense_12 (Dense)             (None, 200)               409800    
=================================================================
Total params: 8,047,416
Trainable params: 8,047,416
Non-trainable params: 0
_________________________________________________________________
Epoch 1/50
 - 34s - loss: 4.9377 - acc: 0.0302 - val_loss: 4.6852 - val_acc: 0.0489
Epoch 2/50
 - 33s - loss: 4.4770 - acc: 0.0750 - val_loss: 4.2748 - val_acc: 0.0995
Epoch 3/50
 - 33s - loss: 4.1587 - acc: 0.1161 - val_loss: 4.0710 - val_acc: 0.1238
Epoch 4/50
 - 33s - loss: 3.9399 - acc: 0.1445 - val_loss: 4.0039 - val_acc: 0.1377
Epoch 5/50
 - 33s - loss: 3.7945 - acc: 0.1646 - val_loss: 3.9561 - val_acc: 0.1469
Epoch 6/50
 - 33s - loss: 3.6717 - acc: 0.1832 - val_loss: 3.8722 - val_acc: 0.1615
Epoch 7/50
 - 33s - loss: 3.5553 - acc: 0.1992 - val_loss: 3.8631 - val_acc: 0.1654
Epoch 8/50
 - 33s - loss: 3.4428 - acc: 0.2166 - val_loss: 3.8480 - val_acc: 0.1721
Epoch 9/50
 - 33s - loss: 3.3416 - acc: 0.2315 - val_loss: 3.8644 - val_acc: 0.1731
Epoch 10/50
 - 33s - loss: 3.2350 - acc: 0.2512 - val_loss: 3.9761 - val_acc: 0.1666
Epoch 11/50
 - 33s - loss: 3.1281 - acc: 0.2673 - val_loss: 4.0189 - val_acc: 0.1659
Epoch 12/50
 - 33s - loss: 3.0065 - acc: 0.2872 - val_loss: 4.0819 - val_acc: 0.1684
Epoch 13/50
 - 33s - loss: 2.8966 - acc: 0.3070 - val_loss: 4.1996 - val_acc: 0.1686
Epoch 14/50
 - 33s - loss: 2.7826 - acc: 0.3265 - val_loss: 4.2914 - val_acc: 0.1615
Epoch 15/50
 - 33s - loss: 2.6552 - acc: 0.3508 - val_loss: 4.4350 - val_acc: 0.1623
Epoch 16/50
 - 33s - loss: 2.5395 - acc: 0.3747 - val_loss: 4.5696 - val_acc: 0.1607
Epoch 17/50
 - 33s - loss: 2.4219 - acc: 0.3972 - val_loss: 4.6368 - val_acc: 0.1655
Epoch 18/50
 - 33s - loss: 2.3063 - acc: 0.4202 - val_loss: 4.8210 - val_acc: 0.1572
Epoch 19/50
 - 33s - loss: 2.1830 - acc: 0.4451 - val_loss: 5.0146 - val_acc: 0.1487
Epoch 20/50
 - 33s - loss: 2.0797 - acc: 0.4674 - val_loss: 5.2451 - val_acc: 0.1481
Epoch 21/50
 - 33s - loss: 1.9715 - acc: 0.4903 - val_loss: 5.4828 - val_acc: 0.1409
Epoch 22/50
 - 33s - loss: 1.8758 - acc: 0.5121 - val_loss: 5.6956 - val_acc: 0.1458
Epoch 23/50
 - 33s - loss: 1.7767 - acc: 0.5341 - val_loss: 5.8061 - val_acc: 0.1395
Epoch 24/50
 - 33s - loss: 1.6817 - acc: 0.5559 - val_loss: 5.9906 - val_acc: 0.1399
Epoch 25/50
 - 33s - loss: 1.6014 - acc: 0.5732 - val_loss: 6.2722 - val_acc: 0.1389
Epoch 26/50
 - 33s - loss: 1.5253 - acc: 0.5917 - val_loss: 6.3223 - val_acc: 0.1364
Epoch 27/50
 - 33s - loss: 1.4618 - acc: 0.6040 - val_loss: 6.5032 - val_acc: 0.1336
Epoch 28/50
 - 33s - loss: 1.3889 - acc: 0.6228 - val_loss: 6.7482 - val_acc: 0.1313
Epoch 29/50
 - 33s - loss: 1.3275 - acc: 0.6371 - val_loss: 7.0379 - val_acc: 0.1353
Epoch 30/50
 - 33s - loss: 1.2818 - acc: 0.6487 - val_loss: 7.1639 - val_acc: 0.1334
Epoch 31/50
 - 33s - loss: 1.2111 - acc: 0.6669 - val_loss: 7.1316 - val_acc: 0.1215
Epoch 32/50
 - 33s - loss: 1.1784 - acc: 0.6736 - val_loss: 7.3226 - val_acc: 0.1237
Epoch 33/50
 - 33s - loss: 1.1365 - acc: 0.6854 - val_loss: 7.5150 - val_acc: 0.1295
Epoch 34/50
 - 33s - loss: 1.0882 - acc: 0.6969 - val_loss: 7.8054 - val_acc: 0.1281
Epoch 35/50
 - 33s - loss: 1.0509 - acc: 0.7066 - val_loss: 7.8962 - val_acc: 0.1267
Epoch 36/50
 - 33s - loss: 1.0231 - acc: 0.7147 - val_loss: 8.1010 - val_acc: 0.1314
Epoch 37/50
 - 33s - loss: 0.9927 - acc: 0.7208 - val_loss: 8.2059 - val_acc: 0.1266
Epoch 38/50
 - 33s - loss: 0.9577 - acc: 0.7319 - val_loss: 8.4124 - val_acc: 0.1216
Epoch 39/50
 - 33s - loss: 0.9239 - acc: 0.7418 - val_loss: 8.2052 - val_acc: 0.1247
Epoch 40/50
 - 33s - loss: 0.9026 - acc: 0.7466 - val_loss: 8.3737 - val_acc: 0.1198
Epoch 41/50
 - 33s - loss: 0.9008 - acc: 0.7465 - val_loss: 8.4107 - val_acc: 0.1282
Epoch 42/50
 - 33s - loss: 0.8675 - acc: 0.7565 - val_loss: 8.4755 - val_acc: 0.1228
Epoch 43/50
 - 33s - loss: 0.8362 - acc: 0.7652 - val_loss: 8.7237 - val_acc: 0.1228
Epoch 44/50
 - 33s - loss: 0.8113 - acc: 0.7730 - val_loss: 8.7378 - val_acc: 0.1198
Epoch 45/50
 - 33s - loss: 0.8234 - acc: 0.7689 - val_loss: 8.9570 - val_acc: 0.1206
Epoch 46/50
 - 33s - loss: 0.7599 - acc: 0.7845 - val_loss: 9.1041 - val_acc: 0.1204
Epoch 47/50
 - 33s - loss: 0.7695 - acc: 0.7849 - val_loss: 9.0212 - val_acc: 0.1216
Epoch 48/50
 - 33s - loss: 0.7606 - acc: 0.7858 - val_loss: 9.1103 - val_acc: 0.1185
Epoch 49/50
 - 33s - loss: 0.7323 - acc: 0.7949 - val_loss: 9.2407 - val_acc: 0.1211
Epoch 50/50
 - 33s - loss: 0.7285 - acc: 0.7982 - val_loss: 9.4044 - val_acc: 0.1229
training history:
{'val_loss': [4.685225763702393, 4.274789175415039, 4.070992860412598, 4.003871980285645, 3.956138890838623, 3.8721951599121094, 3.863086639022827, 3.8479613945007323, 3.8644151023864746, 3.9761395866394045, 4.0189242195129395, 4.081923030090332, 4.19964900970459, 4.291422579956055, 4.4349846893310545, 4.5696046264648436, 4.636816375732422, 4.821005318450927, 5.01461501083374, 5.245099865722656, 5.482757969665528, 5.695588526916504, 5.8060930038452145, 5.990606017303467, 6.272225922393798, 6.322270484924316, 6.503236083984375, 6.748248477172852, 7.0379479873657225, 7.163944799804687, 7.131590801239014, 7.322618727111816, 7.514998776245117, 7.8054094306945805, 7.896193560791016, 8.100965431976318, 8.20594315109253, 8.412380538940429, 8.205150021362305, 8.373655535888672, 8.410666633605958, 8.475491204833984, 8.723727514648438, 8.737830438232422, 8.95701593322754, 9.104102890014648, 9.021248851013183, 9.110308538818359, 9.240740475463868, 9.40443193206787], 'val_acc': [0.0489, 0.0995, 0.1238, 0.1377, 0.1469, 0.1615, 0.1654, 0.1721, 0.1731, 0.1666, 0.1659, 0.1684, 0.1686, 0.1615, 0.1623, 0.1607, 0.1655, 0.1572, 0.1487, 0.1481, 0.1409, 0.1458, 0.1395, 0.1399, 0.1389, 0.1364, 0.1336, 0.1313, 0.1353, 0.1334, 0.1215, 0.1237, 0.1295, 0.1281, 0.1267, 0.1314, 0.1266, 0.1216, 0.1247, 0.1198, 0.1282, 0.1228, 0.1228, 0.1198, 0.1206, 0.1204, 0.1216, 0.1185, 0.1211, 0.1229], 'loss': [4.937619427185059, 4.477097438659668, 4.158677758178711, 3.9398819927978517, 3.794578409347534, 3.6718289070129395, 3.5552526628112795, 3.442896923675537, 3.341514771194458, 3.2348455570983887, 3.12820055770874, 3.0066451222229005, 2.8965423292541503, 2.7825524324798585, 2.6552361237335207, 2.539665873413086, 2.4217962370300294, 2.3063561630249025, 2.1830065634155273, 2.0798394361114503, 1.9715269213104247, 1.8760018427276612, 1.7768141339874268, 1.6816983917236328, 1.6012890536499023, 1.5253029061508179, 1.461793108139038, 1.3890756357955933, 1.3273094536972045, 1.2819788478279113, 1.2110464209747314, 1.1784054500579835, 1.1363847535324096, 1.0882894952964783, 1.0507943123817445, 1.0232329794502257, 0.9926786022186279, 0.9577597028541565, 0.9240937430381775, 0.902497730808258, 0.9009481704998016, 0.8675990735244751, 0.8360152038955688, 0.8111016215801239, 0.823484244480133, 0.7599882354354859, 0.7696707534217835, 0.7606655338287354, 0.7323221778011322, 0.7285390142536163], 'acc': [0.03017, 0.07496, 0.11607, 0.14456, 0.16456, 0.18313, 0.19918, 0.21654, 0.23146, 0.25118, 0.2673, 0.2872, 0.30702, 0.32649, 0.35087, 0.37465, 0.3972, 0.42018, 0.44516, 0.4674, 0.4903, 0.51205, 0.53406, 0.55587, 0.57321, 0.59169, 0.60405, 0.62272, 0.63711, 0.64868, 0.66691, 0.67356, 0.68544, 0.69687, 0.70659, 0.71466, 0.72081, 0.73188, 0.74178, 0.74664, 0.74644, 0.75643, 0.76522, 0.77302, 0.76893, 0.78453, 0.78491, 0.78577, 0.79485, 0.79819]}
Training time: 
1663.4916689395905
Evaluation results:  [9.404431924438477, 0.1229]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_21 (Conv2D)           (None, 58, 61, 48)        4080      
_________________________________________________________________
activation_13 (Activation)   (None, 58, 61, 48)        0         
_________________________________________________________________
max_pooling2d_13 (MaxPooling (None, 28, 30, 48)        0         
_________________________________________________________________
conv2d_22 (Conv2D)           (None, 24, 30, 128)       30848     
_________________________________________________________________
activation_14 (Activation)   (None, 24, 30, 128)       0         
_________________________________________________________________
max_pooling2d_14 (MaxPooling (None, 11, 14, 128)       0         
_________________________________________________________________
conv2d_23 (Conv2D)           (None, 9, 14, 192)        73920     
_________________________________________________________________
conv2d_24 (Conv2D)           (None, 7, 14, 192)        110784    
_________________________________________________________________
conv2d_25 (Conv2D)           (None, 5, 14, 128)        73856     
_________________________________________________________________
activation_15 (Activation)   (None, 5, 14, 128)        0         
_________________________________________________________________
max_pooling2d_15 (MaxPooling (None, 2, 6, 128)         0         
_________________________________________________________________
flatten_5 (Flatten)          (None, 1536)              0         
_________________________________________________________________
dense_13 (Dense)             (None, 2048)              3147776   
_________________________________________________________________
dropout_5 (Dropout)          (None, 2048)              0         
_________________________________________________________________
dense_14 (Dense)             (None, 2048)              4196352   
_________________________________________________________________
dropout_6 (Dropout)          (None, 2048)              0         
_________________________________________________________________
dense_15 (Dense)             (None, 200)               409800    
=================================================================
Total params: 8,047,416
Trainable params: 8,047,416
Non-trainable params: 0
_________________________________________________________________
Epoch 1/50
 - 34s - loss: 5.2994 - acc: 0.0044 - val_loss: 5.2984 - val_acc: 0.0050
Epoch 2/50
 - 33s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 3/50
 - 33s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 4/50
 - 33s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 5/50
 - 33s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 6/50
 - 33s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 7/50
 - 33s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 8/50
 - 33s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 9/50
 - 33s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 10/50
 - 33s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 11/50
 - 33s - loss: 5.3117 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 12/50
 - 33s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 13/50
 - 33s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 14/50
 - 33s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 15/50
 - 33s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2984 - val_acc: 0.0050
Epoch 16/50
 - 33s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 17/50
 - 33s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 18/50
 - 33s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 19/50
 - 33s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 20/50
 - 33s - loss: 5.2991 - acc: 0.0041 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 21/50
 - 33s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 22/50
 - 33s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 23/50
 - 33s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 24/50
 - 33s - loss: 5.2991 - acc: 0.0040 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 25/50
 - 33s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 26/50
 - 33s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 27/50
 - 33s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 28/50
 - 33s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 29/50
 - 33s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 30/50
 - 33s - loss: 5.2991 - acc: 0.0041 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 31/50
 - 33s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 32/50
 - 33s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 33/50
 - 33s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 34/50
 - 33s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 35/50
 - 33s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 36/50
 - 33s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 37/50
 - 33s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 38/50
 - 33s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 39/50
 - 33s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 40/50
 - 33s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 41/50
 - 33s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 42/50
 - 33s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 43/50
 - 33s - loss: 5.2991 - acc: 0.0040 - val_loss: 5.2984 - val_acc: 0.0050
Epoch 44/50
 - 33s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 45/50
 - 33s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 46/50
 - 33s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 47/50
 - 33s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 48/50
 - 33s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 49/50
 - 33s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 50/50
 - 33s - loss: 5.2991 - acc: 0.0041 - val_loss: 5.2983 - val_acc: 0.0050
training history:
{'val_loss': [5.298352766418457, 5.298343766784668, 5.298343304443359, 5.298342019653321, 5.298341397094727, 5.298345288085938, 5.2983404098510745, 5.2983432975769045, 5.298348403930664, 5.2983454315185545, 5.2983475746154785, 5.298343127441406, 5.298345330810547, 5.298346218109131, 5.298383391571045, 5.298340051269531, 5.298339625549317, 5.2983433013916015, 5.298344968414306, 5.298340835571289, 5.298338694763183, 5.298343786621094, 5.298344288635254, 5.298343269348145, 5.298341860961914, 5.298345617675781, 5.298344120788574, 5.298342234802246, 5.2983425521850585, 5.298346137237549, 5.298341927337646, 5.298343988037109, 5.298343003845215, 5.298344848632812, 5.2983451858520505, 5.298340241241455, 5.298343370056152, 5.298344059753418, 5.298341654968262, 5.298344944763183, 5.298342604064941, 5.29834356842041, 5.298350269317627, 5.298344723510742, 5.298344645690918, 5.298343200683593, 5.2983463027954105, 5.2983392837524415, 5.29834285736084, 5.298347142028809], 'val_acc': [0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005], 'loss': [5.299394289245606, 5.2990950622558595, 5.299081043395996, 5.299091593322754, 5.299094634399414, 5.299091012573242, 5.299102086029053, 5.299080446777344, 5.299091668243408, 5.299088424377441, 5.3117390167236325, 5.299090838012695, 5.299080427246094, 5.299110178527832, 5.299066484375, 5.299091560211181, 5.299088739318847, 5.299090356140137, 5.299093020477295, 5.299097049255371, 5.299105069732666, 5.29907822845459, 5.299089596405029, 5.299096202087402, 5.299094351196289, 5.299081463470459, 5.299089077606201, 5.299093074798584, 5.299086388549805, 5.299075011291504, 5.2990863468933105, 5.29908050994873, 5.299085731506348, 5.299074079284668, 5.29910005645752, 5.299087378387451, 5.299092436828613, 5.2990790063476565, 5.299092240905762, 5.2990921607971195, 5.299093028717041, 5.299094017333984, 5.299080487365723, 5.29907883392334, 5.29909690826416, 5.299088755187988, 5.299085896148681, 5.299100942993164, 5.299091141662598, 5.299089712219239], 'acc': [0.00444, 0.00447, 0.0043, 0.00434, 0.00424, 0.00423, 0.00419, 0.00449, 0.00446, 0.00433, 0.00422, 0.00431, 0.00444, 0.00422, 0.00447, 0.00423, 0.00439, 0.00436, 0.00454, 0.00408, 0.00451, 0.00448, 0.00438, 0.00396, 0.0044, 0.0043, 0.00422, 0.00437, 0.0043, 0.0041, 0.00428, 0.00433, 0.00429, 0.00419, 0.00429, 0.00433, 0.00429, 0.00423, 0.00436, 0.00424, 0.00453, 0.00444, 0.00404, 0.0044, 0.00429, 0.0045, 0.00422, 0.00442, 0.00455, 0.00415]}
Training time: 
1667.7206587791443
Evaluation results:  [5.298347116851807, 0.005]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_26 (Conv2D)           (None, 58, 61, 48)        4080      
_________________________________________________________________
activation_16 (Activation)   (None, 58, 61, 48)        0         
_________________________________________________________________
max_pooling2d_16 (MaxPooling (None, 28, 30, 48)        0         
_________________________________________________________________
conv2d_27 (Conv2D)           (None, 24, 30, 128)       30848     
_________________________________________________________________
activation_17 (Activation)   (None, 24, 30, 128)       0         
_________________________________________________________________
max_pooling2d_17 (MaxPooling (None, 11, 14, 128)       0         
_________________________________________________________________
conv2d_28 (Conv2D)           (None, 9, 14, 192)        73920     
_________________________________________________________________
conv2d_29 (Conv2D)           (None, 7, 14, 192)        110784    
_________________________________________________________________
conv2d_30 (Conv2D)           (None, 5, 14, 128)        73856     
_________________________________________________________________
activation_18 (Activation)   (None, 5, 14, 128)        0         
_________________________________________________________________
max_pooling2d_18 (MaxPooling (None, 2, 6, 128)         0         
_________________________________________________________________
flatten_6 (Flatten)          (None, 1536)              0         
_________________________________________________________________
dense_16 (Dense)             (None, 2048)              3147776   
_________________________________________________________________
dropout_7 (Dropout)          (None, 2048)              0         
_________________________________________________________________
dense_17 (Dense)             (None, 2048)              4196352   
_________________________________________________________________
dropout_8 (Dropout)          (None, 2048)              0         
_________________________________________________________________
dense_18 (Dense)             (None, 200)               409800    
=================================================================
Total params: 8,047,416
Trainable params: 8,047,416
Non-trainable params: 0
_________________________________________________________________
Epoch 1/50
 - 34s - loss: 5.3001 - acc: 0.0043 - val_loss: 5.2984 - val_acc: 0.0050
Epoch 2/50
 - 33s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 3/50
 - 33s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 4/50
 - 33s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 5/50
 - 33s - loss: 5.2991 - acc: 0.0046 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 6/50
 - 33s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 7/50
 - 33s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 8/50
 - 33s - loss: 5.2991 - acc: 0.0040 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 9/50
 - 33s - loss: 5.2991 - acc: 0.0046 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 10/50
 - 33s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 11/50
 - 33s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 12/50
 - 33s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 13/50
 - 33s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 14/50
 - 33s - loss: 5.2991 - acc: 0.0049 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 15/50
 - 33s - loss: 5.2991 - acc: 0.0041 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 16/50
 - 33s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 17/50
 - 33s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 18/50
 - 33s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 19/50
 - 33s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 20/50
 - 33s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 21/50
 - 33s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2984 - val_acc: 0.0050
Epoch 22/50
 - 33s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 23/50
 - 33s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 24/50
 - 33s - loss: 5.2991 - acc: 0.0041 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 25/50
 - 33s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 26/50
 - 33s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 27/50
 - 33s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 28/50
 - 33s - loss: 5.2991 - acc: 0.0046 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 29/50
 - 33s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 30/50
 - 33s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 31/50
 - 33s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 32/50
 - 33s - loss: 5.2991 - acc: 0.0046 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 33/50
 - 33s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 34/50
 - 33s - loss: 5.2991 - acc: 0.0046 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 35/50
 - 33s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 36/50
 - 33s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 37/50
 - 33s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 38/50
 - 33s - loss: 5.2991 - acc: 0.0040 - val_loss: 5.2984 - val_acc: 0.0050
Epoch 39/50
 - 33s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 40/50
 - 33s - loss: 5.2991 - acc: 0.0041 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 41/50
 - 33s - loss: 5.2991 - acc: 0.0046 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 42/50
 - 33s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 43/50
 - 33s - loss: 5.2991 - acc: 0.0046 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 44/50
 - 33s - loss: 5.2991 - acc: 0.0047 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 45/50
 - 33s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 46/50
 - 33s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 47/50
 - 33s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 48/50
 - 33s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 49/50
 - 33s - loss: 5.2991 - acc: 0.0041 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 50/50
 - 33s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
training history:
{'val_loss': [5.298354327392578, 5.298341390991211, 5.298341270446778, 5.298342199707031, 5.2983417892456055, 5.298343479919434, 5.298342440032959, 5.298344694519043, 5.298344025421143, 5.298343575286865, 5.298345330047607, 5.298346635437012, 5.298344328308105, 5.298342144775391, 5.298344108581543, 5.298347595977783, 5.298345796203614, 5.298342733764648, 5.298347957611084, 5.298339979553223, 5.298352267456055, 5.2983470520019535, 5.298344159698487, 5.298343450927734, 5.298346171569825, 5.298341639709473, 5.298344314575195, 5.298343351745605, 5.298342962646484, 5.298341851806641, 5.298344548034668, 5.298346910095215, 5.298342724609375, 5.298340934753418, 5.298341946411133, 5.298341887664795, 5.298343119812012, 5.298351870727539, 5.298345655822754, 5.2983459365844725, 5.2983427940368655, 5.298344946289062, 5.2983485626220705, 5.298345067596435, 5.298345848846435, 5.29834030303955, 5.298340261840821, 5.298339953613281, 5.298340750122071, 5.298347889709473], 'val_acc': [0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005], 'loss': [5.300133277282715, 5.29910433303833, 5.299095555267334, 5.2990867578125, 5.299092929382324, 5.2990926727294925, 5.299097119140625, 5.299096527404785, 5.299099190063477, 5.299088864898682, 5.299087207794189, 5.2990968630981445, 5.299086202850342, 5.2990912794494625, 5.2990769223022465, 5.299081108398438, 5.2991094616699215, 5.299083016052246, 5.299076459503174, 5.299106028137207, 5.299056999816894, 5.299081781311036, 5.2990910430908205, 5.2990846351623535, 5.299080581207275, 5.299085291900635, 5.299097770843506, 5.299097253265381, 5.299104575347901, 5.299085146484375, 5.299074078674316, 5.299098813476562, 5.299095938415527, 5.299103074035645, 5.299086337890625, 5.299080192565918, 5.299083844451904, 5.299089445648193, 5.299083565979004, 5.299088312072754, 5.299097945251465, 5.299064891052246, 5.299081628112793, 5.299086553649903, 5.299080918884277, 5.299099856872559, 5.299094150695801, 5.299096285095215, 5.29909121383667, 5.299067804260254], 'acc': [0.0043, 0.00436, 0.00432, 0.00419, 0.0046, 0.00418, 0.00446, 0.00404, 0.00456, 0.00419, 0.00427, 0.00422, 0.00451, 0.00486, 0.00409, 0.00442, 0.00429, 0.00454, 0.00436, 0.00445, 0.00451, 0.0043, 0.00433, 0.00414, 0.00432, 0.00425, 0.00452, 0.00462, 0.00419, 0.00417, 0.00441, 0.00463, 0.00425, 0.00464, 0.00425, 0.0045, 0.00429, 0.00403, 0.00422, 0.00412, 0.00456, 0.00432, 0.00458, 0.00471, 0.00454, 0.00446, 0.00421, 0.00428, 0.00406, 0.00453]}
Training time: 
1669.7840948104858
Evaluation results:  [5.298347903442383, 0.005]
