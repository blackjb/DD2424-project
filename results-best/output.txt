trainingset size: 100000
validationset size: 10000
input_dims:  (64, 64, 3)
nb_labels:  200
nb_labels: 200
x_train.shape  (100000, 64, 64, 3)
y_train.shape  (100000, 200)
x_test.shape  (10000, 64, 64, 3)
y_test.shape  (10000, 200)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 58, 61, 48)        4080      
_________________________________________________________________
activation_1 (Activation)    (None, 58, 61, 48)        0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 28, 30, 48)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 24, 30, 128)       30848     
_________________________________________________________________
activation_2 (Activation)    (None, 24, 30, 128)       0         
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 11, 14, 128)       0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 9, 14, 192)        73920     
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 7, 14, 192)        110784    
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 5, 14, 128)        73856     
_________________________________________________________________
activation_3 (Activation)    (None, 5, 14, 128)        0         
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 2, 6, 128)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 1536)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 2048)              3147776   
_________________________________________________________________
dropout_1 (Dropout)          (None, 2048)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 2048)              4196352   
_________________________________________________________________
dropout_2 (Dropout)          (None, 2048)              0         
_________________________________________________________________
dense_3 (Dense)              (None, 200)               409800    
=================================================================
Total params: 8,047,416
Trainable params: 8,047,416
Non-trainable params: 0
_________________________________________________________________
Epoch 1/75
 - 36s - loss: 5.2718 - acc: 0.0081 - val_loss: 5.1601 - val_acc: 0.0129
Epoch 2/75
 - 27s - loss: 5.1156 - acc: 0.0194 - val_loss: 4.9463 - val_acc: 0.0359
Epoch 3/75
 - 27s - loss: 4.9444 - acc: 0.0328 - val_loss: 4.7426 - val_acc: 0.0610
Epoch 4/75
 - 27s - loss: 4.7798 - acc: 0.0490 - val_loss: 4.5638 - val_acc: 0.0752
Epoch 5/75
 - 27s - loss: 4.6071 - acc: 0.0666 - val_loss: 4.3602 - val_acc: 0.0980
Epoch 6/75
 - 27s - loss: 4.4353 - acc: 0.0850 - val_loss: 4.2082 - val_acc: 0.1192
Epoch 7/75
 - 27s - loss: 4.2811 - acc: 0.1033 - val_loss: 4.0684 - val_acc: 0.1337
Epoch 8/75
 - 27s - loss: 4.1521 - acc: 0.1207 - val_loss: 3.9251 - val_acc: 0.1571
Epoch 9/75
 - 27s - loss: 4.0379 - acc: 0.1356 - val_loss: 3.8176 - val_acc: 0.1730
Epoch 10/75
 - 27s - loss: 3.9454 - acc: 0.1488 - val_loss: 3.7557 - val_acc: 0.1851
Epoch 11/75
 - 27s - loss: 3.8638 - acc: 0.1609 - val_loss: 3.6764 - val_acc: 0.1966
Epoch 12/75
 - 27s - loss: 3.7880 - acc: 0.1715 - val_loss: 3.6597 - val_acc: 0.2019
Epoch 13/75
 - 27s - loss: 3.7135 - acc: 0.1844 - val_loss: 3.5523 - val_acc: 0.2148
Epoch 14/75
 - 27s - loss: 3.6547 - acc: 0.1916 - val_loss: 3.5620 - val_acc: 0.2130
Epoch 15/75
 - 27s - loss: 3.5969 - acc: 0.2020 - val_loss: 3.4488 - val_acc: 0.2343
Epoch 16/75
 - 27s - loss: 3.5441 - acc: 0.2116 - val_loss: 3.4406 - val_acc: 0.2361
Epoch 17/75
 - 27s - loss: 3.4913 - acc: 0.2205 - val_loss: 3.3895 - val_acc: 0.2412
Epoch 18/75
 - 27s - loss: 3.4473 - acc: 0.2262 - val_loss: 3.3607 - val_acc: 0.2498
Epoch 19/75
 - 27s - loss: 3.3983 - acc: 0.2360 - val_loss: 3.3045 - val_acc: 0.2532
Epoch 20/75
 - 27s - loss: 3.3611 - acc: 0.2410 - val_loss: 3.2981 - val_acc: 0.2580
Epoch 21/75
 - 27s - loss: 3.3169 - acc: 0.2502 - val_loss: 3.2693 - val_acc: 0.2672
Epoch 22/75
 - 27s - loss: 3.2787 - acc: 0.2556 - val_loss: 3.2374 - val_acc: 0.2719
Epoch 23/75
 - 27s - loss: 3.2450 - acc: 0.2601 - val_loss: 3.1880 - val_acc: 0.2797
Epoch 24/75
 - 27s - loss: 3.2116 - acc: 0.2668 - val_loss: 3.1796 - val_acc: 0.2834
Epoch 25/75
 - 27s - loss: 3.1781 - acc: 0.2748 - val_loss: 3.1396 - val_acc: 0.2894
Epoch 26/75
 - 27s - loss: 3.1411 - acc: 0.2796 - val_loss: 3.1425 - val_acc: 0.2896
Epoch 27/75
 - 27s - loss: 3.1047 - acc: 0.2853 - val_loss: 3.1218 - val_acc: 0.2887
Epoch 28/75
 - 27s - loss: 3.0836 - acc: 0.2907 - val_loss: 3.1204 - val_acc: 0.2930
Epoch 29/75
 - 27s - loss: 3.0479 - acc: 0.2950 - val_loss: 3.0728 - val_acc: 0.3000
Epoch 30/75
 - 27s - loss: 3.0198 - acc: 0.2979 - val_loss: 3.0750 - val_acc: 0.3004
Epoch 31/75
 - 27s - loss: 2.9841 - acc: 0.3049 - val_loss: 3.0340 - val_acc: 0.3098
Epoch 32/75
 - 27s - loss: 2.9642 - acc: 0.3106 - val_loss: 3.0355 - val_acc: 0.3062
Epoch 33/75
 - 27s - loss: 2.9346 - acc: 0.3162 - val_loss: 3.0110 - val_acc: 0.3126
Epoch 34/75
 - 27s - loss: 2.9071 - acc: 0.3208 - val_loss: 3.0225 - val_acc: 0.3125
Epoch 35/75
 - 27s - loss: 2.8866 - acc: 0.3241 - val_loss: 3.0575 - val_acc: 0.3045
Epoch 36/75
 - 27s - loss: 2.8568 - acc: 0.3279 - val_loss: 2.9822 - val_acc: 0.3156
Epoch 37/75
 - 27s - loss: 2.8308 - acc: 0.3345 - val_loss: 3.0458 - val_acc: 0.3063
Epoch 38/75
 - 27s - loss: 2.8093 - acc: 0.3388 - val_loss: 2.9904 - val_acc: 0.3193
Epoch 39/75
 - 27s - loss: 2.7808 - acc: 0.3427 - val_loss: 3.0379 - val_acc: 0.3085
Epoch 40/75
 - 27s - loss: 2.7585 - acc: 0.3467 - val_loss: 3.0071 - val_acc: 0.3109
Epoch 41/75
 - 27s - loss: 2.7371 - acc: 0.3506 - val_loss: 2.9949 - val_acc: 0.3170
Epoch 42/75
 - 27s - loss: 2.7120 - acc: 0.3552 - val_loss: 2.9317 - val_acc: 0.3255
Epoch 43/75
 - 27s - loss: 2.6910 - acc: 0.3578 - val_loss: 2.9201 - val_acc: 0.3277
Epoch 44/75
 - 27s - loss: 2.6700 - acc: 0.3633 - val_loss: 2.9488 - val_acc: 0.3245
Epoch 45/75
 - 27s - loss: 2.6459 - acc: 0.3655 - val_loss: 2.8876 - val_acc: 0.3365
Epoch 46/75
 - 27s - loss: 2.6279 - acc: 0.3695 - val_loss: 2.9051 - val_acc: 0.3283
Epoch 47/75
 - 27s - loss: 2.6045 - acc: 0.3749 - val_loss: 2.9069 - val_acc: 0.3296
Epoch 48/75
 - 27s - loss: 2.5809 - acc: 0.3800 - val_loss: 2.9202 - val_acc: 0.3319
Epoch 49/75
 - 27s - loss: 2.5615 - acc: 0.3809 - val_loss: 2.9303 - val_acc: 0.3309
Epoch 50/75
 - 27s - loss: 2.5362 - acc: 0.3856 - val_loss: 2.9057 - val_acc: 0.3358
Epoch 51/75
 - 27s - loss: 2.5185 - acc: 0.3891 - val_loss: 2.8658 - val_acc: 0.3422
Epoch 52/75
 - 27s - loss: 2.4936 - acc: 0.3936 - val_loss: 2.8801 - val_acc: 0.3370
Epoch 53/75
 - 27s - loss: 2.4747 - acc: 0.3981 - val_loss: 2.8778 - val_acc: 0.3373
Epoch 54/75
 - 27s - loss: 2.4588 - acc: 0.4004 - val_loss: 2.8769 - val_acc: 0.3388
Epoch 55/75
 - 27s - loss: 2.4320 - acc: 0.4044 - val_loss: 2.9081 - val_acc: 0.3317
Epoch 56/75
 - 27s - loss: 2.4167 - acc: 0.4084 - val_loss: 2.8560 - val_acc: 0.3406
Epoch 57/75
 - 27s - loss: 2.3932 - acc: 0.4126 - val_loss: 2.8765 - val_acc: 0.3397
Epoch 58/75
 - 27s - loss: 2.3769 - acc: 0.4153 - val_loss: 2.8646 - val_acc: 0.3423
Epoch 59/75
 - 27s - loss: 2.3552 - acc: 0.4187 - val_loss: 2.8759 - val_acc: 0.3438
Epoch 60/75
 - 27s - loss: 2.3333 - acc: 0.4250 - val_loss: 2.8516 - val_acc: 0.3415
Epoch 61/75
 - 27s - loss: 2.3130 - acc: 0.4276 - val_loss: 2.8714 - val_acc: 0.3400
Epoch 62/75
 - 27s - loss: 2.2996 - acc: 0.4286 - val_loss: 2.8316 - val_acc: 0.3484
Epoch 63/75
 - 27s - loss: 2.2755 - acc: 0.4324 - val_loss: 2.9313 - val_acc: 0.3307
Epoch 64/75
 - 27s - loss: 2.2564 - acc: 0.4360 - val_loss: 2.8447 - val_acc: 0.3434
Epoch 65/75
 - 27s - loss: 2.2411 - acc: 0.4390 - val_loss: 2.8494 - val_acc: 0.3430
Epoch 66/75
 - 27s - loss: 2.2169 - acc: 0.4443 - val_loss: 2.8613 - val_acc: 0.3393
Epoch 67/75
 - 27s - loss: 2.1996 - acc: 0.4491 - val_loss: 2.8603 - val_acc: 0.3454
Epoch 68/75
 - 27s - loss: 2.1845 - acc: 0.4501 - val_loss: 2.8549 - val_acc: 0.3459
Epoch 69/75
 - 27s - loss: 2.1670 - acc: 0.4554 - val_loss: 2.8733 - val_acc: 0.3416
Epoch 70/75
 - 27s - loss: 2.1427 - acc: 0.4595 - val_loss: 2.8686 - val_acc: 0.3476
Epoch 71/75
 - 27s - loss: 2.1289 - acc: 0.4606 - val_loss: 2.8738 - val_acc: 0.3425
Epoch 72/75
 - 27s - loss: 2.1156 - acc: 0.4639 - val_loss: 2.8451 - val_acc: 0.3435
Epoch 73/75
 - 27s - loss: 2.0938 - acc: 0.4696 - val_loss: 2.8638 - val_acc: 0.3454
Epoch 74/75
 - 27s - loss: 2.0794 - acc: 0.4699 - val_loss: 2.8614 - val_acc: 0.3466
Epoch 75/75
 - 27s - loss: 2.0642 - acc: 0.4739 - val_loss: 2.9012 - val_acc: 0.3361
training history:
{'val_loss': [5.1601328216552735, 4.946254683685303, 4.742562629699707, 4.563756753540039, 4.360175689697265, 4.208184407043457, 4.06838879776001, 3.9251080261230467, 3.8175850105285645, 3.755674779510498, 3.676386442565918, 3.6597094772338865, 3.5523209365844726, 3.561970428466797, 3.4488444099426268, 3.440558108520508, 3.3894802185058595, 3.360730711746216, 3.3045204593658446, 3.298057411193848, 3.269296905899048, 3.2374045875549315, 3.1879673225402834, 3.1795720336914064, 3.1396418746948243, 3.1424631172180177, 3.12183974647522, 3.1203877380371092, 3.0728019981384276, 3.0749676834106445, 3.0339875717163087, 3.035457917022705, 3.0109717388153077, 3.0224572128295897, 3.0574569789886477, 2.9821696113586427, 3.0458043395996093, 2.9904232883453368, 3.0379164417266846, 3.0071119651794436, 2.9948867431640624, 2.9317349098205567, 2.920125506591797, 2.9488494216918943, 2.88761131439209, 2.905102433013916, 2.9068851974487306, 2.920179763031006, 2.9303370307922365, 2.9056927955627443, 2.865802355194092, 2.8800680671691894, 2.8777867599487306, 2.8768961055755615, 2.9080903923034667, 2.856008627319336, 2.876455126953125, 2.864563732910156, 2.875940969848633, 2.85161897315979, 2.8713759300231936, 2.8315617519378664, 2.931279602050781, 2.8447305057525636, 2.8494354942321776, 2.8613389167785646, 2.860314372253418, 2.8548662467956545, 2.873304525756836, 2.8685686904907226, 2.873781945037842, 2.8450549896240234, 2.8637528499603273, 2.8614087745666503, 2.9011539672851563], 'val_acc': [0.0129, 0.0359, 0.061, 0.0752, 0.098, 0.1192, 0.1337, 0.1571, 0.173, 0.1851, 0.1966, 0.2019, 0.2148, 0.213, 0.2343, 0.2361, 0.2412, 0.2498, 0.2532, 0.258, 0.2672, 0.2719, 0.2797, 0.2834, 0.2894, 0.2896, 0.2887, 0.293, 0.3, 0.3004, 0.3098, 0.3062, 0.3126, 0.3125, 0.3045, 0.3156, 0.3063, 0.3193, 0.3085, 0.3109, 0.317, 0.3255, 0.3277, 0.3245, 0.3365, 0.3283, 0.3296, 0.3319, 0.3309, 0.3358, 0.3422, 0.337, 0.3373, 0.3388, 0.3317, 0.3406, 0.3397, 0.3423, 0.3438, 0.3415, 0.34, 0.3484, 0.3307, 0.3434, 0.343, 0.3393, 0.3454, 0.3459, 0.3416, 0.3476, 0.3425, 0.3435, 0.3454, 0.3466, 0.3361], 'loss': [5.271834535217285, 5.1156609896850584, 4.944315726013183, 4.7798174765014645, 4.607158458557129, 4.435337329559326, 4.281100865631103, 4.152147080535888, 4.0378009783935545, 3.945434605102539, 3.863797509689331, 3.7879928398132323, 3.713568811798096, 3.6546556352996826, 3.596930146484375, 3.543994780807495, 3.4914213230895994, 3.44728358543396, 3.3982086729431153, 3.3611020420837403, 3.3168365992736817, 3.278811217880249, 3.244939740753174, 3.2117569064331053, 3.177974815444946, 3.141361501312256, 3.1046344229125977, 3.083644414215088, 3.0478875427246095, 3.019740116958618, 2.9841356927490232, 2.964237289199829, 2.9346708489990236, 2.907147819519043, 2.886643137664795, 2.8567764234924318, 2.830689331436157, 2.8091801865386965, 2.7807347458648684, 2.758471526260376, 2.7369921559143067, 2.712168969116211, 2.69099993309021, 2.6701540686798095, 2.6459565044403077, 2.6278328231811523, 2.6045084286499023, 2.5808626522827147, 2.5613655657958985, 2.5360702285003662, 2.51838662940979, 2.4938456941604614, 2.4749355294799806, 2.4589691828536986, 2.4321083209991454, 2.416705264968872, 2.3931482267761233, 2.3769034558868407, 2.355225789871216, 2.333431128768921, 2.3130073222351073, 2.299812219543457, 2.2754778643035887, 2.25630752243042, 2.2411601041412355, 2.2169554361724852, 2.1994993296051026, 2.184575184173584, 2.1670847498321533, 2.1428466187286377, 2.1288949073028562, 2.115456658554077, 2.093614323348999, 2.0793615837860107, 2.0642826498413087], 'acc': [0.00811, 0.01938, 0.03279, 0.04896, 0.06657, 0.085, 0.10326, 0.12069, 0.13562, 0.14883, 0.16094, 0.17153, 0.18436, 0.19166, 0.20195, 0.21158, 0.22045, 0.22628, 0.23603, 0.24104, 0.25027, 0.25555, 0.26008, 0.26681, 0.27479, 0.2795, 0.28535, 0.29065, 0.29496, 0.29791, 0.3049, 0.31056, 0.31615, 0.32078, 0.32409, 0.32796, 0.33452, 0.33878, 0.34266, 0.34668, 0.35058, 0.35507, 0.35779, 0.36331, 0.36546, 0.36959, 0.37488, 0.37998, 0.38087, 0.38565, 0.38916, 0.39358, 0.39809, 0.40034, 0.40433, 0.4084, 0.41258, 0.41525, 0.4187, 0.42499, 0.42754, 0.42857, 0.43245, 0.43599, 0.43897, 0.44434, 0.44913, 0.45002, 0.45535, 0.4595, 0.46063, 0.46398, 0.46969, 0.46991, 0.47387]}
Training time: 
2028.2319605350494
Evaluation results:  [2.896027569961548, 0.3381]
