trainingset size: 100000
validationset size: 10000
input_dims:  (64, 64, 3)
nb_labels:  200
nb_labels: 200
x_train.shape  (100000, 64, 64, 3)
y_train.shape  (100000, 200)
x_test.shape  (10000, 64, 64, 3)
y_test.shape  (10000, 200)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 58, 61, 48)        4080      
_________________________________________________________________
activation_1 (Activation)    (None, 58, 61, 48)        0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 28, 30, 48)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 24, 30, 128)       30848     
_________________________________________________________________
activation_2 (Activation)    (None, 24, 30, 128)       0         
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 11, 14, 128)       0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 9, 14, 192)        73920     
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 7, 14, 192)        110784    
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 5, 14, 128)        73856     
_________________________________________________________________
activation_3 (Activation)    (None, 5, 14, 128)        0         
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 2, 6, 128)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 1536)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 2048)              3147776   
_________________________________________________________________
dense_2 (Dense)              (None, 2048)              4196352   
_________________________________________________________________
dense_3 (Dense)              (None, 200)               409800    
=================================================================
Total params: 8,047,416
Trainable params: 8,047,416
Non-trainable params: 0
_________________________________________________________________
Epoch 1/50
 - 37s - loss: 5.0279 - acc: 0.0291 - val_loss: 4.8022 - val_acc: 0.0496
Epoch 2/50
 - 27s - loss: 4.5567 - acc: 0.0813 - val_loss: 4.3190 - val_acc: 0.1085
Epoch 3/50
 - 27s - loss: 4.2014 - acc: 0.1263 - val_loss: 4.0286 - val_acc: 0.1472
Epoch 4/50
 - 27s - loss: 3.9287 - acc: 0.1646 - val_loss: 3.8734 - val_acc: 0.1612
Epoch 5/50
 - 27s - loss: 3.7144 - acc: 0.1980 - val_loss: 3.7774 - val_acc: 0.1854
Epoch 6/50
 - 27s - loss: 3.5259 - acc: 0.2259 - val_loss: 3.5649 - val_acc: 0.2213
Epoch 7/50
 - 27s - loss: 3.3665 - acc: 0.2537 - val_loss: 3.5874 - val_acc: 0.2247
Epoch 8/50
 - 27s - loss: 3.2124 - acc: 0.2795 - val_loss: 3.5847 - val_acc: 0.2216
Epoch 9/50
 - 27s - loss: 3.0718 - acc: 0.3012 - val_loss: 3.4452 - val_acc: 0.2410
Epoch 10/50
 - 27s - loss: 2.9334 - acc: 0.3276 - val_loss: 3.3890 - val_acc: 0.2546
Epoch 11/50
 - 27s - loss: 2.7907 - acc: 0.3504 - val_loss: 3.4048 - val_acc: 0.2559
Epoch 12/50
 - 27s - loss: 2.6524 - acc: 0.3771 - val_loss: 3.4263 - val_acc: 0.2619
Epoch 13/50
 - 27s - loss: 2.5082 - acc: 0.4021 - val_loss: 3.4128 - val_acc: 0.2707
Epoch 14/50
 - 27s - loss: 2.3572 - acc: 0.4321 - val_loss: 3.6532 - val_acc: 0.2450
Epoch 15/50
 - 27s - loss: 2.2061 - acc: 0.4616 - val_loss: 3.5671 - val_acc: 0.2605
Epoch 16/50
 - 27s - loss: 2.0412 - acc: 0.4949 - val_loss: 3.6727 - val_acc: 0.2636
Epoch 17/50
 - 27s - loss: 1.8910 - acc: 0.5251 - val_loss: 3.8328 - val_acc: 0.2529
Epoch 18/50
 - 27s - loss: 1.7205 - acc: 0.5609 - val_loss: 4.0561 - val_acc: 0.2528
Epoch 19/50
 - 27s - loss: 1.5506 - acc: 0.5963 - val_loss: 4.1815 - val_acc: 0.2455
Epoch 20/50
 - 27s - loss: 1.4041 - acc: 0.6302 - val_loss: 4.3340 - val_acc: 0.2473
Epoch 21/50
 - 27s - loss: 1.2481 - acc: 0.6655 - val_loss: 4.4310 - val_acc: 0.2436
Epoch 22/50
 - 27s - loss: 1.0960 - acc: 0.7002 - val_loss: 4.8025 - val_acc: 0.2470
Epoch 23/50
 - 27s - loss: 0.9752 - acc: 0.7304 - val_loss: 4.8466 - val_acc: 0.2405
Epoch 24/50
 - 27s - loss: 0.8628 - acc: 0.7578 - val_loss: 5.1564 - val_acc: 0.2459
Epoch 25/50
 - 27s - loss: 0.7510 - acc: 0.7869 - val_loss: 5.4082 - val_acc: 0.2331
Epoch 26/50
 - 27s - loss: 0.6549 - acc: 0.8125 - val_loss: 5.5718 - val_acc: 0.2382
Epoch 27/50
 - 27s - loss: 0.5996 - acc: 0.8263 - val_loss: 5.6174 - val_acc: 0.2345
Epoch 28/50
 - 28s - loss: 0.5230 - acc: 0.8458 - val_loss: 5.9539 - val_acc: 0.2439
Epoch 29/50
 - 27s - loss: 0.4611 - acc: 0.8638 - val_loss: 6.2343 - val_acc: 0.2414
Epoch 30/50
 - 27s - loss: 0.4150 - acc: 0.8761 - val_loss: 6.3305 - val_acc: 0.2292
Epoch 31/50
 - 27s - loss: 0.3738 - acc: 0.8880 - val_loss: 6.3769 - val_acc: 0.2372
Epoch 32/50
 - 27s - loss: 0.3390 - acc: 0.8978 - val_loss: 6.5597 - val_acc: 0.2406
Epoch 33/50
 - 27s - loss: 0.3029 - acc: 0.9078 - val_loss: 6.8350 - val_acc: 0.2393
Epoch 34/50
 - 27s - loss: 0.2762 - acc: 0.9166 - val_loss: 6.7992 - val_acc: 0.2348
Epoch 35/50
 - 27s - loss: 0.2599 - acc: 0.9207 - val_loss: 6.9901 - val_acc: 0.2387
Epoch 36/50
 - 27s - loss: 0.2448 - acc: 0.9260 - val_loss: 7.0662 - val_acc: 0.2343
Epoch 37/50
 - 27s - loss: 0.2281 - acc: 0.9313 - val_loss: 7.2009 - val_acc: 0.2309
Epoch 38/50
 - 27s - loss: 0.2122 - acc: 0.9351 - val_loss: 7.0842 - val_acc: 0.2409
Epoch 39/50
 - 27s - loss: 0.1932 - acc: 0.9408 - val_loss: 7.3955 - val_acc: 0.2445
Epoch 40/50
 - 27s - loss: 0.1806 - acc: 0.9450 - val_loss: 7.4028 - val_acc: 0.2377
Epoch 41/50
 - 27s - loss: 0.1829 - acc: 0.9447 - val_loss: 7.4580 - val_acc: 0.2410
Epoch 42/50
 - 27s - loss: 0.1604 - acc: 0.9506 - val_loss: 7.4622 - val_acc: 0.2364
Epoch 43/50
 - 28s - loss: 0.1617 - acc: 0.9498 - val_loss: 7.6037 - val_acc: 0.2374
Epoch 44/50
 - 27s - loss: 0.1461 - acc: 0.9553 - val_loss: 7.7078 - val_acc: 0.2400
Epoch 45/50
 - 27s - loss: 0.1443 - acc: 0.9565 - val_loss: 7.7113 - val_acc: 0.2414
Epoch 46/50
 - 27s - loss: 0.1349 - acc: 0.9586 - val_loss: 7.8232 - val_acc: 0.2387
Epoch 47/50
 - 27s - loss: 0.1333 - acc: 0.9595 - val_loss: 7.7686 - val_acc: 0.2387
Epoch 48/50
 - 27s - loss: 0.1275 - acc: 0.9600 - val_loss: 7.9199 - val_acc: 0.2395
Epoch 49/50
 - 28s - loss: 0.1233 - acc: 0.9632 - val_loss: 7.9359 - val_acc: 0.2420
Epoch 50/50
 - 28s - loss: 0.1148 - acc: 0.9654 - val_loss: 7.8467 - val_acc: 0.2458
training history:
{'val_loss': [4.802150145721436, 4.319002763366699, 4.028629375457764, 3.8733726806640627, 3.77743962020874, 3.5648910846710207, 3.587394143676758, 3.584667463684082, 3.445231705093384, 3.389045559310913, 3.40478484916687, 3.4263205940246584, 3.412798645019531, 3.6531652458190917, 3.567090179824829, 3.672656703567505, 3.832791404724121, 4.05612839050293, 4.181546424865723, 4.333966454315186, 4.431041455841064, 4.802508976745606, 4.846592869567871, 5.156403064727783, 5.408181090545654, 5.571848065948486, 5.617424819946289, 5.953897731018066, 6.234276574707032, 6.330459272003174, 6.376880296325684, 6.559690717315674, 6.8350052001953125, 6.799180100250244, 6.990091819763183, 7.066202619934082, 7.200945965576172, 7.084179830169678, 7.395481330871582, 7.402830665588379, 7.45802876586914, 7.462247570800781, 7.603712252807617, 7.707758100128173, 7.711291270446777, 7.82323791885376, 7.768634794616699, 7.919920701599121, 7.9358757217407225, 7.846678402709961], 'val_acc': [0.0496, 0.1085, 0.1472, 0.1612, 0.1854, 0.2213, 0.2247, 0.2216, 0.241, 0.2546, 0.2559, 0.2619, 0.2707, 0.245, 0.2605, 0.2636, 0.2529, 0.2528, 0.2455, 0.2473, 0.2436, 0.247, 0.2405, 0.2459, 0.2331, 0.2382, 0.2345, 0.2439, 0.2414, 0.2292, 0.2372, 0.2406, 0.2393, 0.2348, 0.2387, 0.2343, 0.2309, 0.2409, 0.2445, 0.2377, 0.241, 0.2364, 0.2374, 0.24, 0.2414, 0.2387, 0.2387, 0.2395, 0.242, 0.2458], 'loss': [5.027942774658203, 4.55660628982544, 4.201362543029785, 3.928507869415283, 3.7145535584259033, 3.525651599884033, 3.366300153503418, 3.2124427853393556, 3.0718918844604493, 2.9333165729522706, 2.7906430546569823, 2.6524575216674804, 2.508239186553955, 2.3572498412322997, 2.206196700286865, 2.041327980117798, 1.8909684407806397, 1.7203673597717286, 1.5507665492630005, 1.404124804763794, 1.2480301552581787, 1.0960121162414551, 0.9752823771476745, 0.8628433200073242, 0.7511337585449219, 0.6547094338417053, 0.5997755706834793, 0.5229804786396026, 0.46110950233459475, 0.41504121815681455, 0.37378401215076446, 0.33892978451013567, 0.3029645563614368, 0.27619682683229446, 0.2597687504053116, 0.24473449768066408, 0.22803174136161805, 0.21228677930474282, 0.19314932196736337, 0.1805901943397522, 0.18288738708615304, 0.1603833937460184, 0.1617126084640622, 0.14612411622703075, 0.14429139811754227, 0.13489372890770435, 0.13332915277421475, 0.12745644933521746, 0.12330661187857389, 0.11481406552761793], 'acc': [0.02907, 0.0813, 0.12633, 0.16462, 0.198, 0.22594, 0.25369, 0.27944, 0.30123, 0.32766, 0.35044, 0.37712, 0.40212, 0.43205, 0.46156, 0.49493, 0.52512, 0.56092, 0.59622, 0.63016, 0.66553, 0.70017, 0.73042, 0.7578, 0.78687, 0.81259, 0.82627, 0.84582, 0.86377, 0.8761, 0.888, 0.89783, 0.90773, 0.91667, 0.92067, 0.92599, 0.93128, 0.93506, 0.94084, 0.94503, 0.94471, 0.95057, 0.94975, 0.95528, 0.95653, 0.95856, 0.95949, 0.96006, 0.96316, 0.9654]}
Training time: 
1383.3805093765259
Evaluation results:  [7.8844700561523435, 0.2457]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_6 (Conv2D)            (None, 58, 61, 48)        4080      
_________________________________________________________________
activation_4 (Activation)    (None, 58, 61, 48)        0         
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 28, 30, 48)        0         
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 24, 30, 128)       30848     
_________________________________________________________________
activation_5 (Activation)    (None, 24, 30, 128)       0         
_________________________________________________________________
max_pooling2d_5 (MaxPooling2 (None, 11, 14, 128)       0         
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 9, 14, 192)        73920     
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 7, 14, 192)        110784    
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 5, 14, 128)        73856     
_________________________________________________________________
activation_6 (Activation)    (None, 5, 14, 128)        0         
_________________________________________________________________
max_pooling2d_6 (MaxPooling2 (None, 2, 6, 128)         0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 1536)              0         
_________________________________________________________________
dense_4 (Dense)              (None, 2048)              3147776   
_________________________________________________________________
dense_5 (Dense)              (None, 2048)              4196352   
_________________________________________________________________
dense_6 (Dense)              (None, 200)               409800    
=================================================================
Total params: 8,047,416
Trainable params: 8,047,416
Non-trainable params: 0
_________________________________________________________________
Epoch 1/50
 - 28s - loss: 5.3248 - acc: 0.0046 - val_loss: 5.3214 - val_acc: 0.0050
Epoch 2/50
 - 28s - loss: 5.3200 - acc: 0.0050 - val_loss: 5.3197 - val_acc: 0.0050
Epoch 3/50
 - 28s - loss: 5.3189 - acc: 0.0050 - val_loss: 5.3165 - val_acc: 0.0050
Epoch 4/50
 - 28s - loss: 5.3180 - acc: 0.0048 - val_loss: 5.3154 - val_acc: 0.0050
Epoch 5/50
 - 28s - loss: 5.3172 - acc: 0.0045 - val_loss: 5.3123 - val_acc: 0.0050
Epoch 6/50
 - 28s - loss: 5.3156 - acc: 0.0050 - val_loss: 5.3117 - val_acc: 0.0050
Epoch 7/50
 - 28s - loss: 5.3138 - acc: 0.0048 - val_loss: 5.3102 - val_acc: 0.0050
Epoch 8/50
 - 28s - loss: 5.3130 - acc: 0.0047 - val_loss: 5.3091 - val_acc: 0.0050
Epoch 9/50
 - 28s - loss: 5.3121 - acc: 0.0048 - val_loss: 5.3086 - val_acc: 0.0050
Epoch 10/50
 - 28s - loss: 5.3114 - acc: 0.0048 - val_loss: 5.3089 - val_acc: 0.0050
Epoch 11/50
 - 28s - loss: 5.3102 - acc: 0.0052 - val_loss: 5.3073 - val_acc: 0.0050
Epoch 12/50
 - 28s - loss: 5.3094 - acc: 0.0050 - val_loss: 5.3069 - val_acc: 0.0050
Epoch 13/50
 - 28s - loss: 5.3089 - acc: 0.0047 - val_loss: 5.3065 - val_acc: 0.0050
Epoch 14/50
 - 27s - loss: 5.3084 - acc: 0.0044 - val_loss: 5.3050 - val_acc: 0.0050
Epoch 15/50
 - 28s - loss: 5.3077 - acc: 0.0047 - val_loss: 5.3069 - val_acc: 0.0069
Epoch 16/50
 - 28s - loss: 5.3071 - acc: 0.0044 - val_loss: 5.3048 - val_acc: 0.0050
Epoch 17/50
 - 28s - loss: 5.3069 - acc: 0.0044 - val_loss: 5.3044 - val_acc: 0.0050
Epoch 18/50
 - 28s - loss: 5.3065 - acc: 0.0045 - val_loss: 5.3033 - val_acc: 0.0050
Epoch 19/50
 - 27s - loss: 5.3059 - acc: 0.0046 - val_loss: 5.3033 - val_acc: 0.0050
Epoch 20/50
 - 27s - loss: 5.3054 - acc: 0.0048 - val_loss: 5.3035 - val_acc: 0.0047
Epoch 21/50
 - 28s - loss: 5.3048 - acc: 0.0047 - val_loss: 5.3031 - val_acc: 0.0050
Epoch 22/50
 - 28s - loss: 5.3049 - acc: 0.0049 - val_loss: 5.3016 - val_acc: 0.0050
Epoch 23/50
 - 28s - loss: 5.3044 - acc: 0.0046 - val_loss: 5.3023 - val_acc: 0.0050
Epoch 24/50
 - 28s - loss: 5.3042 - acc: 0.0046 - val_loss: 5.3027 - val_acc: 0.0050
Epoch 25/50
 - 28s - loss: 5.3038 - acc: 0.0047 - val_loss: 5.3023 - val_acc: 0.0050
Epoch 26/50
 - 28s - loss: 5.3036 - acc: 0.0048 - val_loss: 5.3013 - val_acc: 0.0050
Epoch 27/50
 - 28s - loss: 5.3031 - acc: 0.0044 - val_loss: 5.3011 - val_acc: 0.0050
Epoch 28/50
 - 28s - loss: 5.3030 - acc: 0.0048 - val_loss: 5.3016 - val_acc: 0.0050
Epoch 29/50
 - 28s - loss: 5.3027 - acc: 0.0048 - val_loss: 5.3008 - val_acc: 0.0050
Epoch 30/50
 - 28s - loss: 5.3024 - acc: 0.0049 - val_loss: 5.3011 - val_acc: 0.0050
Epoch 31/50
 - 28s - loss: 5.3026 - acc: 0.0045 - val_loss: 5.3004 - val_acc: 0.0051
Epoch 32/50
 - 28s - loss: 5.3023 - acc: 0.0050 - val_loss: 5.3005 - val_acc: 0.0050
Epoch 33/50
 - 28s - loss: 5.3022 - acc: 0.0049 - val_loss: 5.3001 - val_acc: 0.0050
Epoch 34/50
 - 28s - loss: 5.3020 - acc: 0.0045 - val_loss: 5.3001 - val_acc: 0.0050
Epoch 35/50
 - 28s - loss: 5.3017 - acc: 0.0047 - val_loss: 5.3001 - val_acc: 0.0050
Epoch 36/50
 - 27s - loss: 5.3019 - acc: 0.0045 - val_loss: 5.2997 - val_acc: 0.0050
Epoch 37/50
 - 28s - loss: 5.3016 - acc: 0.0046 - val_loss: 5.2998 - val_acc: 0.0050
Epoch 38/50
 - 28s - loss: 5.3014 - acc: 0.0046 - val_loss: 5.2997 - val_acc: 0.0050
Epoch 39/50
 - 28s - loss: 5.3015 - acc: 0.0048 - val_loss: 5.2994 - val_acc: 0.0050
Epoch 40/50
 - 28s - loss: 5.3012 - acc: 0.0046 - val_loss: 5.2997 - val_acc: 0.0050
Epoch 41/50
 - 28s - loss: 5.3013 - acc: 0.0044 - val_loss: 5.2992 - val_acc: 0.0050
Epoch 42/50
 - 28s - loss: 5.3011 - acc: 0.0051 - val_loss: 5.2992 - val_acc: 0.0050
Epoch 43/50
 - 28s - loss: 5.3011 - acc: 0.0044 - val_loss: 5.2992 - val_acc: 0.0050
Epoch 44/50
 - 27s - loss: 5.3008 - acc: 0.0044 - val_loss: 5.2993 - val_acc: 0.0050
Epoch 45/50
 - 28s - loss: 5.3008 - acc: 0.0048 - val_loss: 5.2991 - val_acc: 0.0050
Epoch 46/50
 - 28s - loss: 5.3007 - acc: 0.0044 - val_loss: 5.2992 - val_acc: 0.0050
Epoch 47/50
 - 28s - loss: 5.3007 - acc: 0.0048 - val_loss: 5.2990 - val_acc: 0.0050
Epoch 48/50
 - 28s - loss: 5.3006 - acc: 0.0046 - val_loss: 5.2989 - val_acc: 0.0050
Epoch 49/50
 - 28s - loss: 5.3006 - acc: 0.0044 - val_loss: 5.2988 - val_acc: 0.0050
Epoch 50/50
 - 27s - loss: 5.3004 - acc: 0.0046 - val_loss: 5.2988 - val_acc: 0.0050
training history:
{'val_loss': [5.321360398101807, 5.3196592613220215, 5.316489628601074, 5.31541823348999, 5.312269560241699, 5.311654879760742, 5.310157221984864, 5.309128142547608, 5.308580578613281, 5.308865476989746, 5.307327551269531, 5.306910586547851, 5.306519484710694, 5.30502332611084, 5.306884663391113, 5.30479949798584, 5.304434171295166, 5.303290054321289, 5.3033131492614745, 5.303473388671875, 5.303117868041992, 5.301629917907714, 5.3023201301574705, 5.302695544433594, 5.302272856140137, 5.301291561889649, 5.3010895057678225, 5.301648468017578, 5.300820768737793, 5.301070466613769, 5.300421670532226, 5.300454384613037, 5.3000718704223635, 5.300100535583496, 5.30009506225586, 5.2996997596740725, 5.299794692230225, 5.299658792114258, 5.299362928771973, 5.299698274230957, 5.299193991851807, 5.299228623962402, 5.2991994247436525, 5.299309992980957, 5.2991380004882815, 5.299172943878173, 5.298962208557129, 5.29893846206665, 5.2987989143371585, 5.298754429626465], 'val_acc': [0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.0069, 0.005, 0.005, 0.005, 0.005, 0.0047, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.0051, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005], 'loss': [5.3247849401855465, 5.319995193176269, 5.318893075866699, 5.317991868896485, 5.317154097442627, 5.315563068237305, 5.313841755676269, 5.313022448883057, 5.312075877075196, 5.311443218994141, 5.310177254943848, 5.309406393890381, 5.308886887207032, 5.308354268188476, 5.307651223602295, 5.307130145263672, 5.3068928515625, 5.306459761047363, 5.305878772583008, 5.305359430389404, 5.3048170655822755, 5.304928181915283, 5.304433534851074, 5.304198222961426, 5.303844624938965, 5.3036003117370605, 5.303118974609375, 5.303020929718017, 5.302735325012207, 5.302395169677735, 5.302583560333252, 5.302318192138672, 5.302200133361817, 5.301952753448487, 5.301747881317139, 5.301887771453857, 5.301604031066894, 5.301416296081543, 5.301498038787842, 5.301195148010254, 5.301325223083496, 5.301093842926026, 5.301072174377442, 5.300817483215332, 5.300815762786865, 5.30071832824707, 5.300741982727051, 5.300581589050293, 5.300573682250977, 5.300440252685547], 'acc': [0.00462, 0.00496, 0.00498, 0.00483, 0.00454, 0.00497, 0.0048, 0.00465, 0.00484, 0.00482, 0.0052, 0.00501, 0.0047, 0.00443, 0.00468, 0.0044, 0.00445, 0.00454, 0.00458, 0.00481, 0.00473, 0.0049, 0.00459, 0.00465, 0.00473, 0.00477, 0.00443, 0.00478, 0.00479, 0.00495, 0.00446, 0.00499, 0.00487, 0.00454, 0.00466, 0.00451, 0.00456, 0.00462, 0.00475, 0.0046, 0.00439, 0.00512, 0.00444, 0.00436, 0.00476, 0.0044, 0.00485, 0.00461, 0.00438, 0.00458]}
Training time: 
1378.359936952591
Evaluation results:  [5.29875491027832, 0.005]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_11 (Conv2D)           (None, 58, 61, 48)        4080      
_________________________________________________________________
activation_7 (Activation)    (None, 58, 61, 48)        0         
_________________________________________________________________
max_pooling2d_7 (MaxPooling2 (None, 28, 30, 48)        0         
_________________________________________________________________
conv2d_12 (Conv2D)           (None, 24, 30, 128)       30848     
_________________________________________________________________
activation_8 (Activation)    (None, 24, 30, 128)       0         
_________________________________________________________________
max_pooling2d_8 (MaxPooling2 (None, 11, 14, 128)       0         
_________________________________________________________________
conv2d_13 (Conv2D)           (None, 9, 14, 192)        73920     
_________________________________________________________________
conv2d_14 (Conv2D)           (None, 7, 14, 192)        110784    
_________________________________________________________________
conv2d_15 (Conv2D)           (None, 5, 14, 128)        73856     
_________________________________________________________________
activation_9 (Activation)    (None, 5, 14, 128)        0         
_________________________________________________________________
max_pooling2d_9 (MaxPooling2 (None, 2, 6, 128)         0         
_________________________________________________________________
flatten_3 (Flatten)          (None, 1536)              0         
_________________________________________________________________
dense_7 (Dense)              (None, 2048)              3147776   
_________________________________________________________________
dense_8 (Dense)              (None, 2048)              4196352   
_________________________________________________________________
dense_9 (Dense)              (None, 200)               409800    
=================================================================
Total params: 8,047,416
Trainable params: 8,047,416
Non-trainable params: 0
_________________________________________________________________
Epoch 1/50
 - 28s - loss: 4.7991 - acc: 0.0539 - val_loss: 4.5064 - val_acc: 0.0868
Epoch 2/50
 - 28s - loss: 4.2787 - acc: 0.1179 - val_loss: 4.0810 - val_acc: 0.1414
Epoch 3/50
 - 28s - loss: 3.9668 - acc: 0.1612 - val_loss: 3.9002 - val_acc: 0.1712
Epoch 4/50
 - 28s - loss: 3.7544 - acc: 0.1932 - val_loss: 3.7185 - val_acc: 0.1934
Epoch 5/50
 - 28s - loss: 3.5927 - acc: 0.2203 - val_loss: 3.7031 - val_acc: 0.1983
Epoch 6/50
 - 27s - loss: 3.4620 - acc: 0.2416 - val_loss: 3.4975 - val_acc: 0.2276
Epoch 7/50
 - 27s - loss: 3.3492 - acc: 0.2575 - val_loss: 3.4355 - val_acc: 0.2406
Epoch 8/50
 - 28s - loss: 3.2565 - acc: 0.2763 - val_loss: 3.4531 - val_acc: 0.2374
Epoch 9/50
 - 28s - loss: 3.1652 - acc: 0.2915 - val_loss: 3.3264 - val_acc: 0.2572
Epoch 10/50
 - 28s - loss: 3.0923 - acc: 0.3040 - val_loss: 3.3393 - val_acc: 0.2614
Epoch 11/50
 - 28s - loss: 3.0223 - acc: 0.3184 - val_loss: 3.2383 - val_acc: 0.2746
Epoch 12/50
 - 28s - loss: 2.9504 - acc: 0.3325 - val_loss: 3.2509 - val_acc: 0.2707
Epoch 13/50
 - 28s - loss: 2.8898 - acc: 0.3428 - val_loss: 3.2384 - val_acc: 0.2732
Epoch 14/50
 - 28s - loss: 2.8241 - acc: 0.3546 - val_loss: 3.1925 - val_acc: 0.2870
Epoch 15/50
 - 28s - loss: 2.7728 - acc: 0.3650 - val_loss: 3.1962 - val_acc: 0.2811
Epoch 16/50
 - 28s - loss: 2.7196 - acc: 0.3741 - val_loss: 3.1356 - val_acc: 0.2922
Epoch 17/50
 - 28s - loss: 2.6636 - acc: 0.3843 - val_loss: 3.1613 - val_acc: 0.2864
Epoch 18/50
 - 28s - loss: 2.6127 - acc: 0.3957 - val_loss: 3.1432 - val_acc: 0.2930
Epoch 19/50
 - 28s - loss: 2.5647 - acc: 0.4043 - val_loss: 3.1572 - val_acc: 0.2932
Epoch 20/50
 - 28s - loss: 2.5144 - acc: 0.4152 - val_loss: 3.2053 - val_acc: 0.2810
Epoch 21/50
 - 28s - loss: 2.4634 - acc: 0.4250 - val_loss: 3.1046 - val_acc: 0.3009
Epoch 22/50
 - 28s - loss: 2.4172 - acc: 0.4349 - val_loss: 3.1204 - val_acc: 0.2984
Epoch 23/50
 - 28s - loss: 2.3747 - acc: 0.4433 - val_loss: 3.1012 - val_acc: 0.3032
Epoch 24/50
 - 28s - loss: 2.3248 - acc: 0.4539 - val_loss: 3.0840 - val_acc: 0.3066
Epoch 25/50
 - 28s - loss: 2.2747 - acc: 0.4634 - val_loss: 3.0952 - val_acc: 0.3064
Epoch 26/50
 - 28s - loss: 2.2337 - acc: 0.4721 - val_loss: 3.0930 - val_acc: 0.3048
Epoch 27/50
 - 28s - loss: 2.1899 - acc: 0.4810 - val_loss: 3.0640 - val_acc: 0.3108
Epoch 28/50
 - 28s - loss: 2.1423 - acc: 0.4908 - val_loss: 3.1064 - val_acc: 0.3029
Epoch 29/50
 - 28s - loss: 2.0963 - acc: 0.5029 - val_loss: 3.1200 - val_acc: 0.3028
Epoch 30/50
 - 28s - loss: 2.0502 - acc: 0.5120 - val_loss: 3.1345 - val_acc: 0.3058
Epoch 31/50
 - 28s - loss: 2.0052 - acc: 0.5221 - val_loss: 3.1064 - val_acc: 0.3074
Epoch 32/50
 - 28s - loss: 1.9597 - acc: 0.5316 - val_loss: 3.1305 - val_acc: 0.3054
Epoch 33/50
 - 28s - loss: 1.9140 - acc: 0.5426 - val_loss: 3.1257 - val_acc: 0.3063
Epoch 34/50
 - 28s - loss: 1.8655 - acc: 0.5540 - val_loss: 3.1129 - val_acc: 0.3092
Epoch 35/50
 - 28s - loss: 1.8202 - acc: 0.5641 - val_loss: 3.1610 - val_acc: 0.3039
Epoch 36/50
 - 28s - loss: 1.7802 - acc: 0.5742 - val_loss: 3.1400 - val_acc: 0.3081
Epoch 37/50
 - 28s - loss: 1.7305 - acc: 0.5869 - val_loss: 3.1563 - val_acc: 0.3042
Epoch 38/50
 - 28s - loss: 1.6868 - acc: 0.5972 - val_loss: 3.1731 - val_acc: 0.3024
Epoch 39/50
 - 28s - loss: 1.6411 - acc: 0.6068 - val_loss: 3.1969 - val_acc: 0.3071
Epoch 40/50
 - 28s - loss: 1.5902 - acc: 0.6198 - val_loss: 3.1817 - val_acc: 0.3085
Epoch 41/50
 - 28s - loss: 1.5472 - acc: 0.6296 - val_loss: 3.2290 - val_acc: 0.3031
Epoch 42/50
 - 28s - loss: 1.5035 - acc: 0.6421 - val_loss: 3.3193 - val_acc: 0.2873
Epoch 43/50
 - 28s - loss: 1.4545 - acc: 0.6532 - val_loss: 3.2396 - val_acc: 0.3032
Epoch 44/50
 - 28s - loss: 1.4087 - acc: 0.6657 - val_loss: 3.2656 - val_acc: 0.2931
Epoch 45/50
 - 28s - loss: 1.3659 - acc: 0.6768 - val_loss: 3.3071 - val_acc: 0.2951
Epoch 46/50
 - 28s - loss: 1.3159 - acc: 0.6904 - val_loss: 3.3319 - val_acc: 0.2913
Epoch 47/50
 - 28s - loss: 1.2695 - acc: 0.7020 - val_loss: 3.3284 - val_acc: 0.2917
Epoch 48/50
 - 28s - loss: 1.2280 - acc: 0.7132 - val_loss: 3.3533 - val_acc: 0.2870
Epoch 49/50
 - 28s - loss: 1.1833 - acc: 0.7255 - val_loss: 3.3329 - val_acc: 0.2954
Epoch 50/50
 - 28s - loss: 1.1358 - acc: 0.7376 - val_loss: 3.3323 - val_acc: 0.2961
training history:
{'val_loss': [4.50641242980957, 4.080961299133301, 3.9001641525268553, 3.7185497467041015, 3.70309655380249, 3.497489874267578, 3.435537191772461, 3.4531142475128176, 3.3264155616760256, 3.3393305095672607, 3.238317280960083, 3.2509261989593505, 3.238446600341797, 3.1924769538879394, 3.1962187782287597, 3.1356182540893554, 3.161289155578613, 3.143197211074829, 3.157199722290039, 3.205324294281006, 3.1045649353027343, 3.120425791931152, 3.101199929046631, 3.0840253486633302, 3.095154731750488, 3.0930009521484374, 3.0640047481536867, 3.1064217945098878, 3.1199977317810057, 3.13445712890625, 3.106368835449219, 3.1304628242492676, 3.125696747970581, 3.112937915802002, 3.161016756439209, 3.1399990352630613, 3.156315447998047, 3.1730595794677736, 3.1969256439208986, 3.181725079345703, 3.229022268676758, 3.3192644073486326, 3.239585767364502, 3.2656281326293946, 3.3071005489349363, 3.3318628677368163, 3.3283596450805666, 3.3532577362060545, 3.33289921875, 3.3322918724060058], 'val_acc': [0.0868, 0.1414, 0.1712, 0.1934, 0.1983, 0.2276, 0.2406, 0.2374, 0.2572, 0.2614, 0.2746, 0.2707, 0.2732, 0.287, 0.2811, 0.2922, 0.2864, 0.293, 0.2932, 0.281, 0.3009, 0.2984, 0.3032, 0.3066, 0.3064, 0.3048, 0.3108, 0.3029, 0.3028, 0.3058, 0.3074, 0.3054, 0.3063, 0.3092, 0.3039, 0.3081, 0.3042, 0.3024, 0.3071, 0.3085, 0.3031, 0.2873, 0.3032, 0.2931, 0.2951, 0.2913, 0.2917, 0.287, 0.2954, 0.2961], 'loss': [4.79899731842041, 4.2786703125, 3.9668037005615235, 3.754398837966919, 3.592742810058594, 3.4618103286743165, 3.3491796217346192, 3.256503036193848, 3.165059450073242, 3.092350253982544, 3.02239104888916, 2.9504544551086425, 2.8897877992248535, 2.8242185961151125, 2.7729295777893066, 2.719291975326538, 2.6637830215454104, 2.612625697784424, 2.5645549054718018, 2.5144281019592287, 2.463359164199829, 2.4172969264221194, 2.3749919497680665, 2.324877025375366, 2.274773285293579, 2.2336637561798094, 2.189785591049194, 2.142249892120361, 2.0962730181884766, 2.0503307233428956, 2.005023085708618, 1.959705359840393, 1.9140881688690186, 1.8655333771514893, 1.820388078956604, 1.7802796346282959, 1.730542366294861, 1.6868568809890747, 1.6411070929718017, 1.5902121015930175, 1.5471663766098023, 1.5035535840606689, 1.4545268745422364, 1.4085801947784424, 1.3658071443939208, 1.315729913253784, 1.269557929916382, 1.2281552680969239, 1.1831333209228516, 1.135916009941101], 'acc': [0.05395, 0.1179, 0.16126, 0.19323, 0.22028, 0.24164, 0.25745, 0.27631, 0.29159, 0.304, 0.31835, 0.33248, 0.34284, 0.35452, 0.36497, 0.37412, 0.38428, 0.3957, 0.4043, 0.4152, 0.42503, 0.43489, 0.44323, 0.45394, 0.46339, 0.47206, 0.48105, 0.49082, 0.50295, 0.51195, 0.52209, 0.53155, 0.54254, 0.55394, 0.56404, 0.57416, 0.58685, 0.59713, 0.60682, 0.61979, 0.62964, 0.64202, 0.65317, 0.66571, 0.67682, 0.69048, 0.70199, 0.71314, 0.72561, 0.73755]}
Training time: 
1379.8556761741638
Evaluation results:  [3.343467541503906, 0.2936]
