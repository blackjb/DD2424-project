trainingset size: 100000
validationset size: 10000
input_dims:  (64, 64, 3)
nb_labels:  200
nb_labels: 200
x_train.shape  (100000, 64, 64, 3)
y_train.shape  (100000, 200)
x_test.shape  (10000, 64, 64, 3)
y_test.shape  (10000, 200)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 58, 61, 48)        4080      
_________________________________________________________________
activation_1 (Activation)    (None, 58, 61, 48)        0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 28, 30, 48)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 24, 30, 128)       30848     
_________________________________________________________________
activation_2 (Activation)    (None, 24, 30, 128)       0         
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 11, 14, 128)       0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 9, 14, 192)        73920     
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 7, 14, 192)        110784    
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 5, 14, 128)        73856     
_________________________________________________________________
activation_3 (Activation)    (None, 5, 14, 128)        0         
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 2, 6, 128)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 1536)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 2048)              3147776   
_________________________________________________________________
dropout_1 (Dropout)          (None, 2048)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 2048)              4196352   
_________________________________________________________________
dropout_2 (Dropout)          (None, 2048)              0         
_________________________________________________________________
dense_3 (Dense)              (None, 200)               409800    
=================================================================
Total params: 8,047,416
Trainable params: 8,047,416
Non-trainable params: 0
_________________________________________________________________
Epoch 1/50
 - 36s - loss: 5.0487 - acc: 0.0268 - val_loss: 4.7753 - val_acc: 0.0559
Epoch 2/50
 - 28s - loss: 4.5660 - acc: 0.0779 - val_loss: 4.3163 - val_acc: 0.1081
Epoch 3/50
 - 28s - loss: 4.1957 - acc: 0.1255 - val_loss: 4.0471 - val_acc: 0.1452
Epoch 4/50
 - 28s - loss: 3.9080 - acc: 0.1673 - val_loss: 3.9854 - val_acc: 0.1503
Epoch 5/50
 - 27s - loss: 3.6620 - acc: 0.2044 - val_loss: 3.8676 - val_acc: 0.1783
Epoch 6/50
 - 28s - loss: 3.4414 - acc: 0.2397 - val_loss: 3.7007 - val_acc: 0.1997
Epoch 7/50
 - 28s - loss: 3.2193 - acc: 0.2756 - val_loss: 3.6020 - val_acc: 0.2203
Epoch 8/50
 - 28s - loss: 2.9998 - acc: 0.3124 - val_loss: 3.6597 - val_acc: 0.2205
Epoch 9/50
 - 28s - loss: 2.7663 - acc: 0.3555 - val_loss: 3.6883 - val_acc: 0.2310
Epoch 10/50
 - 27s - loss: 2.5021 - acc: 0.4029 - val_loss: 3.8506 - val_acc: 0.2268
Epoch 11/50
 - 28s - loss: 2.2200 - acc: 0.4551 - val_loss: 3.9420 - val_acc: 0.2157
Epoch 12/50
 - 28s - loss: 1.9112 - acc: 0.5169 - val_loss: 4.2057 - val_acc: 0.2011
Epoch 13/50
 - 28s - loss: 1.5803 - acc: 0.5871 - val_loss: 4.5443 - val_acc: 0.1997
Epoch 14/50
 - 28s - loss: 1.2647 - acc: 0.6579 - val_loss: 5.0518 - val_acc: 0.1978
Epoch 15/50
 - 28s - loss: 0.9835 - acc: 0.7238 - val_loss: 5.3843 - val_acc: 0.1955
Epoch 16/50
 - 28s - loss: 0.7521 - acc: 0.7828 - val_loss: 6.0071 - val_acc: 0.1880
Epoch 17/50
 - 28s - loss: 0.5824 - acc: 0.8280 - val_loss: 6.2773 - val_acc: 0.1901
Epoch 18/50
 - 28s - loss: 0.4667 - acc: 0.8630 - val_loss: 6.8520 - val_acc: 0.1898
Epoch 19/50
 - 28s - loss: 0.3844 - acc: 0.8857 - val_loss: 6.9429 - val_acc: 0.1929
Epoch 20/50
 - 28s - loss: 0.3108 - acc: 0.9061 - val_loss: 7.4041 - val_acc: 0.1963
Epoch 21/50
 - 28s - loss: 0.2764 - acc: 0.9167 - val_loss: 7.2815 - val_acc: 0.1888
Epoch 22/50
 - 28s - loss: 0.2281 - acc: 0.9320 - val_loss: 7.6707 - val_acc: 0.1868
Epoch 23/50
 - 28s - loss: 0.1937 - acc: 0.9413 - val_loss: 7.8244 - val_acc: 0.1900
Epoch 24/50
 - 28s - loss: 0.1805 - acc: 0.9457 - val_loss: 8.0716 - val_acc: 0.1933
Epoch 25/50
 - 28s - loss: 0.1525 - acc: 0.9538 - val_loss: 8.1116 - val_acc: 0.1887
Epoch 26/50
 - 28s - loss: 0.1289 - acc: 0.9616 - val_loss: 8.3220 - val_acc: 0.1946
Epoch 27/50
 - 28s - loss: 0.1288 - acc: 0.9620 - val_loss: 8.1994 - val_acc: 0.1895
Epoch 28/50
 - 28s - loss: 0.1154 - acc: 0.9657 - val_loss: 8.4026 - val_acc: 0.1960
Epoch 29/50
 - 28s - loss: 0.1106 - acc: 0.9673 - val_loss: 8.4289 - val_acc: 0.1951
Epoch 30/50
 - 28s - loss: 0.0853 - acc: 0.9752 - val_loss: 8.5627 - val_acc: 0.1953
Epoch 31/50
 - 28s - loss: 0.0921 - acc: 0.9721 - val_loss: 8.6844 - val_acc: 0.1977
Epoch 32/50
 - 28s - loss: 0.0829 - acc: 0.9751 - val_loss: 8.6440 - val_acc: 0.1955
Epoch 33/50
 - 28s - loss: 0.0852 - acc: 0.9749 - val_loss: 8.6446 - val_acc: 0.1938
Epoch 34/50
 - 28s - loss: 0.0727 - acc: 0.9791 - val_loss: 8.7085 - val_acc: 0.1953
Epoch 35/50
 - 28s - loss: 0.0625 - acc: 0.9815 - val_loss: 8.8096 - val_acc: 0.1950
Epoch 36/50
 - 28s - loss: 0.0720 - acc: 0.9795 - val_loss: 8.7688 - val_acc: 0.1917
Epoch 37/50
 - 28s - loss: 0.0635 - acc: 0.9813 - val_loss: 8.7713 - val_acc: 0.1972
Epoch 38/50
 - 28s - loss: 0.0601 - acc: 0.9825 - val_loss: 8.8471 - val_acc: 0.1970
Epoch 39/50
 - 28s - loss: 0.0586 - acc: 0.9827 - val_loss: 8.9903 - val_acc: 0.1952
Epoch 40/50
 - 28s - loss: 0.0562 - acc: 0.9834 - val_loss: 8.9728 - val_acc: 0.1981
Epoch 41/50
 - 28s - loss: 0.0541 - acc: 0.9841 - val_loss: 8.9481 - val_acc: 0.1956
Epoch 42/50
 - 28s - loss: 0.0619 - acc: 0.9817 - val_loss: 8.8628 - val_acc: 0.2023
Epoch 43/50
 - 28s - loss: 0.0504 - acc: 0.9852 - val_loss: 9.0534 - val_acc: 0.1987
Epoch 44/50
 - 28s - loss: 0.0561 - acc: 0.9839 - val_loss: 9.0850 - val_acc: 0.1946
Epoch 45/50
 - 28s - loss: 0.0385 - acc: 0.9887 - val_loss: 9.1487 - val_acc: 0.2039
Epoch 46/50
 - 28s - loss: 0.0387 - acc: 0.9894 - val_loss: 9.1373 - val_acc: 0.1987
Epoch 47/50
 - 28s - loss: 0.0502 - acc: 0.9854 - val_loss: 9.2733 - val_acc: 0.2021
Epoch 48/50
 - 28s - loss: 0.0500 - acc: 0.9854 - val_loss: 9.3133 - val_acc: 0.1958
Epoch 49/50
 - 28s - loss: 0.0423 - acc: 0.9884 - val_loss: 9.2275 - val_acc: 0.2008
Epoch 50/50
 - 28s - loss: 0.0397 - acc: 0.9888 - val_loss: 9.3254 - val_acc: 0.1971
training history:
{'val_loss': [4.7752568183898925, 4.316314665985107, 4.047137724304199, 3.985429239654541, 3.8676144813537596, 3.70073384475708, 3.602017510986328, 3.659691455078125, 3.6883263332366942, 3.8505546268463133, 3.942008883666992, 4.205685105133057, 4.54425436630249, 5.051797065734863, 5.384271357727051, 6.007098904418945, 6.277321908569336, 6.85197507019043, 6.942881069946289, 7.404102072906494, 7.281467942810059, 7.670704045104981, 7.82439242401123, 8.07162041015625, 8.111552670288086, 8.321993266296387, 8.199399069213868, 8.402564604187011, 8.428857772827149, 8.56274143371582, 8.684390954589844, 8.644016297912598, 8.644595278930664, 8.70848625793457, 8.809628172302245, 8.768780422973633, 8.77132691040039, 8.84705895767212, 8.990308003234864, 8.972759390258789, 8.948096035766602, 8.862816981506347, 9.05336463470459, 9.085045526123046, 9.148689294433593, 9.137319108581544, 9.27332014312744, 9.313258824157716, 9.227497991943359, 9.325432626342774], 'val_acc': [0.0559, 0.1081, 0.1452, 0.1503, 0.1783, 0.1997, 0.2203, 0.2205, 0.231, 0.2268, 0.2157, 0.2011, 0.1997, 0.1978, 0.1955, 0.188, 0.1901, 0.1898, 0.1929, 0.1963, 0.1888, 0.1868, 0.19, 0.1933, 0.1887, 0.1946, 0.1895, 0.196, 0.1951, 0.1953, 0.1977, 0.1955, 0.1938, 0.1953, 0.195, 0.1917, 0.1972, 0.197, 0.1952, 0.1981, 0.1956, 0.2023, 0.1987, 0.1946, 0.2039, 0.1987, 0.2021, 0.1958, 0.2008, 0.1971], 'loss': [5.048669042358399, 4.56602906539917, 4.195639216918945, 3.907985299682617, 3.6620733644104004, 3.4413919234466555, 3.2194755418395995, 3.0000285166931153, 2.766105319900513, 2.5022177318573, 2.219882272567749, 1.9110318564605713, 1.58018747341156, 1.2647976719093323, 0.9835659395980835, 0.7519764910984039, 0.5822734004592895, 0.46676444559574126, 0.3842179438829422, 0.3108435817170143, 0.2763987183451653, 0.2280563348388672, 0.19353621668219567, 0.1803694525897503, 0.152471143682003, 0.12892691993534566, 0.12875084695249797, 0.11539791362851858, 0.11064678944408894, 0.08528174017310143, 0.09213032365188002, 0.08296928239360452, 0.0852577880705893, 0.07265179019108414, 0.062465163993686436, 0.07198724862702191, 0.06346306392133236, 0.060147661232575776, 0.058567228278853, 0.05625416819479317, 0.05409142309788614, 0.06190500295538455, 0.05039189956121147, 0.05614515639267862, 0.03848166744441725, 0.0387044700184837, 0.05019105734229088, 0.05001156804632396, 0.04226552620966919, 0.03968060356080532], 'acc': [0.02676, 0.07789, 0.12555, 0.16728, 0.20438, 0.23973, 0.27556, 0.31234, 0.35553, 0.40283, 0.45508, 0.51689, 0.58714, 0.65791, 0.72383, 0.78282, 0.82805, 0.86302, 0.88578, 0.90611, 0.91667, 0.93205, 0.9413, 0.94568, 0.95384, 0.96155, 0.96199, 0.96566, 0.9673, 0.97516, 0.97207, 0.97513, 0.97487, 0.97911, 0.98151, 0.97947, 0.98133, 0.98246, 0.98268, 0.98339, 0.98412, 0.98172, 0.98517, 0.98389, 0.98873, 0.98944, 0.98543, 0.9854, 0.98843, 0.98884]}
Training time: 
1393.5968029499054
Evaluation results:  [9.325432635498046, 0.1971]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_6 (Conv2D)            (None, 58, 61, 48)        4080      
_________________________________________________________________
activation_4 (Activation)    (None, 58, 61, 48)        0         
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 28, 30, 48)        0         
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 24, 30, 128)       30848     
_________________________________________________________________
activation_5 (Activation)    (None, 24, 30, 128)       0         
_________________________________________________________________
max_pooling2d_5 (MaxPooling2 (None, 11, 14, 128)       0         
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 9, 14, 192)        73920     
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 7, 14, 192)        110784    
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 5, 14, 128)        73856     
_________________________________________________________________
activation_6 (Activation)    (None, 5, 14, 128)        0         
_________________________________________________________________
max_pooling2d_6 (MaxPooling2 (None, 2, 6, 128)         0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 1536)              0         
_________________________________________________________________
dense_4 (Dense)              (None, 2048)              3147776   
_________________________________________________________________
dense_5 (Dense)              (None, 2048)              4196352   
_________________________________________________________________
dense_6 (Dense)              (None, 200)               409800    
=================================================================
Total params: 8,047,416
Trainable params: 8,047,416
Non-trainable params: 0
_________________________________________________________________
Epoch 1/50
 - 28s - loss: 5.0331 - acc: 0.0284 - val_loss: 4.7199 - val_acc: 0.0577
Epoch 2/50
 - 28s - loss: 4.5321 - acc: 0.0836 - val_loss: 4.3045 - val_acc: 0.1098
Epoch 3/50
 - 28s - loss: 4.1460 - acc: 0.1338 - val_loss: 4.0792 - val_acc: 0.1368
Epoch 4/50
 - 28s - loss: 3.8587 - acc: 0.1755 - val_loss: 3.8089 - val_acc: 0.1818
Epoch 5/50
 - 28s - loss: 3.6104 - acc: 0.2136 - val_loss: 3.8111 - val_acc: 0.1848
Epoch 6/50
 - 28s - loss: 3.3910 - acc: 0.2482 - val_loss: 3.6090 - val_acc: 0.2127
Epoch 7/50
 - 28s - loss: 3.1771 - acc: 0.2850 - val_loss: 3.6235 - val_acc: 0.2238
Epoch 8/50
 - 28s - loss: 2.9489 - acc: 0.3227 - val_loss: 3.6248 - val_acc: 0.2308
Epoch 9/50
 - 28s - loss: 2.7113 - acc: 0.3647 - val_loss: 3.6506 - val_acc: 0.2328
Epoch 10/50
 - 28s - loss: 2.4462 - acc: 0.4132 - val_loss: 3.7919 - val_acc: 0.2233
Epoch 11/50
 - 28s - loss: 2.1604 - acc: 0.4674 - val_loss: 3.9306 - val_acc: 0.2276
Epoch 12/50
 - 28s - loss: 1.8457 - acc: 0.5281 - val_loss: 4.2247 - val_acc: 0.2148
Epoch 13/50
 - 28s - loss: 1.5231 - acc: 0.5995 - val_loss: 4.6259 - val_acc: 0.2122
Epoch 14/50
 - 28s - loss: 1.2058 - acc: 0.6701 - val_loss: 5.0152 - val_acc: 0.1972
Epoch 15/50
 - 28s - loss: 0.9418 - acc: 0.7335 - val_loss: 5.7108 - val_acc: 0.1908
Epoch 16/50
 - 28s - loss: 0.7089 - acc: 0.7959 - val_loss: 5.9953 - val_acc: 0.1870
Epoch 17/50
 - 28s - loss: 0.5616 - acc: 0.8354 - val_loss: 6.5250 - val_acc: 0.1933
Epoch 18/50
 - 28s - loss: 0.4524 - acc: 0.8644 - val_loss: 6.6869 - val_acc: 0.1984
Epoch 19/50
 - 28s - loss: 0.3646 - acc: 0.8904 - val_loss: 6.9128 - val_acc: 0.1965
Epoch 20/50
 - 28s - loss: 0.2862 - acc: 0.9134 - val_loss: 7.2832 - val_acc: 0.1972
Epoch 21/50
 - 28s - loss: 0.2526 - acc: 0.9239 - val_loss: 7.5107 - val_acc: 0.1932
Epoch 22/50
 - 28s - loss: 0.2323 - acc: 0.9294 - val_loss: 7.7255 - val_acc: 0.1921
Epoch 23/50
 - 28s - loss: 0.1958 - acc: 0.9405 - val_loss: 7.6805 - val_acc: 0.1990
Epoch 24/50
 - 28s - loss: 0.1756 - acc: 0.9468 - val_loss: 8.0735 - val_acc: 0.2015
Epoch 25/50
 - 28s - loss: 0.1653 - acc: 0.9504 - val_loss: 7.9018 - val_acc: 0.1987
Epoch 26/50
 - 28s - loss: 0.1379 - acc: 0.9586 - val_loss: 8.2292 - val_acc: 0.1936
Epoch 27/50
 - 28s - loss: 0.1184 - acc: 0.9643 - val_loss: 8.3485 - val_acc: 0.1952
Epoch 28/50
 - 28s - loss: 0.1008 - acc: 0.9698 - val_loss: 8.4640 - val_acc: 0.1965
Epoch 29/50
 - 28s - loss: 0.0969 - acc: 0.9715 - val_loss: 8.2963 - val_acc: 0.2023
Epoch 30/50
 - 28s - loss: 0.0916 - acc: 0.9730 - val_loss: 8.4582 - val_acc: 0.2023
Epoch 31/50
 - 28s - loss: 0.1020 - acc: 0.9696 - val_loss: 8.5006 - val_acc: 0.1989
Epoch 32/50
 - 28s - loss: 0.0883 - acc: 0.9742 - val_loss: 8.6072 - val_acc: 0.2007
Epoch 33/50
 - 28s - loss: 0.0774 - acc: 0.9765 - val_loss: 8.6209 - val_acc: 0.1947
Epoch 34/50
 - 28s - loss: 0.0752 - acc: 0.9772 - val_loss: 8.9074 - val_acc: 0.1956
Epoch 35/50
 - 28s - loss: 0.0734 - acc: 0.9780 - val_loss: 8.7025 - val_acc: 0.2008
Epoch 36/50
 - 28s - loss: 0.0686 - acc: 0.9798 - val_loss: 8.6794 - val_acc: 0.2037
Epoch 37/50
 - 28s - loss: 0.0552 - acc: 0.9841 - val_loss: 8.8987 - val_acc: 0.2049
Epoch 38/50
 - 28s - loss: 0.0478 - acc: 0.9859 - val_loss: 8.9430 - val_acc: 0.2031
Epoch 39/50
 - 28s - loss: 0.0440 - acc: 0.9873 - val_loss: 8.9071 - val_acc: 0.2012
Epoch 40/50
 - 28s - loss: 0.0489 - acc: 0.9861 - val_loss: 9.0220 - val_acc: 0.2010
Epoch 41/50
 - 28s - loss: 0.0595 - acc: 0.9824 - val_loss: 9.0283 - val_acc: 0.2027
Epoch 42/50
 - 28s - loss: 0.0501 - acc: 0.9852 - val_loss: 9.1463 - val_acc: 0.2056
Epoch 43/50
 - 28s - loss: 0.0478 - acc: 0.9862 - val_loss: 9.0489 - val_acc: 0.1991
Epoch 44/50
 - 28s - loss: 0.0678 - acc: 0.9802 - val_loss: 9.1407 - val_acc: 0.1948
Epoch 45/50
 - 28s - loss: 0.0518 - acc: 0.9852 - val_loss: 9.0916 - val_acc: 0.1987
Epoch 46/50
 - 28s - loss: 0.0382 - acc: 0.9891 - val_loss: 9.1541 - val_acc: 0.2017
Epoch 47/50
 - 28s - loss: 0.0383 - acc: 0.9891 - val_loss: 9.1036 - val_acc: 0.2021
Epoch 48/50
 - 28s - loss: 0.0350 - acc: 0.9901 - val_loss: 9.1662 - val_acc: 0.1971
Epoch 49/50
 - 28s - loss: 0.0486 - acc: 0.9857 - val_loss: 9.2026 - val_acc: 0.1969
Epoch 50/50
 - 28s - loss: 0.0407 - acc: 0.9887 - val_loss: 9.1982 - val_acc: 0.2049
training history:
{'val_loss': [4.719915957641602, 4.304528266143799, 4.079170608139038, 3.8089275421142577, 3.811085908126831, 3.6089539440155027, 3.623492794799805, 3.6247827041625977, 3.650588385009766, 3.791905317687988, 3.9305503425598145, 4.224689739227295, 4.6259307968139645, 5.015198747253418, 5.710820644378662, 5.9952704574584965, 6.525037586975098, 6.686937271118164, 6.912802543640137, 7.283162791442871, 7.510737707519532, 7.725526153564453, 7.680484744262695, 8.073508685302734, 7.901789920043945, 8.229216346740722, 8.348499423217774, 8.463966722106933, 8.296335009765626, 8.45824894104004, 8.500557894897462, 8.60724752960205, 8.620918721008302, 8.90741637878418, 8.70251474761963, 8.679402946472168, 8.898651010131836, 8.943034164428711, 8.907095556640625, 9.022010464477539, 9.02828059387207, 9.146258557128906, 9.04889539489746, 9.140687861633301, 9.091598104858399, 9.154142880249024, 9.103565487670899, 9.166211691284179, 9.202636706542968, 9.198187042236329], 'val_acc': [0.0577, 0.1098, 0.1368, 0.1818, 0.1848, 0.2127, 0.2238, 0.2308, 0.2328, 0.2233, 0.2276, 0.2148, 0.2122, 0.1972, 0.1908, 0.187, 0.1933, 0.1984, 0.1965, 0.1972, 0.1932, 0.1921, 0.199, 0.2015, 0.1987, 0.1936, 0.1952, 0.1965, 0.2023, 0.2023, 0.1989, 0.2007, 0.1947, 0.1956, 0.2008, 0.2037, 0.2049, 0.2031, 0.2012, 0.201, 0.2027, 0.2056, 0.1991, 0.1948, 0.1987, 0.2017, 0.2021, 0.1971, 0.1969, 0.2049], 'loss': [5.033156913146972, 4.531960094604492, 4.146049248352051, 3.858667255554199, 3.610491082611084, 3.39116885093689, 3.1771428007507323, 2.948921766204834, 2.7112381465148925, 2.4460835255432127, 2.1604131438446044, 1.845830046157837, 1.5231415928649903, 1.2056313709831237, 0.9417174438476562, 0.7089365054988861, 0.5616382920646668, 0.45235018763542173, 0.3646522159719467, 0.2862597265458107, 0.2526941375696659, 0.23231354008436203, 0.1958077614146471, 0.17563490248680114, 0.16535188795089723, 0.13787487791121006, 0.11837002527475357, 0.10079181047409773, 0.09695876566588879, 0.09162508787333966, 0.1019797981147468, 0.08831742819681764, 0.07742646138031035, 0.07516569777876139, 0.07341158362418414, 0.06858071802712977, 0.05518952251836658, 0.04776097943896428, 0.044019518966116014, 0.04886699541512877, 0.05953919212795794, 0.05006506829086691, 0.04774762125374749, 0.06773806356735528, 0.051770471714660524, 0.03813123767092824, 0.038328293274194, 0.03503085989246145, 0.048582529808059334, 0.04067043351093307], 'acc': [0.02835, 0.08358, 0.13384, 0.17553, 0.21359, 0.24814, 0.28505, 0.32269, 0.3647, 0.41327, 0.46734, 0.52803, 0.59947, 0.67011, 0.73351, 0.79591, 0.83534, 0.86444, 0.89038, 0.91342, 0.92392, 0.92936, 0.9405, 0.94677, 0.95036, 0.95856, 0.96427, 0.96978, 0.97154, 0.97304, 0.96962, 0.97419, 0.97654, 0.97725, 0.97799, 0.97981, 0.98413, 0.98591, 0.98725, 0.98606, 0.98244, 0.98523, 0.98624, 0.9802, 0.98523, 0.98908, 0.98914, 0.9901, 0.98567, 0.98872]}
Training time: 
1387.5605082511902
Evaluation results:  [9.198187065124511, 0.2049]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_11 (Conv2D)           (None, 58, 61, 48)        4080      
_________________________________________________________________
activation_7 (Activation)    (None, 58, 61, 48)        0         
_________________________________________________________________
max_pooling2d_7 (MaxPooling2 (None, 28, 30, 48)        0         
_________________________________________________________________
conv2d_12 (Conv2D)           (None, 24, 30, 128)       30848     
_________________________________________________________________
activation_8 (Activation)    (None, 24, 30, 128)       0         
_________________________________________________________________
max_pooling2d_8 (MaxPooling2 (None, 11, 14, 128)       0         
_________________________________________________________________
conv2d_13 (Conv2D)           (None, 9, 14, 192)        73920     
_________________________________________________________________
conv2d_14 (Conv2D)           (None, 7, 14, 192)        110784    
_________________________________________________________________
conv2d_15 (Conv2D)           (None, 5, 14, 128)        73856     
_________________________________________________________________
activation_9 (Activation)    (None, 5, 14, 128)        0         
_________________________________________________________________
max_pooling2d_9 (MaxPooling2 (None, 2, 6, 128)         0         
_________________________________________________________________
flatten_3 (Flatten)          (None, 1536)              0         
_________________________________________________________________
dense_7 (Dense)              (None, 2048)              3147776   
_________________________________________________________________
dense_8 (Dense)              (None, 2048)              4196352   
_________________________________________________________________
dense_9 (Dense)              (None, 200)               409800    
=================================================================
Total params: 8,047,416
Trainable params: 8,047,416
Non-trainable params: 0
_________________________________________________________________
Epoch 1/50
 - 28s - loss: 5.0376 - acc: 0.0287 - val_loss: 4.7445 - val_acc: 0.0600
Epoch 2/50
 - 28s - loss: 4.5473 - acc: 0.0823 - val_loss: 4.3663 - val_acc: 0.1005
Epoch 3/50
 - 28s - loss: 4.1876 - acc: 0.1278 - val_loss: 4.1013 - val_acc: 0.1373
Epoch 4/50
 - 28s - loss: 3.9010 - acc: 0.1694 - val_loss: 3.9209 - val_acc: 0.1583
Epoch 5/50
 - 28s - loss: 3.6547 - acc: 0.2051 - val_loss: 3.7372 - val_acc: 0.1948
Epoch 6/50
 - 28s - loss: 3.4309 - acc: 0.2411 - val_loss: 3.6751 - val_acc: 0.2025
Epoch 7/50
 - 28s - loss: 3.2222 - acc: 0.2748 - val_loss: 3.6658 - val_acc: 0.2123
Epoch 8/50
 - 28s - loss: 3.0002 - acc: 0.3111 - val_loss: 3.5961 - val_acc: 0.2231
Epoch 9/50
 - 28s - loss: 2.7651 - acc: 0.3523 - val_loss: 3.6840 - val_acc: 0.2234
Epoch 10/50
 - 28s - loss: 2.5143 - acc: 0.3979 - val_loss: 3.7496 - val_acc: 0.2285
Epoch 11/50
 - 28s - loss: 2.2252 - acc: 0.4529 - val_loss: 4.0585 - val_acc: 0.2167
Epoch 12/50
 - 28s - loss: 1.9184 - acc: 0.5142 - val_loss: 4.1233 - val_acc: 0.2173
Epoch 13/50
 - 28s - loss: 1.5972 - acc: 0.5822 - val_loss: 4.4580 - val_acc: 0.2095
Epoch 14/50
 - 28s - loss: 1.2690 - acc: 0.6567 - val_loss: 4.9703 - val_acc: 0.1988
Epoch 15/50
 - 28s - loss: 1.0029 - acc: 0.7186 - val_loss: 5.3623 - val_acc: 0.1950
Epoch 16/50
 - 28s - loss: 0.7548 - acc: 0.7834 - val_loss: 6.0786 - val_acc: 0.1973
Epoch 17/50
 - 28s - loss: 0.5987 - acc: 0.8248 - val_loss: 6.3805 - val_acc: 0.1893
Epoch 18/50
 - 28s - loss: 0.4829 - acc: 0.8551 - val_loss: 6.8328 - val_acc: 0.1890
Epoch 19/50
 - 28s - loss: 0.3880 - acc: 0.8836 - val_loss: 6.9235 - val_acc: 0.1898
Epoch 20/50
 - 28s - loss: 0.3239 - acc: 0.9027 - val_loss: 7.4415 - val_acc: 0.1957
Epoch 21/50
 - 28s - loss: 0.2700 - acc: 0.9200 - val_loss: 7.3578 - val_acc: 0.1964
Epoch 22/50
 - 28s - loss: 0.2293 - acc: 0.9314 - val_loss: 7.6474 - val_acc: 0.1969
Epoch 23/50
 - 28s - loss: 0.1986 - acc: 0.9401 - val_loss: 7.7671 - val_acc: 0.1916
Epoch 24/50
 - 28s - loss: 0.1803 - acc: 0.9455 - val_loss: 7.7226 - val_acc: 0.2011
Epoch 25/50
 - 28s - loss: 0.1606 - acc: 0.9509 - val_loss: 8.0141 - val_acc: 0.1993
Epoch 26/50
 - 28s - loss: 0.1401 - acc: 0.9577 - val_loss: 8.1236 - val_acc: 0.1974
Epoch 27/50
 - 28s - loss: 0.1405 - acc: 0.9581 - val_loss: 8.0777 - val_acc: 0.2013
Epoch 28/50
 - 28s - loss: 0.1110 - acc: 0.9666 - val_loss: 8.1310 - val_acc: 0.2014
Epoch 29/50
 - 28s - loss: 0.0960 - acc: 0.9713 - val_loss: 8.3794 - val_acc: 0.2002
Epoch 30/50
 - 28s - loss: 0.1038 - acc: 0.9688 - val_loss: 8.6134 - val_acc: 0.1864
Epoch 31/50
 - 28s - loss: 0.1002 - acc: 0.9700 - val_loss: 8.6076 - val_acc: 0.1958
Epoch 32/50
 - 28s - loss: 0.0791 - acc: 0.9765 - val_loss: 8.7443 - val_acc: 0.1881
Epoch 33/50
 - 28s - loss: 0.0706 - acc: 0.9789 - val_loss: 8.5836 - val_acc: 0.2048
Epoch 34/50
 - 28s - loss: 0.0701 - acc: 0.9789 - val_loss: 8.6826 - val_acc: 0.1995
Epoch 35/50
 - 28s - loss: 0.0840 - acc: 0.9753 - val_loss: 8.6804 - val_acc: 0.2057
Epoch 36/50
 - 28s - loss: 0.0822 - acc: 0.9756 - val_loss: 8.6232 - val_acc: 0.2013
Epoch 37/50
 - 28s - loss: 0.0666 - acc: 0.9806 - val_loss: 8.7460 - val_acc: 0.2033
Epoch 38/50
 - 28s - loss: 0.0485 - acc: 0.9859 - val_loss: 8.8656 - val_acc: 0.1982
Epoch 39/50
 - 28s - loss: 0.0561 - acc: 0.9831 - val_loss: 8.9194 - val_acc: 0.1992
Epoch 40/50
 - 28s - loss: 0.0507 - acc: 0.9855 - val_loss: 8.9340 - val_acc: 0.2015
Epoch 41/50
 - 28s - loss: 0.0504 - acc: 0.9855 - val_loss: 9.0486 - val_acc: 0.2033
Epoch 42/50
 - 28s - loss: 0.0364 - acc: 0.9899 - val_loss: 9.1336 - val_acc: 0.1938
Epoch 43/50
 - 28s - loss: 0.0361 - acc: 0.9894 - val_loss: 9.0456 - val_acc: 0.2057
Epoch 44/50
 - 28s - loss: 0.0368 - acc: 0.9894 - val_loss: 9.2659 - val_acc: 0.1983
Epoch 45/50
 - 28s - loss: 0.0438 - acc: 0.9872 - val_loss: 9.1973 - val_acc: 0.2001
Epoch 46/50
 - 28s - loss: 0.0586 - acc: 0.9830 - val_loss: 9.0898 - val_acc: 0.1940
Epoch 47/50
 - 28s - loss: 0.0629 - acc: 0.9812 - val_loss: 9.2112 - val_acc: 0.2033
Epoch 48/50
 - 28s - loss: 0.0509 - acc: 0.9854 - val_loss: 9.1871 - val_acc: 0.2004
Epoch 49/50
 - 28s - loss: 0.0525 - acc: 0.9850 - val_loss: 9.2159 - val_acc: 0.1988
Epoch 50/50
 - 28s - loss: 0.0443 - acc: 0.9870 - val_loss: 9.1991 - val_acc: 0.2026
training history:
{'val_loss': [4.744512059020996, 4.366284394836426, 4.101281027221679, 3.9208871780395507, 3.7372025230407715, 3.675063899612427, 3.665787449645996, 3.59606805229187, 3.6840078773498535, 3.749597180175781, 4.058543269348145, 4.123277687835693, 4.457951836395264, 4.970293829345703, 5.362325148010254, 6.078645344543457, 6.380514030456543, 6.832811268615723, 6.923523110961914, 7.441500174713135, 7.35780528717041, 7.647357696533203, 7.767116825866699, 7.722565086364746, 8.014113862609863, 8.123602310180663, 8.077738555908203, 8.131033084106445, 8.37943011932373, 8.613415451049805, 8.607617578125, 8.744299807739258, 8.583643855285645, 8.68255048828125, 8.680376512145996, 8.623238189697265, 8.745962841796874, 8.865556973266601, 8.91938370513916, 8.933999908447266, 9.048575491333008, 9.133588676452638, 9.04561512145996, 9.265931393432616, 9.197316223144531, 9.089798043823242, 9.211226239013671, 9.187096968078613, 9.215915466308594, 9.199074603271484], 'val_acc': [0.06, 0.1005, 0.1373, 0.1583, 0.1948, 0.2025, 0.2123, 0.2231, 0.2234, 0.2285, 0.2167, 0.2173, 0.2095, 0.1988, 0.195, 0.1973, 0.1893, 0.189, 0.1898, 0.1957, 0.1964, 0.1969, 0.1916, 0.2011, 0.1993, 0.1974, 0.2013, 0.2014, 0.2002, 0.1864, 0.1958, 0.1881, 0.2048, 0.1995, 0.2057, 0.2013, 0.2033, 0.1982, 0.1992, 0.2015, 0.2033, 0.1938, 0.2057, 0.1983, 0.2001, 0.194, 0.2033, 0.2004, 0.1988, 0.2026], 'loss': [5.037515916748047, 4.547173990325928, 4.187711880493164, 3.900944528198242, 3.6547029760742187, 3.4309528160858154, 3.222095923156738, 3.0001922528076173, 2.7651305168914795, 2.5143760638427732, 2.225281175613403, 1.9183702350997924, 1.5972990550231934, 1.2690927722167968, 1.002654889163971, 0.7547417050075531, 0.5984822756719589, 0.4827901478242874, 0.3880220895123482, 0.323896995947361, 0.2700253613269329, 0.2292863008105755, 0.19864588379859924, 0.18033231269598007, 0.1606552486485243, 0.14006008093476296, 0.14041352793991566, 0.11097309432953596, 0.09602227563589812, 0.1038483928193152, 0.10018195187181235, 0.07904954352468252, 0.07059532063931227, 0.07004781095966697, 0.08394060084789991, 0.08214738701812922, 0.06656932157836855, 0.04852402371816337, 0.05608077518314123, 0.05066793513972312, 0.05037057007472962, 0.03643149012111127, 0.03612272993735969, 0.03676208400070667, 0.04384620490314439, 0.058641455213464797, 0.06288462796159089, 0.050947769200243055, 0.05248545719366521, 0.04424261240744963], 'acc': [0.02871, 0.08232, 0.12779, 0.16939, 0.20506, 0.24112, 0.27481, 0.31108, 0.35233, 0.39796, 0.45291, 0.51421, 0.58223, 0.65674, 0.71868, 0.78336, 0.82486, 0.85508, 0.88365, 0.90269, 0.91996, 0.93143, 0.94009, 0.94545, 0.95092, 0.95768, 0.95807, 0.96662, 0.97133, 0.9688, 0.96999, 0.97654, 0.97885, 0.97896, 0.9753, 0.97561, 0.98056, 0.98586, 0.98311, 0.98554, 0.98554, 0.98992, 0.98944, 0.98939, 0.98717, 0.98304, 0.98118, 0.98537, 0.98505, 0.98705]}
Training time: 
1388.561014175415
Evaluation results:  [9.199074600219726, 0.2026]
