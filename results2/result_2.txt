trainingset size: 100000
validationset size: 10000
input_dims:  (64, 64, 3)
nb_labels:  200
nb_labels: 200
x_train.shape  (100000, 64, 64, 3)
y_train.shape  (100000, 200)
x_test.shape  (10000, 64, 64, 3)
y_test.shape  (10000, 200)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 58, 62, 48)        3072      
_________________________________________________________________
activation_1 (Activation)    (None, 58, 62, 48)        0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 28, 30, 48)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 24, 30, 128)       30848     
_________________________________________________________________
activation_2 (Activation)    (None, 24, 30, 128)       0         
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 11, 14, 128)       0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 9, 14, 192)        73920     
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 7, 14, 192)        110784    
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 5, 14, 128)        73856     
_________________________________________________________________
activation_3 (Activation)    (None, 5, 14, 128)        0         
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 2, 6, 128)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 1536)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 2048)              3147776   
_________________________________________________________________
dense_2 (Dense)              (None, 2048)              4196352   
_________________________________________________________________
dense_3 (Dense)              (None, 200)               409800    
=================================================================
Total params: 8,046,408
Trainable params: 8,046,408
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
 - 39s - loss: 4.9537 - acc: 0.0273 - val_loss: 4.7141 - val_acc: 0.0487
Epoch 2/100
 - 33s - loss: 4.5653 - acc: 0.0656 - val_loss: 4.4606 - val_acc: 0.0762
Epoch 3/100
 - 33s - loss: 4.3263 - acc: 0.0923 - val_loss: 4.2628 - val_acc: 0.0985
Epoch 4/100
 - 33s - loss: 4.1411 - acc: 0.1154 - val_loss: 4.1002 - val_acc: 0.1261
Epoch 5/100
 - 33s - loss: 3.9989 - acc: 0.1345 - val_loss: 4.0748 - val_acc: 0.1243
Epoch 6/100
 - 33s - loss: 3.8927 - acc: 0.1493 - val_loss: 4.0200 - val_acc: 0.1381
Epoch 7/100
 - 33s - loss: 3.8041 - acc: 0.1637 - val_loss: 4.0363 - val_acc: 0.1368
Epoch 8/100
 - 33s - loss: 3.7213 - acc: 0.1751 - val_loss: 3.9733 - val_acc: 0.1538
Epoch 9/100
 - 33s - loss: 3.6389 - acc: 0.1858 - val_loss: 3.9961 - val_acc: 0.1447
Epoch 10/100
 - 33s - loss: 3.5535 - acc: 0.1983 - val_loss: 3.9851 - val_acc: 0.1532
Epoch 11/100
 - 33s - loss: 3.4834 - acc: 0.2099 - val_loss: 4.0538 - val_acc: 0.1437
Epoch 12/100
 - 33s - loss: 3.3998 - acc: 0.2223 - val_loss: 4.0808 - val_acc: 0.1504
Epoch 13/100
 - 33s - loss: 3.3134 - acc: 0.2376 - val_loss: 4.1191 - val_acc: 0.1462
Epoch 14/100
 - 33s - loss: 3.2251 - acc: 0.2508 - val_loss: 4.1376 - val_acc: 0.1518
Epoch 15/100
 - 33s - loss: 3.1352 - acc: 0.2641 - val_loss: 4.1833 - val_acc: 0.1476
Epoch 16/100
 - 33s - loss: 3.0447 - acc: 0.2810 - val_loss: 4.2791 - val_acc: 0.1458
Epoch 17/100
 - 33s - loss: 2.9479 - acc: 0.2995 - val_loss: 4.4621 - val_acc: 0.1478
Epoch 18/100
 - 33s - loss: 2.8586 - acc: 0.3153 - val_loss: 4.5208 - val_acc: 0.1436
Epoch 19/100
 - 33s - loss: 2.7584 - acc: 0.3321 - val_loss: 4.6767 - val_acc: 0.1400
Epoch 20/100
 - 33s - loss: 2.6657 - acc: 0.3519 - val_loss: 4.6919 - val_acc: 0.1423
Epoch 21/100
 - 33s - loss: 2.5645 - acc: 0.3714 - val_loss: 4.8732 - val_acc: 0.1362
Epoch 22/100
 - 33s - loss: 2.4658 - acc: 0.3897 - val_loss: 4.9925 - val_acc: 0.1301
Epoch 23/100
 - 33s - loss: 2.3597 - acc: 0.4127 - val_loss: 5.1869 - val_acc: 0.1351
Epoch 24/100
 - 33s - loss: 2.2665 - acc: 0.4305 - val_loss: 5.3413 - val_acc: 0.1308
Epoch 25/100
 - 33s - loss: 2.1745 - acc: 0.4512 - val_loss: 5.4731 - val_acc: 0.1271
Epoch 26/100
 - 33s - loss: 2.0819 - acc: 0.4696 - val_loss: 5.6071 - val_acc: 0.1267
Epoch 27/100
 - 33s - loss: 1.9976 - acc: 0.4884 - val_loss: 5.7889 - val_acc: 0.1273
Epoch 28/100
 - 33s - loss: 1.8892 - acc: 0.5104 - val_loss: 5.9541 - val_acc: 0.1200
Epoch 29/100
 - 33s - loss: 1.8116 - acc: 0.5296 - val_loss: 6.0826 - val_acc: 0.1225
Epoch 30/100
 - 33s - loss: 1.7381 - acc: 0.5450 - val_loss: 6.3874 - val_acc: 0.1193
Epoch 31/100
 - 33s - loss: 1.6496 - acc: 0.5650 - val_loss: 6.5603 - val_acc: 0.1173
Epoch 32/100
 - 33s - loss: 1.5885 - acc: 0.5768 - val_loss: 6.8130 - val_acc: 0.1155
Epoch 33/100
 - 33s - loss: 1.5070 - acc: 0.5964 - val_loss: 6.7584 - val_acc: 0.1156
Epoch 34/100
 - 33s - loss: 1.4514 - acc: 0.6089 - val_loss: 7.0131 - val_acc: 0.1126
Epoch 35/100
 - 33s - loss: 1.3730 - acc: 0.6300 - val_loss: 7.0857 - val_acc: 0.1061
Epoch 36/100
 - 33s - loss: 1.3097 - acc: 0.6433 - val_loss: 7.3975 - val_acc: 0.1077
Epoch 37/100
 - 33s - loss: 1.2706 - acc: 0.6523 - val_loss: 7.5445 - val_acc: 0.1100
Epoch 38/100
 - 33s - loss: 1.2143 - acc: 0.6655 - val_loss: 7.6963 - val_acc: 0.1129
Epoch 39/100
 - 33s - loss: 1.1628 - acc: 0.6768 - val_loss: 7.9151 - val_acc: 0.1108
Epoch 40/100
 - 33s - loss: 1.1141 - acc: 0.6911 - val_loss: 7.9036 - val_acc: 0.1070
Epoch 41/100
 - 33s - loss: 1.0830 - acc: 0.6979 - val_loss: 8.1387 - val_acc: 0.1078
Epoch 42/100
 - 33s - loss: 1.0307 - acc: 0.7111 - val_loss: 8.3491 - val_acc: 0.1088
Epoch 43/100
 - 33s - loss: 0.9967 - acc: 0.7206 - val_loss: 8.3196 - val_acc: 0.1079
Epoch 44/100
 - 33s - loss: 0.9748 - acc: 0.7257 - val_loss: 8.5010 - val_acc: 0.1066
Epoch 45/100
 - 33s - loss: 0.9257 - acc: 0.7377 - val_loss: 8.8299 - val_acc: 0.1046
Epoch 46/100
 - 33s - loss: 0.9166 - acc: 0.7395 - val_loss: 8.6842 - val_acc: 0.1037
Epoch 47/100
 - 33s - loss: 0.8723 - acc: 0.7529 - val_loss: 8.6617 - val_acc: 0.1071
Epoch 48/100
 - 33s - loss: 0.8418 - acc: 0.7617 - val_loss: 8.8929 - val_acc: 0.1043
Epoch 49/100
 - 33s - loss: 0.8189 - acc: 0.7682 - val_loss: 9.0702 - val_acc: 0.1053
Epoch 50/100
 - 33s - loss: 0.7977 - acc: 0.7750 - val_loss: 9.0314 - val_acc: 0.1042
Epoch 51/100
 - 33s - loss: 0.7807 - acc: 0.7762 - val_loss: 9.2119 - val_acc: 0.1015
Epoch 52/100
 - 33s - loss: 0.7555 - acc: 0.7851 - val_loss: 9.2405 - val_acc: 0.1020
Epoch 53/100
 - 33s - loss: 0.7673 - acc: 0.7812 - val_loss: 9.4005 - val_acc: 0.1052
Epoch 54/100
 - 33s - loss: 0.6997 - acc: 0.7990 - val_loss: 9.4418 - val_acc: 0.1070
Epoch 55/100
 - 33s - loss: 0.7348 - acc: 0.7909 - val_loss: 9.6190 - val_acc: 0.1030
Epoch 56/100
 - 33s - loss: 0.6914 - acc: 0.8027 - val_loss: 9.5503 - val_acc: 0.1074
Epoch 57/100
 - 33s - loss: 0.6663 - acc: 0.8098 - val_loss: 9.6901 - val_acc: 0.1043
Epoch 58/100
 - 33s - loss: 0.6586 - acc: 0.8112 - val_loss: 9.7532 - val_acc: 0.1002
Epoch 59/100
 - 33s - loss: 0.6608 - acc: 0.8118 - val_loss: 9.7631 - val_acc: 0.1070
Epoch 60/100
 - 33s - loss: 0.6277 - acc: 0.8204 - val_loss: 9.9081 - val_acc: 0.1037
Epoch 61/100
 - 33s - loss: 0.6537 - acc: 0.8143 - val_loss: 9.9036 - val_acc: 0.1011
Epoch 62/100
 - 33s - loss: 0.5959 - acc: 0.8296 - val_loss: 10.1047 - val_acc: 0.1028
Epoch 63/100
 - 33s - loss: 0.6293 - acc: 0.8220 - val_loss: 10.1705 - val_acc: 0.1026
Epoch 64/100
 - 33s - loss: 0.5864 - acc: 0.8340 - val_loss: 10.1321 - val_acc: 0.0991
Epoch 65/100
 - 33s - loss: 0.5740 - acc: 0.8360 - val_loss: 10.2876 - val_acc: 0.1009
Epoch 66/100
 - 33s - loss: 0.6038 - acc: 0.8301 - val_loss: 10.3719 - val_acc: 0.0994
Epoch 67/100
 - 33s - loss: 0.5630 - acc: 0.8402 - val_loss: 10.2618 - val_acc: 0.1024
Epoch 68/100
 - 33s - loss: 0.5644 - acc: 0.8398 - val_loss: 10.2301 - val_acc: 0.1000
Epoch 69/100
 - 33s - loss: 0.5693 - acc: 0.8397 - val_loss: 10.3824 - val_acc: 0.1020
Epoch 70/100
 - 33s - loss: 0.5294 - acc: 0.8509 - val_loss: 10.3991 - val_acc: 0.1021
Epoch 71/100
 - 33s - loss: 0.5630 - acc: 0.8429 - val_loss: 10.6077 - val_acc: 0.0982
Epoch 72/100
 - 33s - loss: 0.5245 - acc: 0.8517 - val_loss: 10.5418 - val_acc: 0.1012
Epoch 73/100
 - 33s - loss: 0.5461 - acc: 0.8460 - val_loss: 10.6842 - val_acc: 0.1003
Epoch 74/100
 - 33s - loss: 0.5371 - acc: 0.8502 - val_loss: 10.5835 - val_acc: 0.1075
Epoch 75/100
 - 33s - loss: 0.5214 - acc: 0.8533 - val_loss: 10.6454 - val_acc: 0.1027
Epoch 76/100
 - 33s - loss: 0.5279 - acc: 0.8521 - val_loss: 10.6566 - val_acc: 0.1019
Epoch 77/100
 - 33s - loss: 0.4992 - acc: 0.8607 - val_loss: 10.7037 - val_acc: 0.0977
Epoch 78/100
 - 33s - loss: 0.5101 - acc: 0.8593 - val_loss: 10.8167 - val_acc: 0.0999
Epoch 79/100
 - 33s - loss: 0.5256 - acc: 0.8551 - val_loss: 10.8728 - val_acc: 0.1043
Epoch 80/100
 - 33s - loss: 0.4937 - acc: 0.8624 - val_loss: 10.8481 - val_acc: 0.0997
Epoch 81/100
 - 33s - loss: 0.4838 - acc: 0.8662 - val_loss: 10.9431 - val_acc: 0.0988
Epoch 82/100
 - 33s - loss: 0.5168 - acc: 0.8584 - val_loss: 10.9774 - val_acc: 0.1015
Epoch 83/100
 - 33s - loss: 0.4530 - acc: 0.8732 - val_loss: 10.8869 - val_acc: 0.1010
Epoch 84/100
 - 33s - loss: 0.4952 - acc: 0.8637 - val_loss: 11.1418 - val_acc: 0.0968
Epoch 85/100
 - 33s - loss: 0.5090 - acc: 0.8630 - val_loss: 11.0748 - val_acc: 0.1008
Epoch 86/100
 - 33s - loss: 0.4942 - acc: 0.8665 - val_loss: 10.9034 - val_acc: 0.1035
Epoch 87/100
 - 33s - loss: 0.4698 - acc: 0.8726 - val_loss: 11.0162 - val_acc: 0.1033
Epoch 88/100
 - 33s - loss: 0.4835 - acc: 0.8689 - val_loss: 11.1591 - val_acc: 0.1012
Epoch 89/100
 - 33s - loss: 0.4915 - acc: 0.8679 - val_loss: 11.1120 - val_acc: 0.1001
Epoch 90/100
 - 33s - loss: 0.4409 - acc: 0.8795 - val_loss: 11.0098 - val_acc: 0.1008
Epoch 91/100
 - 33s - loss: 0.5011 - acc: 0.8673 - val_loss: 11.1989 - val_acc: 0.1009
Epoch 92/100
 - 33s - loss: 0.4787 - acc: 0.8725 - val_loss: 11.1565 - val_acc: 0.1003
Epoch 93/100
 - 33s - loss: 0.4475 - acc: 0.8776 - val_loss: 11.3889 - val_acc: 0.0968
Epoch 94/100
 - 33s - loss: 0.4807 - acc: 0.8711 - val_loss: 11.2940 - val_acc: 0.0964
Epoch 95/100
 - 33s - loss: 0.4539 - acc: 0.8785 - val_loss: 11.2453 - val_acc: 0.1001
Epoch 96/100
 - 33s - loss: 0.4521 - acc: 0.8789 - val_loss: 11.1351 - val_acc: 0.1018
Epoch 97/100
 - 33s - loss: 0.4615 - acc: 0.8775 - val_loss: 11.3165 - val_acc: 0.1017
Epoch 98/100
 - 33s - loss: 0.4473 - acc: 0.8798 - val_loss: 11.3443 - val_acc: 0.0964
Epoch 99/100
 - 33s - loss: 0.4666 - acc: 0.8773 - val_loss: 11.3323 - val_acc: 0.0950
Epoch 100/100
 - 33s - loss: 0.4438 - acc: 0.8825 - val_loss: 11.3587 - val_acc: 0.0981
training history:
{'val_loss': [4.71406626663208, 4.4606253555297855, 4.262756594085693, 4.100160943603516, 4.074800085449219, 4.020009725189209, 4.036348788452148, 3.9732981567382812, 3.9960674507141114, 3.985058595275879, 4.053755416107178, 4.08083703994751, 4.119067403030395, 4.137610508728027, 4.183250775909424, 4.279110499572754, 4.4620599349975585, 4.520832862854004, 4.676738794708252, 4.691859773254395, 4.873152173614502, 4.9924686317443845, 5.1868743881225585, 5.34130998840332, 5.473137678527832, 5.607144546508789, 5.788890696716309, 5.954124938964844, 6.082580074310303, 6.387374970245362, 6.560264416503906, 6.812971554565429, 6.758376045989991, 7.013090789794922, 7.085682887268066, 7.397513386535644, 7.544502677154541, 7.696272788238526, 7.915105673217774, 7.903614932250977, 8.138659292602538, 8.3491201171875, 8.319615747070312, 8.501000350952149, 8.829907528686524, 8.684207542419433, 8.661682682800294, 8.892886697387695, 9.070156280517578, 9.031413316345215, 9.211880897521972, 9.240526036071778, 9.400497776794433, 9.441840005493164, 9.619009588623047, 9.550301890563965, 9.690084713745117, 9.753185458374023, 9.763089196777344, 9.90809553527832, 9.903586619567871, 10.104701736450195, 10.170529635620117, 10.132099493408203, 10.287631991577149, 10.371899937438965, 10.261800396728516, 10.230061599731446, 10.382425183105468, 10.399063252258301, 10.607653567504883, 10.541811853027344, 10.684176654052735, 10.583469102478027, 10.645403411865235, 10.656616567993163, 10.703699896240234, 10.816748776245117, 10.872773040771484, 10.848146340942384, 10.943095478820801, 10.977434823608398, 10.886870069885253, 11.14179561767578, 11.074770700073243, 10.903351062011719, 11.016198287963867, 11.159105545043944, 11.112005191040039, 11.009836418151856, 11.198931713867188, 11.156481509399415, 11.388869416809081, 11.294047427368165, 11.245295719909668, 11.135146943664552, 11.316517755126952, 11.344348748779296, 11.332342736816406, 11.35867991027832], 'val_acc': [0.0487, 0.0762, 0.0985, 0.1261, 0.1243, 0.1381, 0.1368, 0.1538, 0.1447, 0.1532, 0.1437, 0.1504, 0.1462, 0.1518, 0.1476, 0.1458, 0.1478, 0.1436, 0.14, 0.1423, 0.1362, 0.1301, 0.1351, 0.1308, 0.1271, 0.1267, 0.1273, 0.12, 0.1225, 0.1193, 0.1173, 0.1155, 0.1156, 0.1126, 0.1061, 0.1077, 0.11, 0.1129, 0.1108, 0.107, 0.1078, 0.1088, 0.1079, 0.1066, 0.1046, 0.1037, 0.1071, 0.1043, 0.1053, 0.1042, 0.1015, 0.102, 0.1052, 0.107, 0.103, 0.1074, 0.1043, 0.1002, 0.107, 0.1037, 0.1011, 0.1028, 0.1026, 0.0991, 0.1009, 0.0994, 0.1024, 0.1, 0.102, 0.1021, 0.0982, 0.1012, 0.1003, 0.1075, 0.1027, 0.1019, 0.0977, 0.0999, 0.1043, 0.0997, 0.0988, 0.1015, 0.101, 0.0968, 0.1008, 0.1035, 0.1033, 0.1012, 0.1001, 0.1008, 0.1009, 0.1003, 0.0968, 0.0964, 0.1001, 0.1018, 0.1017, 0.0964, 0.095, 0.0981], 'loss': [4.953751741638183, 4.5652532781982424, 4.326369976119995, 4.141212272796631, 3.9987594229125976, 3.892567103424072, 3.804154076080322, 3.721206193008423, 3.6389266806030274, 3.5535235835266112, 3.4833137331390382, 3.3999135508728027, 3.31334851852417, 3.2250219501495363, 3.1352782916259767, 3.0446824438476563, 2.9477390892791746, 2.8584661030578613, 2.7584015824890136, 2.665480795440674, 2.564385126953125, 2.4658240938568117, 2.359590752334595, 2.2665564096832274, 2.174421838760376, 2.0818243230438234, 1.997640256729126, 1.8889051544189452, 1.811599768486023, 1.738217043685913, 1.6497731444168091, 1.5884858011245728, 1.5071666179656982, 1.4514515115356446, 1.3730840417861938, 1.3098043482208253, 1.2705435877609252, 1.2143911974143982, 1.1627849448776244, 1.1142353240394591, 1.0828440383720397, 1.0307216160583497, 0.996524620513916, 0.9745578343009949, 0.92569015209198, 0.9165219081687928, 0.8722563677597046, 0.8418762710762024, 0.8189359361076355, 0.7978159942436218, 0.7806959565353394, 0.7554330706977844, 0.7674152866458893, 0.6997754686355591, 0.7349020399284363, 0.6913955174350739, 0.6664132392787934, 0.6586486051654815, 0.6608046800899505, 0.6277388145828247, 0.6537633453464508, 0.5957947146129609, 0.629350135307312, 0.5863425610685349, 0.5740224733591079, 0.6038716197395325, 0.5629840516138077, 0.5645288882637024, 0.5691260113430023, 0.5294847241258621, 0.5630333384799957, 0.5244966210651397, 0.546012079539299, 0.5372437546348572, 0.5214918535518647, 0.5278825320243835, 0.4990966152906418, 0.5101067020606994, 0.5255203504514694, 0.4937314597320557, 0.4837933097600937, 0.5168140562677384, 0.4530625599050522, 0.4953000430250168, 0.5087705716323853, 0.4941815216255188, 0.4698252216720581, 0.48345751321315766, 0.49158241523981094, 0.4410493162560463, 0.5011496115636825, 0.4788353777194023, 0.4475355220293999, 0.48074216295719147, 0.45404249870061875, 0.45221282752275466, 0.4614778459572792, 0.44739668806791305, 0.4666361215734482, 0.4436939376115799], 'acc': [0.02728, 0.06566, 0.09227, 0.11536, 0.1345, 0.1493, 0.16368, 0.1751, 0.18582, 0.19824, 0.20984, 0.22224, 0.2376, 0.25083, 0.26404, 0.28104, 0.29953, 0.31532, 0.33206, 0.35194, 0.37148, 0.38973, 0.41269, 0.43051, 0.45117, 0.46961, 0.48844, 0.51041, 0.52954, 0.54499, 0.56493, 0.5768, 0.59642, 0.6089, 0.63003, 0.64325, 0.65226, 0.6655, 0.67681, 0.69111, 0.69785, 0.7111, 0.72059, 0.72574, 0.7377, 0.73955, 0.75289, 0.76168, 0.76823, 0.77493, 0.77614, 0.78514, 0.7812, 0.79899, 0.79089, 0.80268, 0.80983, 0.81121, 0.81187, 0.82036, 0.81433, 0.82959, 0.82205, 0.83406, 0.83598, 0.83005, 0.84018, 0.83981, 0.83979, 0.85089, 0.84291, 0.85166, 0.84604, 0.85014, 0.85329, 0.85214, 0.86069, 0.85929, 0.85505, 0.86237, 0.8662, 0.85844, 0.87325, 0.86369, 0.86295, 0.86647, 0.87262, 0.86889, 0.86791, 0.87945, 0.86733, 0.87247, 0.87759, 0.87104, 0.87846, 0.8789, 0.87753, 0.87979, 0.87732, 0.88253]}
Training time: 
3330.2603273391724
Evaluation results:  [11.358679975891114, 0.0981]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_6 (Conv2D)            (None, 58, 62, 48)        3072      
_________________________________________________________________
batch_normalization_1 (Batch (None, 58, 62, 48)        192       
_________________________________________________________________
activation_4 (Activation)    (None, 58, 62, 48)        0         
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 28, 30, 48)        0         
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 24, 30, 128)       30848     
_________________________________________________________________
batch_normalization_2 (Batch (None, 24, 30, 128)       512       
_________________________________________________________________
activation_5 (Activation)    (None, 24, 30, 128)       0         
_________________________________________________________________
max_pooling2d_5 (MaxPooling2 (None, 11, 14, 128)       0         
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 9, 14, 192)        73920     
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 7, 14, 192)        110784    
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 5, 14, 128)        73856     
_________________________________________________________________
activation_6 (Activation)    (None, 5, 14, 128)        0         
_________________________________________________________________
max_pooling2d_6 (MaxPooling2 (None, 2, 6, 128)         0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 1536)              0         
_________________________________________________________________
dense_4 (Dense)              (None, 2048)              3147776   
_________________________________________________________________
dense_5 (Dense)              (None, 2048)              4196352   
_________________________________________________________________
dense_6 (Dense)              (None, 200)               409800    
=================================================================
Total params: 8,047,112
Trainable params: 8,046,760
Non-trainable params: 352
_________________________________________________________________
Epoch 1/100
 - 43s - loss: 5.3012 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 2/100
 - 43s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 3/100
 - 43s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 4/100
 - 43s - loss: 5.2991 - acc: 0.0041 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 5/100
 - 43s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 6/100
 - 43s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 7/100
 - 43s - loss: 5.2991 - acc: 0.0041 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 8/100
 - 43s - loss: 5.2991 - acc: 0.0041 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 9/100
 - 43s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 10/100
 - 43s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 11/100
 - 43s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 12/100
 - 43s - loss: 5.2991 - acc: 0.0047 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 13/100
 - 43s - loss: 5.2991 - acc: 0.0046 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 14/100
 - 43s - loss: 5.2991 - acc: 0.0041 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 15/100
 - 43s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 16/100
 - 43s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 17/100
 - 43s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 18/100
 - 43s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 19/100
 - 43s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 20/100
 - 43s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 21/100
 - 43s - loss: 5.2991 - acc: 0.0041 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 22/100
 - 43s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 23/100
 - 43s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 24/100
 - 43s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 25/100
 - 43s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 26/100
 - 43s - loss: 5.2991 - acc: 0.0046 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 27/100
 - 43s - loss: 5.2991 - acc: 0.0046 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 28/100
 - 43s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 29/100
 - 43s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 30/100
 - 43s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 31/100
 - 43s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 32/100
 - 43s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 33/100
 - 43s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 34/100
 - 43s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 35/100
 - 43s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2984 - val_acc: 0.0050
Epoch 36/100
 - 43s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 37/100
 - 43s - loss: 5.2991 - acc: 0.0041 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 38/100
 - 43s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 39/100
 - 43s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 40/100
 - 43s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 41/100
 - 43s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 42/100
 - 43s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 43/100
 - 43s - loss: 5.2991 - acc: 0.0041 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 44/100
 - 43s - loss: 5.2991 - acc: 0.0047 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 45/100
 - 43s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 46/100
 - 43s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 47/100
 - 43s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 48/100
 - 43s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2984 - val_acc: 0.0050
Epoch 49/100
 - 43s - loss: 5.2991 - acc: 0.0046 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 50/100
 - 43s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 51/100
 - 43s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 52/100
 - 43s - loss: 5.2991 - acc: 0.0040 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 53/100
 - 43s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 54/100
 - 43s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 55/100
 - 43s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 56/100
 - 43s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 57/100
 - 43s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 58/100
 - 43s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 59/100
 - 43s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 60/100
 - 43s - loss: 5.2991 - acc: 0.0047 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 61/100
 - 43s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 62/100
 - 43s - loss: 5.2991 - acc: 0.0041 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 63/100
 - 43s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 64/100
 - 43s - loss: 5.2991 - acc: 0.0046 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 65/100
 - 43s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2984 - val_acc: 0.0050
Epoch 66/100
 - 43s - loss: 5.2991 - acc: 0.0041 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 67/100
 - 43s - loss: 5.2991 - acc: 0.0041 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 68/100
 - 43s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 69/100
 - 43s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 70/100
 - 43s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 71/100
 - 43s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 72/100
 - 43s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 73/100
 - 43s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 74/100
 - 43s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 75/100
 - 43s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 76/100
 - 43s - loss: 5.2991 - acc: 0.0040 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 77/100
 - 43s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 78/100
 - 43s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 79/100
 - 43s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 80/100
 - 43s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 81/100
 - 43s - loss: 5.2991 - acc: 0.0040 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 82/100
 - 43s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 83/100
 - 43s - loss: 5.2991 - acc: 0.0048 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 84/100
 - 43s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 85/100
 - 43s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 86/100
 - 43s - loss: 5.2991 - acc: 0.0046 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 87/100
 - 43s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 88/100
 - 43s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 89/100
 - 43s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 90/100
 - 43s - loss: 5.2991 - acc: 0.0039 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 91/100
 - 43s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 92/100
 - 43s - loss: 5.2991 - acc: 0.0046 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 93/100
 - 43s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 94/100
 - 43s - loss: 5.2991 - acc: 0.0042 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 95/100
 - 43s - loss: 5.2991 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 96/100
 - 43s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 97/100
 - 43s - loss: 5.2991 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 98/100
 - 43s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 99/100
 - 43s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
Epoch 100/100
 - 43s - loss: 5.2991 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0050
training history:
{'val_loss': [5.298347415161133, 5.298346147155762, 5.298342143249512, 5.298343705749511, 5.2983411697387695, 5.2983426712036135, 5.2983421875, 5.298338907623291, 5.298343240356445, 5.298340455627441, 5.298345599365234, 5.298341076660156, 5.298345883178711, 5.298341929626464, 5.29834411315918, 5.298344788360596, 5.298345069122314, 5.298343578338623, 5.29834183807373, 5.298343601989746, 5.298344205474853, 5.29834513168335, 5.29834702911377, 5.298347955322265, 5.298339304351806, 5.298345646667481, 5.298346546936036, 5.298339434814453, 5.298344003295899, 5.29834503326416, 5.298343692779541, 5.298340237426758, 5.2983432998657225, 5.298344361114502, 5.298350930786133, 5.298345114135742, 5.298343092346191, 5.2983425491333005, 5.29834151763916, 5.298338227844238, 5.298342488861084, 5.298346614074707, 5.29834704284668, 5.2983499572753905, 5.2983432998657225, 5.298346036529541, 5.298340467834473, 5.298350122070312, 5.298342861175537, 5.298346746826172, 5.298345544433594, 5.298340776062012, 5.2983456771850586, 5.298344934082031, 5.2983430557250975, 5.298342144775391, 5.2983419082641605, 5.298344718933105, 5.298340824127197, 5.298345975494385, 5.298346745300293, 5.298344199371338, 5.298341400146485, 5.298348509979248, 5.298351002502441, 5.2983434188842775, 5.298346417236328, 5.298344496154785, 5.2983422981262205, 5.298346199035644, 5.298345045471192, 5.298348477172851, 5.29834503326416, 5.298346270751953, 5.298345135498047, 5.2983429122924806, 5.298344219970703, 5.298344403076172, 5.298345516204834, 5.298347468566894, 5.298342753601074, 5.298346479034424, 5.298346333312988, 5.298343161773682, 5.298347894287109, 5.298346949768066, 5.298346556091309, 5.2983435363769535, 5.298344284820557, 5.2983451789855955, 5.298342081451416, 5.2983453025817875, 5.29834747390747, 5.298348249816894, 5.298344421386719, 5.298343020629883, 5.298345901489258, 5.2983421676635745, 5.2983425430297855, 5.298344375610352], 'val_acc': [0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005], 'loss': [5.301168388366699, 5.2990931086730955, 5.2990966888427735, 5.299086484985351, 5.299089477539063, 5.299088887939453, 5.2990910623168945, 5.299100601196289, 5.299082693786621, 5.2990943231201175, 5.299086555328369, 5.299098931884766, 5.299080009155273, 5.299095566864014, 5.299089956054687, 5.299079454345703, 5.299085863037109, 5.2990891302490235, 5.299088588867187, 5.299090030517578, 5.299085308685303, 5.299083990478516, 5.299082668457031, 5.299087329101562, 5.299083495941162, 5.299081758270264, 5.2990752197265625, 5.299108866271973, 5.299093582763672, 5.299093321533203, 5.299065835876465, 5.299081902008057, 5.299083030090332, 5.299075412902832, 5.2990803726196285, 5.299084702148438, 5.299105379943848, 5.2990929766845705, 5.299099256896973, 5.299104252624511, 5.29908808227539, 5.299091934204101, 5.299074280090332, 5.299069358215332, 5.299093305664062, 5.299090162506103, 5.299078167419434, 5.2990836349487305, 5.299102448120117, 5.299069844970703, 5.299090055847168, 5.299090365905761, 5.299078807373047, 5.2990798429870605, 5.299102660217285, 5.299070977783203, 5.2990848869323735, 5.299085578765869, 5.299089863128662, 5.299078671264648, 5.2990793341064455, 5.299096289978027, 5.2990936215209965, 5.299070118713379, 5.2990775770568845, 5.299087059631348, 5.299076869659424, 5.299100867919922, 5.2990820413208, 5.299073735351563, 5.299089063568116, 5.299079189605713, 5.299093433532715, 5.299100363616943, 5.2990890550231935, 5.299077305603027, 5.299088524169922, 5.29908379486084, 5.2990916694641115, 5.299081950378418, 5.2990960379028325, 5.2990713563537595, 5.299091998596191, 5.299096670837402, 5.299090498352051, 5.299084375610351, 5.299087407226563, 5.299096335601806, 5.299085720977783, 5.299103006744385, 5.299095409393311, 5.299092768859864, 5.299080988159179, 5.299091490936279, 5.29909406616211, 5.299092578430176, 5.299076033935547, 5.2990906385803225, 5.2990775555419924, 5.299098881072998], 'acc': [0.00417, 0.00419, 0.00448, 0.00414, 0.0043, 0.00432, 0.00415, 0.00415, 0.00423, 0.0044, 0.00451, 0.00475, 0.00458, 0.00413, 0.00421, 0.00436, 0.00437, 0.00446, 0.00425, 0.00439, 0.00412, 0.00424, 0.00426, 0.0043, 0.0043, 0.0046, 0.00459, 0.00424, 0.00441, 0.00438, 0.00445, 0.00426, 0.00422, 0.00444, 0.00433, 0.00443, 0.00411, 0.00422, 0.00425, 0.00426, 0.00424, 0.00443, 0.00407, 0.00466, 0.00445, 0.00429, 0.0044, 0.00446, 0.00461, 0.00438, 0.00419, 0.004, 0.00421, 0.00439, 0.00447, 0.00455, 0.0043, 0.00432, 0.00453, 0.0047, 0.00445, 0.00411, 0.00424, 0.00456, 0.00424, 0.00406, 0.00412, 0.00435, 0.00441, 0.00438, 0.00437, 0.00418, 0.00452, 0.00436, 0.00433, 0.00404, 0.00423, 0.00433, 0.00439, 0.00437, 0.00404, 0.00448, 0.00481, 0.00434, 0.00417, 0.0046, 0.00416, 0.0044, 0.00447, 0.00394, 0.00427, 0.00464, 0.00444, 0.00425, 0.00449, 0.00444, 0.00442, 0.00434, 0.0043, 0.00431]}
Training time: 
4277.009421825409
Evaluation results:  [5.298344376373291, 0.005]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_11 (Conv2D)           (None, 58, 62, 48)        3072      
_________________________________________________________________
activation_7 (Activation)    (None, 58, 62, 48)        0         
_________________________________________________________________
max_pooling2d_7 (MaxPooling2 (None, 28, 30, 48)        0         
_________________________________________________________________
conv2d_12 (Conv2D)           (None, 24, 30, 128)       30848     
_________________________________________________________________
activation_8 (Activation)    (None, 24, 30, 128)       0         
_________________________________________________________________
max_pooling2d_8 (MaxPooling2 (None, 11, 14, 128)       0         
_________________________________________________________________
conv2d_13 (Conv2D)           (None, 9, 14, 192)        73920     
_________________________________________________________________
conv2d_14 (Conv2D)           (None, 7, 14, 192)        110784    
_________________________________________________________________
conv2d_15 (Conv2D)           (None, 5, 14, 128)        73856     
_________________________________________________________________
activation_9 (Activation)    (None, 5, 14, 128)        0         
_________________________________________________________________
max_pooling2d_9 (MaxPooling2 (None, 2, 6, 128)         0         
_________________________________________________________________
flatten_3 (Flatten)          (None, 1536)              0         
_________________________________________________________________
dense_7 (Dense)              (None, 2048)              3147776   
_________________________________________________________________
dense_8 (Dense)              (None, 2048)              4196352   
_________________________________________________________________
dense_9 (Dense)              (None, 200)               409800    
=================================================================
Total params: 8,046,408
Trainable params: 8,046,408
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
 - 34s - loss: 5.3114 - acc: 0.0047 - val_loss: 5.3013 - val_acc: 0.0050
Epoch 2/100
 - 33s - loss: 5.3022 - acc: 0.0049 - val_loss: 5.3001 - val_acc: 0.0050
Epoch 3/100
 - 33s - loss: 5.3019 - acc: 0.0048 - val_loss: 5.2998 - val_acc: 0.0050
Epoch 4/100
 - 33s - loss: 5.3018 - acc: 0.0046 - val_loss: 5.3000 - val_acc: 0.0050
Epoch 5/100
 - 33s - loss: 5.3017 - acc: 0.0046 - val_loss: 5.3004 - val_acc: 0.0050
Epoch 6/100
 - 33s - loss: 5.3018 - acc: 0.0046 - val_loss: 5.3000 - val_acc: 0.0050
Epoch 7/100
 - 33s - loss: 5.3015 - acc: 0.0045 - val_loss: 5.2997 - val_acc: 0.0050
Epoch 8/100
 - 33s - loss: 5.3012 - acc: 0.0049 - val_loss: 5.2995 - val_acc: 0.0050
Epoch 9/100
 - 33s - loss: 5.3013 - acc: 0.0044 - val_loss: 5.2996 - val_acc: 0.0050
Epoch 10/100
 - 33s - loss: 5.3013 - acc: 0.0050 - val_loss: 5.2995 - val_acc: 0.0050
Epoch 11/100
 - 33s - loss: 5.3012 - acc: 0.0047 - val_loss: 5.2996 - val_acc: 0.0050
Epoch 12/100
 - 33s - loss: 5.3012 - acc: 0.0046 - val_loss: 5.2997 - val_acc: 0.0050
Epoch 13/100
 - 33s - loss: 5.3014 - acc: 0.0045 - val_loss: 5.2993 - val_acc: 0.0050
Epoch 14/100
 - 33s - loss: 5.3012 - acc: 0.0046 - val_loss: 5.2996 - val_acc: 0.0050
Epoch 15/100
 - 33s - loss: 5.3012 - acc: 0.0045 - val_loss: 5.2997 - val_acc: 0.0050
Epoch 16/100
 - 33s - loss: 5.3012 - acc: 0.0046 - val_loss: 5.2996 - val_acc: 0.0050
Epoch 17/100
 - 33s - loss: 5.3013 - acc: 0.0046 - val_loss: 5.2997 - val_acc: 0.0050
Epoch 18/100
 - 33s - loss: 5.3013 - acc: 0.0045 - val_loss: 5.2996 - val_acc: 0.0050
Epoch 19/100
 - 33s - loss: 5.3013 - acc: 0.0043 - val_loss: 5.2996 - val_acc: 0.0050
Epoch 20/100
 - 33s - loss: 5.3012 - acc: 0.0046 - val_loss: 5.2996 - val_acc: 0.0050
Epoch 21/100
 - 33s - loss: 5.3013 - acc: 0.0046 - val_loss: 5.2993 - val_acc: 0.0050
Epoch 22/100
 - 33s - loss: 5.3012 - acc: 0.0047 - val_loss: 5.2996 - val_acc: 0.0050
Epoch 23/100
 - 33s - loss: 5.3013 - acc: 0.0045 - val_loss: 5.2995 - val_acc: 0.0050
Epoch 24/100
 - 33s - loss: 5.3014 - acc: 0.0045 - val_loss: 5.2993 - val_acc: 0.0050
Epoch 25/100
 - 33s - loss: 5.3013 - acc: 0.0047 - val_loss: 5.2994 - val_acc: 0.0050
Epoch 26/100
 - 33s - loss: 5.3012 - acc: 0.0046 - val_loss: 5.2998 - val_acc: 0.0050
Epoch 27/100
 - 33s - loss: 5.3013 - acc: 0.0045 - val_loss: 5.2996 - val_acc: 0.0050
Epoch 28/100
 - 33s - loss: 5.3013 - acc: 0.0043 - val_loss: 5.2996 - val_acc: 0.0050
Epoch 29/100
 - 33s - loss: 5.3014 - acc: 0.0046 - val_loss: 5.2995 - val_acc: 0.0050
Epoch 30/100
 - 33s - loss: 5.3013 - acc: 0.0047 - val_loss: 5.2996 - val_acc: 0.0050
Epoch 31/100
 - 33s - loss: 5.3014 - acc: 0.0043 - val_loss: 5.2996 - val_acc: 0.0050
Epoch 32/100
 - 33s - loss: 5.3013 - acc: 0.0046 - val_loss: 5.2996 - val_acc: 0.0050
Epoch 33/100
 - 33s - loss: 5.3013 - acc: 0.0049 - val_loss: 5.2996 - val_acc: 0.0050
Epoch 34/100
 - 33s - loss: 5.3012 - acc: 0.0045 - val_loss: 5.2995 - val_acc: 0.0050
Epoch 35/100
 - 33s - loss: 5.3013 - acc: 0.0046 - val_loss: 5.2995 - val_acc: 0.0050
Epoch 36/100
 - 33s - loss: 5.3012 - acc: 0.0046 - val_loss: 5.2996 - val_acc: 0.0050
Epoch 37/100
 - 33s - loss: 5.3013 - acc: 0.0046 - val_loss: 5.2996 - val_acc: 0.0050
Epoch 38/100
 - 33s - loss: 5.3013 - acc: 0.0048 - val_loss: 5.2995 - val_acc: 0.0050
Epoch 39/100
 - 33s - loss: 5.3013 - acc: 0.0047 - val_loss: 5.2995 - val_acc: 0.0050
Epoch 40/100
 - 33s - loss: 5.3014 - acc: 0.0046 - val_loss: 5.2994 - val_acc: 0.0050
Epoch 41/100
 - 33s - loss: 5.3012 - acc: 0.0045 - val_loss: 5.2996 - val_acc: 0.0050
Epoch 42/100
 - 33s - loss: 5.3014 - acc: 0.0044 - val_loss: 5.2995 - val_acc: 0.0050
Epoch 43/100
 - 33s - loss: 5.3012 - acc: 0.0047 - val_loss: 5.2997 - val_acc: 0.0050
Epoch 44/100
 - 33s - loss: 5.3012 - acc: 0.0048 - val_loss: 5.2996 - val_acc: 0.0050
Epoch 45/100
 - 33s - loss: 5.3013 - acc: 0.0047 - val_loss: 5.2996 - val_acc: 0.0050
Epoch 46/100
 - 33s - loss: 5.3013 - acc: 0.0046 - val_loss: 5.2995 - val_acc: 0.0050
Epoch 47/100
 - 33s - loss: 5.3013 - acc: 0.0047 - val_loss: 5.2995 - val_acc: 0.0050
Epoch 48/100
 - 33s - loss: 5.3013 - acc: 0.0044 - val_loss: 5.2995 - val_acc: 0.0050
Epoch 49/100
 - 33s - loss: 5.3013 - acc: 0.0045 - val_loss: 5.2994 - val_acc: 0.0050
Epoch 50/100
 - 33s - loss: 5.3013 - acc: 0.0043 - val_loss: 5.2995 - val_acc: 0.0050
Epoch 51/100
 - 33s - loss: 5.3013 - acc: 0.0045 - val_loss: 5.2995 - val_acc: 0.0050
Epoch 52/100
 - 33s - loss: 5.3014 - acc: 0.0042 - val_loss: 5.2994 - val_acc: 0.0050
Epoch 53/100
 - 33s - loss: 5.3013 - acc: 0.0047 - val_loss: 5.2994 - val_acc: 0.0050
Epoch 54/100
 - 33s - loss: 5.3012 - acc: 0.0049 - val_loss: 5.2996 - val_acc: 0.0050
Epoch 55/100
 - 33s - loss: 5.3013 - acc: 0.0049 - val_loss: 5.2996 - val_acc: 0.0050
Epoch 56/100
 - 33s - loss: 5.3014 - acc: 0.0044 - val_loss: 5.2995 - val_acc: 0.0050
Epoch 57/100
 - 33s - loss: 5.3012 - acc: 0.0048 - val_loss: 5.2995 - val_acc: 0.0050
Epoch 58/100
 - 33s - loss: 5.3012 - acc: 0.0048 - val_loss: 5.2997 - val_acc: 0.0050
Epoch 59/100
 - 33s - loss: 5.3013 - acc: 0.0046 - val_loss: 5.2994 - val_acc: 0.0050
Epoch 60/100
 - 33s - loss: 5.3012 - acc: 0.0049 - val_loss: 5.2996 - val_acc: 0.0050
Epoch 61/100
 - 33s - loss: 5.3012 - acc: 0.0048 - val_loss: 5.2995 - val_acc: 0.0050
Epoch 62/100
 - 33s - loss: 5.3013 - acc: 0.0047 - val_loss: 5.2996 - val_acc: 0.0050
Epoch 63/100
 - 33s - loss: 5.3013 - acc: 0.0043 - val_loss: 5.2996 - val_acc: 0.0050
Epoch 64/100
 - 33s - loss: 5.3013 - acc: 0.0045 - val_loss: 5.2998 - val_acc: 0.0050
Epoch 65/100
 - 33s - loss: 5.3013 - acc: 0.0046 - val_loss: 5.2997 - val_acc: 0.0050
Epoch 66/100
 - 33s - loss: 5.3013 - acc: 0.0045 - val_loss: 5.2994 - val_acc: 0.0050
Epoch 67/100
 - 33s - loss: 5.3013 - acc: 0.0047 - val_loss: 5.2995 - val_acc: 0.0050
Epoch 68/100
 - 33s - loss: 5.3011 - acc: 0.0047 - val_loss: 5.2997 - val_acc: 0.0050
Epoch 69/100
 - 33s - loss: 5.3013 - acc: 0.0046 - val_loss: 5.2994 - val_acc: 0.0050
Epoch 70/100
 - 33s - loss: 5.3013 - acc: 0.0046 - val_loss: 5.2995 - val_acc: 0.0050
Epoch 71/100
 - 33s - loss: 5.3011 - acc: 0.0046 - val_loss: 5.2996 - val_acc: 0.0050
Epoch 72/100
 - 33s - loss: 5.3012 - acc: 0.0041 - val_loss: 5.2996 - val_acc: 0.0050
Epoch 73/100
 - 33s - loss: 5.3011 - acc: 0.0045 - val_loss: 5.2998 - val_acc: 0.0050
Epoch 74/100
 - 33s - loss: 5.3014 - acc: 0.0046 - val_loss: 5.2994 - val_acc: 0.0050
Epoch 75/100
 - 33s - loss: 5.3013 - acc: 0.0045 - val_loss: 5.2996 - val_acc: 0.0050
Epoch 76/100
 - 33s - loss: 5.3013 - acc: 0.0047 - val_loss: 5.2995 - val_acc: 0.0050
Epoch 77/100
 - 33s - loss: 5.3013 - acc: 0.0047 - val_loss: 5.2995 - val_acc: 0.0050
Epoch 78/100
 - 33s - loss: 5.3014 - acc: 0.0042 - val_loss: 5.2993 - val_acc: 0.0050
Epoch 79/100
 - 33s - loss: 5.3012 - acc: 0.0047 - val_loss: 5.2994 - val_acc: 0.0050
Epoch 80/100
 - 33s - loss: 5.3013 - acc: 0.0045 - val_loss: 5.2994 - val_acc: 0.0050
Epoch 81/100
 - 33s - loss: 5.3013 - acc: 0.0046 - val_loss: 5.2997 - val_acc: 0.0050
Epoch 82/100
 - 33s - loss: 5.3014 - acc: 0.0046 - val_loss: 5.2993 - val_acc: 0.0050
Epoch 83/100
 - 33s - loss: 5.3012 - acc: 0.0045 - val_loss: 5.2996 - val_acc: 0.0050
Epoch 84/100
 - 33s - loss: 5.3014 - acc: 0.0044 - val_loss: 5.2994 - val_acc: 0.0050
Epoch 85/100
 - 33s - loss: 5.3013 - acc: 0.0046 - val_loss: 5.2997 - val_acc: 0.0050
Epoch 86/100
 - 33s - loss: 5.3013 - acc: 0.0045 - val_loss: 5.2996 - val_acc: 0.0050
Epoch 87/100
 - 33s - loss: 5.3014 - acc: 0.0041 - val_loss: 5.2994 - val_acc: 0.0050
Epoch 88/100
 - 33s - loss: 5.3013 - acc: 0.0046 - val_loss: 5.2996 - val_acc: 0.0050
Epoch 89/100
 - 33s - loss: 5.3012 - acc: 0.0049 - val_loss: 5.2997 - val_acc: 0.0050
Epoch 90/100
 - 33s - loss: 5.3013 - acc: 0.0047 - val_loss: 5.2995 - val_acc: 0.0050
Epoch 91/100
 - 33s - loss: 5.3012 - acc: 0.0046 - val_loss: 5.2998 - val_acc: 0.0050
Epoch 92/100
 - 33s - loss: 5.3012 - acc: 0.0046 - val_loss: 5.2996 - val_acc: 0.0050
Epoch 93/100
 - 33s - loss: 5.3013 - acc: 0.0043 - val_loss: 5.2994 - val_acc: 0.0050
Epoch 94/100
 - 33s - loss: 5.3013 - acc: 0.0048 - val_loss: 5.2996 - val_acc: 0.0050
Epoch 95/100
 - 33s - loss: 5.3013 - acc: 0.0044 - val_loss: 5.2995 - val_acc: 0.0050
Epoch 96/100
 - 33s - loss: 5.3011 - acc: 0.0046 - val_loss: 5.2997 - val_acc: 0.0050
Epoch 97/100
 - 33s - loss: 5.3013 - acc: 0.0045 - val_loss: 5.2995 - val_acc: 0.0050
Epoch 98/100
 - 33s - loss: 5.3012 - acc: 0.0043 - val_loss: 5.2998 - val_acc: 0.0050
Epoch 99/100
 - 33s - loss: 5.3013 - acc: 0.0045 - val_loss: 5.2996 - val_acc: 0.0050
Epoch 100/100
 - 33s - loss: 5.3013 - acc: 0.0048 - val_loss: 5.2995 - val_acc: 0.0050
training history:
{'val_loss': [5.301279319000244, 5.30010786895752, 5.299773484802246, 5.299987509155273, 5.300360760498047, 5.299960269165039, 5.299682818603515, 5.299545402526856, 5.299575337219238, 5.299459948730469, 5.299561434936524, 5.299693920135498, 5.299305480957031, 5.299603521728516, 5.299656098937988, 5.299621566772461, 5.299682123565674, 5.299641693115235, 5.299610254669189, 5.299630863952637, 5.299322392272949, 5.299630639648438, 5.299500552368164, 5.299344905090332, 5.299374287414551, 5.299798917388916, 5.299552737426758, 5.2995952682495115, 5.299486740112305, 5.299579025268555, 5.299578407287598, 5.299628415679932, 5.29960549621582, 5.299547743988037, 5.2995356307983394, 5.299593370056153, 5.299608297729492, 5.299481213378907, 5.299480836486817, 5.299437995910645, 5.299615447235108, 5.299545571136474, 5.299671813964844, 5.299628527832032, 5.2996080772399905, 5.299491696166992, 5.299526467895507, 5.299525057220459, 5.299401712799073, 5.299454830932617, 5.299497174072266, 5.2993561500549315, 5.299367611694336, 5.299631222534179, 5.299584811401367, 5.29953960723877, 5.299535565185547, 5.299667893981933, 5.299443119812012, 5.29955602722168, 5.2994716941833495, 5.299565061950684, 5.299647502136231, 5.299802645874023, 5.299713861846924, 5.2993821571350095, 5.299496826171875, 5.2996747695922855, 5.2994499710083005, 5.299528491210937, 5.299631092834472, 5.299581413269043, 5.299841986083984, 5.299392057800293, 5.2995902420043945, 5.299518951416015, 5.299499310302735, 5.299338145446777, 5.2994326278686525, 5.299412130737305, 5.2996962409973145, 5.299301943206787, 5.2995781845092775, 5.299426090240479, 5.299657165527344, 5.299625682067871, 5.299427850341797, 5.299586785888672, 5.299729425048828, 5.2995244491577145, 5.299793537139893, 5.299633764648438, 5.299426278686523, 5.299647915649414, 5.299486418151855, 5.299661916351319, 5.29948671951294, 5.299769667816162, 5.2995809616088865, 5.299520484924316], 'val_acc': [0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005], 'loss': [5.311443314819336, 5.3022172161865235, 5.301887772369385, 5.301778028564453, 5.301700839233399, 5.30180970123291, 5.301483691558838, 5.301164813232422, 5.301251222076416, 5.3013401419067385, 5.30123379486084, 5.301209639129639, 5.301394277496338, 5.301194630126953, 5.3012148622131345, 5.301201942749024, 5.301255001068116, 5.301294140014648, 5.301252711486816, 5.301181138305664, 5.3012652760314944, 5.301247899475098, 5.301329585876465, 5.301362432556152, 5.301292052001953, 5.301236353607178, 5.301291441650391, 5.30125281539917, 5.301414080505371, 5.301336821899414, 5.301355885162353, 5.301287696075439, 5.301315616149902, 5.301243780517578, 5.301320947875976, 5.301217165679931, 5.3012505549621585, 5.301315658721924, 5.30129706085205, 5.301370009765625, 5.30122589263916, 5.301351700134277, 5.301154636535644, 5.3012232543945315, 5.3013174508667, 5.301290306243897, 5.301320046081543, 5.301287803955078, 5.301308018341064, 5.301291266784668, 5.301300653076172, 5.301355427856445, 5.30129593170166, 5.301231206512451, 5.301327077026367, 5.301386354980469, 5.3012226953125, 5.301184616699219, 5.301312385864258, 5.301220988922119, 5.301244822235107, 5.3013036477661135, 5.301289316101074, 5.301251210021973, 5.301262649536133, 5.3013390756225585, 5.301257078247071, 5.301122684020996, 5.30128974243164, 5.301330469665527, 5.301145819549561, 5.301192462463379, 5.301096790466309, 5.301443560180664, 5.301260218811035, 5.301351292724609, 5.301318013000488, 5.30135634552002, 5.301197439727783, 5.301263162536621, 5.3012526658630374, 5.301362280883789, 5.3012236389160154, 5.30138926361084, 5.301307984161377, 5.301292823486328, 5.3014216673278804, 5.3012687080383305, 5.301201021728516, 5.301345024414062, 5.3012014329528805, 5.301211883392334, 5.3012927508544925, 5.301311478881836, 5.301283621520996, 5.301130395812988, 5.301342691345215, 5.301148346710205, 5.301328677368164, 5.301350139770507], 'acc': [0.00474, 0.00491, 0.00479, 0.00459, 0.00464, 0.00464, 0.0045, 0.0049, 0.00436, 0.00497, 0.00468, 0.00456, 0.00453, 0.00465, 0.00451, 0.00458, 0.00464, 0.00448, 0.00432, 0.00465, 0.00465, 0.00468, 0.00455, 0.00451, 0.0047, 0.00464, 0.00453, 0.00434, 0.00465, 0.00469, 0.00431, 0.00462, 0.00488, 0.00448, 0.00463, 0.00462, 0.00459, 0.00478, 0.0047, 0.00459, 0.00452, 0.00444, 0.00467, 0.00476, 0.00471, 0.00456, 0.00474, 0.00443, 0.00447, 0.00435, 0.00449, 0.00417, 0.00469, 0.00491, 0.00486, 0.00438, 0.00485, 0.00476, 0.00464, 0.0049, 0.00485, 0.00469, 0.00434, 0.00449, 0.00461, 0.00448, 0.00467, 0.00466, 0.00464, 0.00463, 0.00455, 0.00415, 0.00447, 0.0046, 0.00449, 0.00473, 0.0047, 0.00425, 0.00473, 0.0045, 0.0046, 0.00459, 0.00453, 0.0044, 0.0046, 0.00455, 0.00412, 0.00457, 0.00486, 0.00469, 0.0046, 0.00464, 0.00435, 0.00484, 0.00436, 0.00461, 0.00449, 0.0043, 0.00447, 0.0048]}
Training time: 
3341.8676192760468
Evaluation results:  [5.299520484161377, 0.005]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_16 (Conv2D)           (None, 58, 62, 48)        3072      
_________________________________________________________________
activation_10 (Activation)   (None, 58, 62, 48)        0         
_________________________________________________________________
max_pooling2d_10 (MaxPooling (None, 28, 30, 48)        0         
_________________________________________________________________
conv2d_17 (Conv2D)           (None, 24, 30, 128)       30848     
_________________________________________________________________
activation_11 (Activation)   (None, 24, 30, 128)       0         
_________________________________________________________________
max_pooling2d_11 (MaxPooling (None, 11, 14, 128)       0         
_________________________________________________________________
conv2d_18 (Conv2D)           (None, 9, 14, 192)        73920     
_________________________________________________________________
conv2d_19 (Conv2D)           (None, 7, 14, 192)        110784    
_________________________________________________________________
conv2d_20 (Conv2D)           (None, 5, 14, 128)        73856     
_________________________________________________________________
activation_12 (Activation)   (None, 5, 14, 128)        0         
_________________________________________________________________
max_pooling2d_12 (MaxPooling (None, 2, 6, 128)         0         
_________________________________________________________________
flatten_4 (Flatten)          (None, 1536)              0         
_________________________________________________________________
dense_10 (Dense)             (None, 2048)              3147776   
_________________________________________________________________
dense_11 (Dense)             (None, 2048)              4196352   
_________________________________________________________________
dense_12 (Dense)             (None, 200)               409800    
=================================================================
Total params: 8,046,408
Trainable params: 8,046,408
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
 - 31s - loss: 4.9328 - acc: 0.0377 - val_loss: 4.4610 - val_acc: 0.0837
Epoch 2/100
 - 31s - loss: 4.3262 - acc: 0.1024 - val_loss: 4.3211 - val_acc: 0.1098
Epoch 3/100
 - 31s - loss: 4.0541 - acc: 0.1416 - val_loss: 4.1515 - val_acc: 0.1309
Epoch 4/100
 - 31s - loss: 3.9041 - acc: 0.1631 - val_loss: 3.8968 - val_acc: 0.1674
Epoch 5/100
 - 31s - loss: 3.8106 - acc: 0.1767 - val_loss: 3.9782 - val_acc: 0.1543
Epoch 6/100
 - 31s - loss: 3.7334 - acc: 0.1889 - val_loss: 3.8388 - val_acc: 0.1769
Epoch 7/100
 - 31s - loss: 3.6838 - acc: 0.1978 - val_loss: 4.1538 - val_acc: 0.1444
Epoch 8/100
 - 31s - loss: 3.6496 - acc: 0.2041 - val_loss: 3.8828 - val_acc: 0.1763
Epoch 9/100
 - 31s - loss: 3.6149 - acc: 0.2107 - val_loss: 3.8509 - val_acc: 0.1744
Epoch 10/100
 - 31s - loss: 3.5861 - acc: 0.2147 - val_loss: 3.8554 - val_acc: 0.1833
Epoch 11/100
 - 31s - loss: 3.5607 - acc: 0.2209 - val_loss: 4.2207 - val_acc: 0.1416
Epoch 12/100
 - 31s - loss: 3.5411 - acc: 0.2222 - val_loss: 4.1605 - val_acc: 0.1660
Epoch 13/100
 - 31s - loss: 3.5292 - acc: 0.2240 - val_loss: 4.1784 - val_acc: 0.1803
Epoch 14/100
 - 31s - loss: 3.5163 - acc: 0.2273 - val_loss: 3.9741 - val_acc: 0.1633
Epoch 15/100
 - 31s - loss: 3.5126 - acc: 0.2280 - val_loss: 4.0149 - val_acc: 0.1710
Epoch 16/100
 - 31s - loss: 3.4984 - acc: 0.2320 - val_loss: 3.9333 - val_acc: 0.1803
Epoch 17/100
 - 31s - loss: 3.4875 - acc: 0.2318 - val_loss: 3.9717 - val_acc: 0.1856
Epoch 18/100
 - 31s - loss: 3.4769 - acc: 0.2332 - val_loss: 3.9905 - val_acc: 0.1665
Epoch 19/100
 - 31s - loss: 3.4781 - acc: 0.2352 - val_loss: 3.9776 - val_acc: 0.1717
Epoch 20/100
 - 31s - loss: 3.4772 - acc: 0.2361 - val_loss: 3.9888 - val_acc: 0.1784
Epoch 21/100
 - 31s - loss: 3.4734 - acc: 0.2388 - val_loss: 4.0150 - val_acc: 0.1679
Epoch 22/100
 - 31s - loss: 3.4768 - acc: 0.2368 - val_loss: 4.1544 - val_acc: 0.1634
Epoch 23/100
 - 31s - loss: 3.4647 - acc: 0.2365 - val_loss: 4.0527 - val_acc: 0.1793
Epoch 24/100
 - 31s - loss: 3.4659 - acc: 0.2390 - val_loss: 4.1436 - val_acc: 0.1706
Epoch 25/100
 - 31s - loss: 3.4567 - acc: 0.2413 - val_loss: 4.1578 - val_acc: 0.1527
Epoch 26/100
 - 31s - loss: 3.4354 - acc: 0.2437 - val_loss: 4.0351 - val_acc: 0.1799
Epoch 27/100
 - 31s - loss: 3.4421 - acc: 0.2425 - val_loss: 4.1815 - val_acc: 0.1699
Epoch 28/100
 - 31s - loss: 3.4207 - acc: 0.2480 - val_loss: 4.2023 - val_acc: 0.1549
Epoch 29/100
 - 31s - loss: 3.4403 - acc: 0.2444 - val_loss: 4.0474 - val_acc: 0.1724
Epoch 30/100
 - 31s - loss: 3.4285 - acc: 0.2465 - val_loss: 4.0875 - val_acc: 0.1663
Epoch 31/100
 - 31s - loss: 3.4349 - acc: 0.2449 - val_loss: 4.0495 - val_acc: 0.1723
Epoch 32/100
 - 31s - loss: 3.4402 - acc: 0.2433 - val_loss: 4.1942 - val_acc: 0.1431
Epoch 33/100
 - 31s - loss: 3.4344 - acc: 0.2446 - val_loss: 4.2341 - val_acc: 0.1742
Epoch 34/100
 - 31s - loss: 3.4397 - acc: 0.2433 - val_loss: 4.2773 - val_acc: 0.1679
Epoch 35/100
 - 31s - loss: 3.4372 - acc: 0.2458 - val_loss: 4.2488 - val_acc: 0.1585
Epoch 36/100
 - 31s - loss: 3.4493 - acc: 0.2424 - val_loss: 4.1538 - val_acc: 0.1623
Epoch 37/100
 - 31s - loss: 3.4444 - acc: 0.2445 - val_loss: 4.3969 - val_acc: 0.1524
Epoch 38/100
 - 31s - loss: 3.4469 - acc: 0.2464 - val_loss: 4.2511 - val_acc: 0.1547
Epoch 39/100
 - 31s - loss: 3.4452 - acc: 0.2455 - val_loss: 4.1545 - val_acc: 0.1697
Epoch 40/100
 - 31s - loss: 3.4342 - acc: 0.2461 - val_loss: 4.4353 - val_acc: 0.1507
Epoch 41/100
 - 31s - loss: 3.4437 - acc: 0.2458 - val_loss: 4.6338 - val_acc: 0.1529
Epoch 42/100
 - 31s - loss: 3.4411 - acc: 0.2465 - val_loss: 4.4011 - val_acc: 0.1380
Epoch 43/100
 - 31s - loss: 3.4350 - acc: 0.2460 - val_loss: 4.4680 - val_acc: 0.1316
Epoch 44/100
 - 31s - loss: 3.4421 - acc: 0.2460 - val_loss: 4.2379 - val_acc: 0.1563
Epoch 45/100
 - 31s - loss: 3.4352 - acc: 0.2459 - val_loss: 4.2226 - val_acc: 0.1575
Epoch 46/100
 - 31s - loss: 3.4182 - acc: 0.2503 - val_loss: 4.3368 - val_acc: 0.1510
Epoch 47/100
 - 31s - loss: 3.4271 - acc: 0.2489 - val_loss: 4.3852 - val_acc: 0.1692
Epoch 48/100
 - 31s - loss: 3.4284 - acc: 0.2485 - val_loss: 4.2538 - val_acc: 0.1608
Epoch 49/100
 - 31s - loss: 3.4190 - acc: 0.2512 - val_loss: 4.4877 - val_acc: 0.1391
Epoch 50/100
 - 31s - loss: 3.4219 - acc: 0.2505 - val_loss: 4.3960 - val_acc: 0.1511
Epoch 51/100
 - 31s - loss: 3.4289 - acc: 0.2497 - val_loss: 4.4099 - val_acc: 0.1461
Epoch 52/100
 - 31s - loss: 3.4334 - acc: 0.2489 - val_loss: 4.5732 - val_acc: 0.1430
Epoch 53/100
 - 31s - loss: 3.4463 - acc: 0.2477 - val_loss: 4.5746 - val_acc: 0.1571
Epoch 54/100
 - 31s - loss: 3.4775 - acc: 0.2439 - val_loss: 4.7251 - val_acc: 0.1382
Epoch 55/100
 - 31s - loss: 3.4770 - acc: 0.2428 - val_loss: 4.5175 - val_acc: 0.1379
Epoch 56/100
 - 31s - loss: 3.4866 - acc: 0.2426 - val_loss: 4.3658 - val_acc: 0.1522
Epoch 57/100
 - 31s - loss: 3.4911 - acc: 0.2427 - val_loss: 4.6366 - val_acc: 0.1433
Epoch 58/100
 - 31s - loss: 3.4984 - acc: 0.2413 - val_loss: 4.8880 - val_acc: 0.1084
Epoch 59/100
 - 31s - loss: 3.5164 - acc: 0.2393 - val_loss: 4.3310 - val_acc: 0.1404
Epoch 60/100
 - 31s - loss: 3.5451 - acc: 0.2355 - val_loss: 5.2044 - val_acc: 0.1125
Epoch 61/100
 - 31s - loss: 3.5734 - acc: 0.2301 - val_loss: 4.6534 - val_acc: 0.1459
Epoch 62/100
 - 31s - loss: 3.5951 - acc: 0.2262 - val_loss: 4.5264 - val_acc: 0.1378
Epoch 63/100
 - 31s - loss: 3.6286 - acc: 0.2222 - val_loss: 4.2890 - val_acc: 0.1408
Epoch 64/100
 - 31s - loss: 3.6446 - acc: 0.2199 - val_loss: 4.3531 - val_acc: 0.1461
Epoch 65/100
 - 31s - loss: 3.6719 - acc: 0.2147 - val_loss: 4.3865 - val_acc: 0.1328
Epoch 66/100
 - 31s - loss: 3.6859 - acc: 0.2128 - val_loss: 4.4298 - val_acc: 0.1363
Epoch 67/100
 - 31s - loss: 3.7160 - acc: 0.2093 - val_loss: 4.8626 - val_acc: 0.1379
Epoch 68/100
 - 31s - loss: 3.7266 - acc: 0.2067 - val_loss: 4.4252 - val_acc: 0.1336
Epoch 69/100
 - 31s - loss: 3.7748 - acc: 0.2028 - val_loss: 4.3562 - val_acc: 0.1395
Epoch 70/100
 - 31s - loss: 3.7912 - acc: 0.1990 - val_loss: 4.6308 - val_acc: 0.1081
Epoch 71/100
 - 31s - loss: 3.8076 - acc: 0.1954 - val_loss: 4.4158 - val_acc: 0.1258
Epoch 72/100
 - 31s - loss: 3.8203 - acc: 0.1937 - val_loss: 4.9114 - val_acc: 0.1034
Epoch 73/100
 - 31s - loss: 3.8277 - acc: 0.1937 - val_loss: 4.4793 - val_acc: 0.1381
Epoch 74/100
 - 31s - loss: 3.8418 - acc: 0.1883 - val_loss: 4.5015 - val_acc: 0.1274
Epoch 75/100
 - 31s - loss: 3.8588 - acc: 0.1880 - val_loss: 4.5454 - val_acc: 0.1244
Epoch 76/100
 - 31s - loss: 3.8678 - acc: 0.1861 - val_loss: 4.2832 - val_acc: 0.1342
Epoch 77/100
 - 31s - loss: 3.9069 - acc: 0.1823 - val_loss: 4.4879 - val_acc: 0.1354
Epoch 78/100
 - 31s - loss: 3.9268 - acc: 0.1790 - val_loss: 4.4357 - val_acc: 0.1216
Epoch 79/100
 - 31s - loss: 3.9365 - acc: 0.1774 - val_loss: 4.2573 - val_acc: 0.1289
Epoch 80/100
 - 31s - loss: 3.9805 - acc: 0.1708 - val_loss: 4.3957 - val_acc: 0.1278
Epoch 81/100
 - 31s - loss: 4.0199 - acc: 0.1642 - val_loss: 4.5120 - val_acc: 0.1220
Epoch 82/100
 - 31s - loss: 4.0616 - acc: 0.1576 - val_loss: 4.1932 - val_acc: 0.1384
Epoch 83/100
 - 31s - loss: 4.0749 - acc: 0.1570 - val_loss: 4.4327 - val_acc: 0.1077
Epoch 84/100
 - 31s - loss: 4.0970 - acc: 0.1523 - val_loss: 4.9917 - val_acc: 0.0826
Epoch 85/100
 - 31s - loss: 4.0878 - acc: 0.1520 - val_loss: 4.7169 - val_acc: 0.0771
Epoch 86/100
 - 31s - loss: 4.0863 - acc: 0.1534 - val_loss: 4.3805 - val_acc: 0.1134
Epoch 87/100
 - 31s - loss: 4.0689 - acc: 0.1543 - val_loss: 4.4244 - val_acc: 0.1131
Epoch 88/100
 - 31s - loss: 4.0548 - acc: 0.1569 - val_loss: 4.2173 - val_acc: 0.1330
Epoch 89/100
 - 31s - loss: 4.0361 - acc: 0.1592 - val_loss: 4.4101 - val_acc: 0.1377
Epoch 90/100
 - 31s - loss: 4.0223 - acc: 0.1615 - val_loss: 4.3543 - val_acc: 0.1307
Epoch 91/100
 - 31s - loss: 4.0095 - acc: 0.1623 - val_loss: 4.7930 - val_acc: 0.1059
Epoch 92/100
 - 31s - loss: 4.0106 - acc: 0.1626 - val_loss: 4.1984 - val_acc: 0.1272
Epoch 93/100
 - 31s - loss: 4.0271 - acc: 0.1595 - val_loss: 4.3417 - val_acc: 0.1181
Epoch 94/100
 - 31s - loss: 4.0167 - acc: 0.1631 - val_loss: 4.2045 - val_acc: 0.1460
Epoch 95/100
 - 31s - loss: 4.0213 - acc: 0.1598 - val_loss: 4.2722 - val_acc: 0.1283
Epoch 96/100
 - 31s - loss: 4.0129 - acc: 0.1625 - val_loss: 4.5516 - val_acc: 0.1077
Epoch 97/100
 - 31s - loss: 4.0106 - acc: 0.1628 - val_loss: 4.4869 - val_acc: 0.1357
Epoch 98/100
 - 31s - loss: 4.0213 - acc: 0.1618 - val_loss: 4.2791 - val_acc: 0.1309
Epoch 99/100
 - 31s - loss: 4.0086 - acc: 0.1639 - val_loss: 4.3517 - val_acc: 0.1385
Epoch 100/100
 - 31s - loss: 4.0195 - acc: 0.1621 - val_loss: 4.3070 - val_acc: 0.1253
training history:
{'val_loss': [4.460954739379883, 4.321097245025634, 4.151471248626709, 3.8968378257751466, 3.978155871582031, 3.838807890319824, 4.153783512878418, 3.882781521987915, 3.850910778808594, 3.855382426071167, 4.220726587677002, 4.160497058868408, 4.178420803833008, 3.9741353523254395, 4.014913641357422, 3.9333129272460936, 3.971686671447754, 3.990467960357666, 3.9775816032409668, 3.988775329589844, 4.0149634765625, 4.154350411224366, 4.052657492065429, 4.1435635360717775, 4.15780544128418, 4.035060162353516, 4.1815442405700685, 4.202268801879883, 4.047432537841797, 4.087501194763184, 4.049512782287597, 4.194175968170166, 4.234084149169922, 4.2772677032470705, 4.24883512802124, 4.153778070831299, 4.396876923370361, 4.251101992034912, 4.154533428955078, 4.435299569702148, 4.633753143310547, 4.401070159912109, 4.468024653625489, 4.237941452026368, 4.222645024871826, 4.336752863311768, 4.385203147888183, 4.2537989318847655, 4.487742204284668, 4.396030794525147, 4.409885353851318, 4.573226502990723, 4.574565014648438, 4.72508032989502, 4.517532550048828, 4.365844050598144, 4.636615421295166, 4.888017420959472, 4.331007292175293, 5.204426214599609, 4.6533933486938475, 4.526389029693603, 4.289031861877441, 4.35308277130127, 4.3864711536407475, 4.429769985198974, 4.862606350708008, 4.425235824584961, 4.356168286132813, 4.630840287780762, 4.415813163757324, 4.911421144104004, 4.479327501678466, 4.501483470153809, 4.545377983093262, 4.283185884857177, 4.487929726409912, 4.435681492614746, 4.2573020492553715, 4.395744897460937, 4.512030841064453, 4.193206156921387, 4.432717477416992, 4.99166922454834, 4.716929392242432, 4.380517852783203, 4.424399775695801, 4.217252212524414, 4.410127590179443, 4.354266743469238, 4.793017527008057, 4.198408617401123, 4.3416903129577635, 4.204527124786377, 4.272225985717774, 4.551580609130859, 4.486941970825195, 4.279073065948486, 4.351694525146485, 4.307034732055664], 'val_acc': [0.0837, 0.1098, 0.1309, 0.1674, 0.1543, 0.1769, 0.1444, 0.1763, 0.1744, 0.1833, 0.1416, 0.166, 0.1803, 0.1633, 0.171, 0.1803, 0.1856, 0.1665, 0.1717, 0.1784, 0.1679, 0.1634, 0.1793, 0.1706, 0.1527, 0.1799, 0.1699, 0.1549, 0.1724, 0.1663, 0.1723, 0.1431, 0.1742, 0.1679, 0.1585, 0.1623, 0.1524, 0.1547, 0.1697, 0.1507, 0.1529, 0.138, 0.1316, 0.1563, 0.1575, 0.151, 0.1692, 0.1608, 0.1391, 0.1511, 0.1461, 0.143, 0.1571, 0.1382, 0.1379, 0.1522, 0.1433, 0.1084, 0.1404, 0.1125, 0.1459, 0.1378, 0.1408, 0.1461, 0.1328, 0.1363, 0.1379, 0.1336, 0.1395, 0.1081, 0.1258, 0.1034, 0.1381, 0.1274, 0.1244, 0.1342, 0.1354, 0.1216, 0.1289, 0.1278, 0.122, 0.1384, 0.1077, 0.0826, 0.0771, 0.1134, 0.1131, 0.133, 0.1377, 0.1307, 0.1059, 0.1272, 0.1181, 0.146, 0.1283, 0.1077, 0.1357, 0.1309, 0.1385, 0.1253], 'loss': [4.932734788208008, 4.326034692230224, 4.053946222534179, 3.904251789550781, 3.810487628326416, 3.733598203125, 3.683808102416992, 3.6495758863830567, 3.61497369468689, 3.586075002288818, 3.560791194381714, 3.541192224960327, 3.529287278442383, 3.516222546386719, 3.5124790537261963, 3.498527389373779, 3.4874500498199463, 3.476930067825317, 3.47791345123291, 3.4772752676391603, 3.4734377159118655, 3.4768188316345214, 3.4648472816467284, 3.465854386062622, 3.4566126715087893, 3.4355335051727294, 3.4422570724487302, 3.4206304301452635, 3.440343666534424, 3.4286699197387693, 3.435074634399414, 3.440245986328125, 3.4341295236206055, 3.4395192663574217, 3.437200188217163, 3.449546548538208, 3.44431087600708, 3.4468190319824217, 3.4450457750701906, 3.434327004852295, 3.443671591720581, 3.4414649310302736, 3.435246460723877, 3.441974054107666, 3.4352204632568357, 3.417885494232178, 3.426853162841797, 3.4282922735595704, 3.4189229402160644, 3.421867121429443, 3.42861287902832, 3.433369225463867, 3.4463075676727293, 3.4778073959350584, 3.4770018766021726, 3.486596104888916, 3.49098864364624, 3.4985359266662597, 3.516428589630127, 3.544762480163574, 3.5734374283599855, 3.5949138777160643, 3.628522932662964, 3.6446122957611085, 3.671788170471191, 3.6859616539001463, 3.7160008915710447, 3.7266461515808107, 3.77502044090271, 3.791230551147461, 3.807534606628418, 3.8204056272888183, 3.827563885498047, 3.841667152557373, 3.8584310177612307, 3.8678341535949707, 3.9070419371032714, 3.9268844917297363, 3.936661167755127, 3.980437024841309, 4.019770387573242, 4.06145862197876, 4.074761649932861, 4.097032771759033, 4.087831985015869, 4.08628316329956, 4.068851464233399, 4.054912046813965, 4.03598029876709, 4.022347109603881, 4.009617183380127, 4.010622074584961, 4.027053394775391, 4.016845475769043, 4.021338571472168, 4.012613679351807, 4.010541785430908, 4.021278988647461, 4.008758977661133, 4.019621572418213], 'acc': [0.03767, 0.10238, 0.14163, 0.16304, 0.17673, 0.18888, 0.19775, 0.20408, 0.21066, 0.21467, 0.22086, 0.22215, 0.224, 0.22733, 0.22804, 0.23195, 0.23182, 0.23315, 0.23524, 0.23613, 0.23879, 0.23678, 0.23651, 0.23898, 0.24129, 0.24369, 0.24251, 0.24799, 0.24434, 0.24644, 0.24483, 0.24326, 0.24458, 0.24339, 0.2458, 0.24239, 0.24447, 0.24638, 0.24554, 0.24606, 0.24583, 0.24646, 0.24596, 0.24599, 0.24596, 0.2503, 0.2489, 0.24849, 0.25126, 0.25055, 0.24972, 0.24891, 0.2477, 0.24383, 0.24278, 0.2426, 0.24267, 0.24133, 0.23931, 0.23554, 0.23007, 0.22623, 0.2222, 0.21993, 0.21474, 0.2128, 0.20931, 0.20678, 0.20277, 0.19904, 0.19545, 0.19365, 0.19374, 0.18835, 0.18809, 0.18613, 0.18226, 0.17896, 0.1773, 0.17083, 0.16425, 0.15763, 0.157, 0.15228, 0.15203, 0.15338, 0.15427, 0.15692, 0.15917, 0.16152, 0.16226, 0.16261, 0.15946, 0.16309, 0.15982, 0.16253, 0.16286, 0.16182, 0.16391, 0.1621]}
Training time: 
3104.794345140457
Evaluation results:  [4.3070347705841066, 0.1253]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_21 (Conv2D)           (None, 58, 62, 48)        3072      
_________________________________________________________________
activation_13 (Activation)   (None, 58, 62, 48)        0         
_________________________________________________________________
max_pooling2d_13 (MaxPooling (None, 28, 30, 48)        0         
_________________________________________________________________
conv2d_22 (Conv2D)           (None, 24, 30, 128)       30848     
_________________________________________________________________
activation_14 (Activation)   (None, 24, 30, 128)       0         
_________________________________________________________________
max_pooling2d_14 (MaxPooling (None, 11, 14, 128)       0         
_________________________________________________________________
conv2d_23 (Conv2D)           (None, 9, 14, 192)        73920     
_________________________________________________________________
conv2d_24 (Conv2D)           (None, 7, 14, 192)        110784    
_________________________________________________________________
conv2d_25 (Conv2D)           (None, 5, 14, 128)        73856     
_________________________________________________________________
activation_15 (Activation)   (None, 5, 14, 128)        0         
_________________________________________________________________
max_pooling2d_15 (MaxPooling (None, 2, 6, 128)         0         
_________________________________________________________________
flatten_5 (Flatten)          (None, 1536)              0         
_________________________________________________________________
dense_13 (Dense)             (None, 2048)              3147776   
_________________________________________________________________
dense_14 (Dense)             (None, 2048)              4196352   
_________________________________________________________________
dense_15 (Dense)             (None, 200)               409800    
=================================================================
Total params: 8,046,408
Trainable params: 8,046,408
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
 - 28s - loss: 5.0758 - acc: 0.0240 - val_loss: 4.9048 - val_acc: 0.0420
Epoch 2/100
 - 28s - loss: 4.5934 - acc: 0.0751 - val_loss: 4.3610 - val_acc: 0.1058
Epoch 3/100
 - 28s - loss: 4.2243 - acc: 0.1218 - val_loss: 4.1574 - val_acc: 0.1291
Epoch 4/100
 - 28s - loss: 3.9384 - acc: 0.1623 - val_loss: 4.0035 - val_acc: 0.1523
Epoch 5/100
 - 28s - loss: 3.7007 - acc: 0.1992 - val_loss: 3.8135 - val_acc: 0.1840
Epoch 6/100
 - 28s - loss: 3.4742 - acc: 0.2346 - val_loss: 3.6917 - val_acc: 0.2039
Epoch 7/100
 - 28s - loss: 3.2585 - acc: 0.2701 - val_loss: 3.6319 - val_acc: 0.2167
Epoch 8/100
 - 28s - loss: 3.0445 - acc: 0.3052 - val_loss: 3.6418 - val_acc: 0.2230
Epoch 9/100
 - 28s - loss: 2.8133 - acc: 0.3453 - val_loss: 3.7910 - val_acc: 0.2193
Epoch 10/100
 - 28s - loss: 2.5634 - acc: 0.3925 - val_loss: 3.7934 - val_acc: 0.2267
Epoch 11/100
 - 28s - loss: 2.2855 - acc: 0.4441 - val_loss: 3.9079 - val_acc: 0.2196
Epoch 12/100
 - 28s - loss: 1.9812 - acc: 0.5019 - val_loss: 4.1152 - val_acc: 0.2113
Epoch 13/100
 - 28s - loss: 1.6648 - acc: 0.5687 - val_loss: 4.5541 - val_acc: 0.2043
Epoch 14/100
 - 28s - loss: 1.3447 - acc: 0.6381 - val_loss: 4.8224 - val_acc: 0.1993
Epoch 15/100
 - 28s - loss: 1.0479 - acc: 0.7092 - val_loss: 5.3573 - val_acc: 0.1957
Epoch 16/100
 - 28s - loss: 0.8044 - acc: 0.7687 - val_loss: 5.7727 - val_acc: 0.2015
Epoch 17/100
 - 28s - loss: 0.6297 - acc: 0.8144 - val_loss: 6.1549 - val_acc: 0.1955
Epoch 18/100
 - 28s - loss: 0.5042 - acc: 0.8502 - val_loss: 6.6599 - val_acc: 0.1956
Epoch 19/100
 - 28s - loss: 0.3941 - acc: 0.8821 - val_loss: 7.0417 - val_acc: 0.1975
Epoch 20/100
 - 28s - loss: 0.3406 - acc: 0.8974 - val_loss: 7.1697 - val_acc: 0.1921
Epoch 21/100
 - 28s - loss: 0.2775 - acc: 0.9161 - val_loss: 7.3676 - val_acc: 0.1961
Epoch 22/100
 - 28s - loss: 0.2189 - acc: 0.9332 - val_loss: 7.5127 - val_acc: 0.1987
Epoch 23/100
 - 28s - loss: 0.2104 - acc: 0.9360 - val_loss: 7.6081 - val_acc: 0.1871
Epoch 24/100
 - 28s - loss: 0.1790 - acc: 0.9462 - val_loss: 7.9629 - val_acc: 0.1978
Epoch 25/100
 - 28s - loss: 0.1622 - acc: 0.9512 - val_loss: 8.0584 - val_acc: 0.1902
Epoch 26/100
 - 28s - loss: 0.1381 - acc: 0.9576 - val_loss: 8.0769 - val_acc: 0.1979
Epoch 27/100
 - 28s - loss: 0.1145 - acc: 0.9653 - val_loss: 8.3098 - val_acc: 0.2011
Epoch 28/100
 - 28s - loss: 0.1174 - acc: 0.9652 - val_loss: 8.3145 - val_acc: 0.2002
Epoch 29/100
 - 28s - loss: 0.1119 - acc: 0.9662 - val_loss: 8.2267 - val_acc: 0.1948
Epoch 30/100
 - 28s - loss: 0.0928 - acc: 0.9728 - val_loss: 8.6079 - val_acc: 0.1999
Epoch 31/100
 - 28s - loss: 0.0890 - acc: 0.9736 - val_loss: 8.5933 - val_acc: 0.1974
Epoch 32/100
 - 28s - loss: 0.0773 - acc: 0.9764 - val_loss: 8.8684 - val_acc: 0.1957
Epoch 33/100
 - 28s - loss: 0.0855 - acc: 0.9746 - val_loss: 8.7685 - val_acc: 0.1885
Epoch 34/100
 - 28s - loss: 0.0777 - acc: 0.9769 - val_loss: 8.8531 - val_acc: 0.1954
Epoch 35/100
 - 28s - loss: 0.0779 - acc: 0.9770 - val_loss: 8.8456 - val_acc: 0.1925
Epoch 36/100
 - 28s - loss: 0.0676 - acc: 0.9798 - val_loss: 8.8690 - val_acc: 0.1951
Epoch 37/100
 - 28s - loss: 0.0630 - acc: 0.9814 - val_loss: 8.9158 - val_acc: 0.1932
Epoch 38/100
 - 28s - loss: 0.0578 - acc: 0.9834 - val_loss: 9.0040 - val_acc: 0.1986
Epoch 39/100
 - 28s - loss: 0.0522 - acc: 0.9849 - val_loss: 8.9722 - val_acc: 0.1991
Epoch 40/100
 - 28s - loss: 0.0423 - acc: 0.9879 - val_loss: 9.0423 - val_acc: 0.1950
Epoch 41/100
 - 28s - loss: 0.0550 - acc: 0.9840 - val_loss: 8.9897 - val_acc: 0.1922
Epoch 42/100
 - 28s - loss: 0.0489 - acc: 0.9857 - val_loss: 9.2074 - val_acc: 0.1957
Epoch 43/100
 - 28s - loss: 0.0588 - acc: 0.9823 - val_loss: 9.1115 - val_acc: 0.1966
Epoch 44/100
 - 28s - loss: 0.0535 - acc: 0.9845 - val_loss: 8.9530 - val_acc: 0.1974
Epoch 45/100
 - 28s - loss: 0.0479 - acc: 0.9859 - val_loss: 9.1877 - val_acc: 0.1971
Epoch 46/100
 - 28s - loss: 0.0570 - acc: 0.9838 - val_loss: 9.0515 - val_acc: 0.2035
Epoch 47/100
 - 28s - loss: 0.0533 - acc: 0.9839 - val_loss: 9.0081 - val_acc: 0.1995
Epoch 48/100
 - 28s - loss: 0.0393 - acc: 0.9887 - val_loss: 9.0455 - val_acc: 0.1983
Epoch 49/100
 - 28s - loss: 0.0349 - acc: 0.9903 - val_loss: 9.2375 - val_acc: 0.2052
Epoch 50/100
 - 28s - loss: 0.0319 - acc: 0.9906 - val_loss: 9.2146 - val_acc: 0.2028
Epoch 51/100
 - 28s - loss: 0.0328 - acc: 0.9906 - val_loss: 9.2178 - val_acc: 0.2081
Epoch 52/100
 - 28s - loss: 0.0310 - acc: 0.9914 - val_loss: 9.5375 - val_acc: 0.2011
Epoch 53/100
 - 28s - loss: 0.0373 - acc: 0.9896 - val_loss: 9.1479 - val_acc: 0.2007
Epoch 54/100
 - 28s - loss: 0.0299 - acc: 0.9916 - val_loss: 9.2229 - val_acc: 0.1992
Epoch 55/100
 - 28s - loss: 0.0277 - acc: 0.9919 - val_loss: 9.2748 - val_acc: 0.1994
Epoch 56/100
 - 28s - loss: 0.0309 - acc: 0.9911 - val_loss: 9.3399 - val_acc: 0.2059
Epoch 57/100
 - 28s - loss: 0.0345 - acc: 0.9904 - val_loss: 9.4026 - val_acc: 0.2023
Epoch 58/100
 - 28s - loss: 0.0342 - acc: 0.9903 - val_loss: 9.4039 - val_acc: 0.2039
Epoch 59/100
 - 28s - loss: 0.0349 - acc: 0.9903 - val_loss: 9.2162 - val_acc: 0.2004
Epoch 60/100
 - 28s - loss: 0.0457 - acc: 0.9868 - val_loss: 9.3334 - val_acc: 0.2020
Epoch 61/100
 - 28s - loss: 0.0281 - acc: 0.9923 - val_loss: 9.3676 - val_acc: 0.2059
Epoch 62/100
 - 28s - loss: 0.0226 - acc: 0.9938 - val_loss: 9.5959 - val_acc: 0.2012
Epoch 63/100
 - 28s - loss: 0.0297 - acc: 0.9913 - val_loss: 9.4827 - val_acc: 0.2048
Epoch 64/100
 - 28s - loss: 0.0232 - acc: 0.9934 - val_loss: 9.6384 - val_acc: 0.1996
Epoch 65/100
 - 28s - loss: 0.0440 - acc: 0.9873 - val_loss: 9.5320 - val_acc: 0.1974
Epoch 66/100
 - 28s - loss: 0.0601 - acc: 0.9833 - val_loss: 9.4442 - val_acc: 0.2023
Epoch 67/100
 - 28s - loss: 0.0481 - acc: 0.9865 - val_loss: 9.4389 - val_acc: 0.1992
Epoch 68/100
 - 28s - loss: 0.0348 - acc: 0.9900 - val_loss: 9.4700 - val_acc: 0.2002
Epoch 69/100
 - 28s - loss: 0.0260 - acc: 0.9926 - val_loss: 9.4554 - val_acc: 0.2027
Epoch 70/100
 - 28s - loss: 0.0229 - acc: 0.9939 - val_loss: 9.6381 - val_acc: 0.2014
Epoch 71/100
 - 28s - loss: 0.0192 - acc: 0.9946 - val_loss: 9.4965 - val_acc: 0.2019
Epoch 72/100
 - 28s - loss: 0.0137 - acc: 0.9963 - val_loss: 9.4957 - val_acc: 0.2056
Epoch 73/100
 - 28s - loss: 0.0092 - acc: 0.9980 - val_loss: 9.4998 - val_acc: 0.2075
Epoch 74/100
 - 28s - loss: 0.0072 - acc: 0.9988 - val_loss: 9.5756 - val_acc: 0.2065
Epoch 75/100
 - 28s - loss: 0.0048 - acc: 0.9994 - val_loss: 9.5878 - val_acc: 0.2106
Epoch 76/100
 - 28s - loss: 0.0037 - acc: 0.9997 - val_loss: 9.5074 - val_acc: 0.2130
Epoch 77/100
 - 28s - loss: 0.0032 - acc: 0.9997 - val_loss: 9.4825 - val_acc: 0.2101
Epoch 78/100
 - 28s - loss: 0.0031 - acc: 0.9998 - val_loss: 9.5229 - val_acc: 0.2151
Epoch 79/100
 - 28s - loss: 0.0030 - acc: 0.9998 - val_loss: 9.4952 - val_acc: 0.2137
Epoch 80/100
 - 28s - loss: 0.0029 - acc: 0.9998 - val_loss: 9.4659 - val_acc: 0.2123
Epoch 81/100
 - 28s - loss: 0.0029 - acc: 0.9998 - val_loss: 9.4789 - val_acc: 0.2148
Epoch 82/100
 - 28s - loss: 0.0029 - acc: 0.9998 - val_loss: 9.4564 - val_acc: 0.2129
Epoch 83/100
 - 28s - loss: 0.0029 - acc: 0.9998 - val_loss: 9.4208 - val_acc: 0.2123
Epoch 84/100
 - 28s - loss: 0.0029 - acc: 0.9998 - val_loss: 9.4692 - val_acc: 0.2136
Epoch 85/100
 - 28s - loss: 0.0028 - acc: 0.9998 - val_loss: 9.4604 - val_acc: 0.2142
Epoch 86/100
 - 28s - loss: 0.0027 - acc: 0.9998 - val_loss: 9.4557 - val_acc: 0.2165
Epoch 87/100
 - 28s - loss: 0.0028 - acc: 0.9998 - val_loss: 9.4312 - val_acc: 0.2140
Epoch 88/100
 - 28s - loss: 0.0027 - acc: 0.9998 - val_loss: 9.4555 - val_acc: 0.2165
Epoch 89/100
 - 28s - loss: 0.0028 - acc: 0.9998 - val_loss: 9.4365 - val_acc: 0.2152
Epoch 90/100
 - 28s - loss: 0.0027 - acc: 0.9998 - val_loss: 9.4437 - val_acc: 0.2163
Epoch 91/100
 - 28s - loss: 0.0027 - acc: 0.9998 - val_loss: 9.4214 - val_acc: 0.2159
Epoch 92/100
 - 28s - loss: 0.0028 - acc: 0.9998 - val_loss: 9.4151 - val_acc: 0.2159
Epoch 93/100
 - 28s - loss: 0.0028 - acc: 0.9998 - val_loss: 9.4278 - val_acc: 0.2170
Epoch 94/100
 - 28s - loss: 0.0027 - acc: 0.9998 - val_loss: 9.4231 - val_acc: 0.2161
Epoch 95/100
 - 28s - loss: 0.0027 - acc: 0.9998 - val_loss: 9.4161 - val_acc: 0.2171
Epoch 96/100
 - 28s - loss: 0.0026 - acc: 0.9998 - val_loss: 9.4267 - val_acc: 0.2163
Epoch 97/100
 - 28s - loss: 0.0029 - acc: 0.9998 - val_loss: 9.3904 - val_acc: 0.2166
Epoch 98/100
 - 28s - loss: 0.0027 - acc: 0.9998 - val_loss: 9.3843 - val_acc: 0.2172
Epoch 99/100
 - 28s - loss: 0.0027 - acc: 0.9998 - val_loss: 9.3841 - val_acc: 0.2170
Epoch 100/100
 - 28s - loss: 0.0027 - acc: 0.9998 - val_loss: 9.3759 - val_acc: 0.2152
training history:
{'val_loss': [4.904817095184326, 4.361005670166016, 4.157416412353515, 4.0034755527496335, 3.813492658996582, 3.6917158737182616, 3.631869853591919, 3.641809851074219, 3.7910305488586427, 3.7933653854370117, 3.9079178951263427, 4.1152174797058105, 4.554149645233155, 4.822446884155274, 5.3572714920043945, 5.772732727050781, 6.154879360961914, 6.6598521499633785, 7.041699641418457, 7.169737881469726, 7.367600895690918, 7.512684880065918, 7.608082374572754, 7.962937364196778, 8.058412928771972, 8.076858325195312, 8.30976053466797, 8.314457813262939, 8.226677923583985, 8.607908840942383, 8.593252759552001, 8.868366412353515, 8.768519494628906, 8.85305632019043, 8.845619128417969, 8.869011288452148, 8.915808221435547, 9.004021765136718, 8.972186770629882, 9.042285079956054, 8.98974313812256, 9.207446626281738, 9.111452665710448, 8.95298734588623, 9.187734196472167, 9.051495419311523, 9.008060470581055, 9.045485830688477, 9.237542520141602, 9.21460383605957, 9.217773986816406, 9.537470819091796, 9.1478685546875, 9.222850897216796, 9.274821475219726, 9.339871850585938, 9.40256782989502, 9.403866229248047, 9.216211012268067, 9.333440020751953, 9.367590173339844, 9.59585419921875, 9.482726293945312, 9.638352229309081, 9.532039517211913, 9.4441978515625, 9.438943132019043, 9.470015966796876, 9.455427386474609, 9.638091818237305, 9.496492892456054, 9.495728169250489, 9.499834927368164, 9.575558393859863, 9.587816973876953, 9.507370719909668, 9.482508917236329, 9.522855337524414, 9.495244610595703, 9.46592080078125, 9.478892306518555, 9.456407095336914, 9.420774432373047, 9.469238121032715, 9.460437287902833, 9.45571543121338, 9.43119226989746, 9.455460052490235, 9.436512322998047, 9.443725440979003, 9.421369496154785, 9.415144776916504, 9.427823846435547, 9.423136015319825, 9.416093994140626, 9.426672315979005, 9.390429107666016, 9.384272723388673, 9.38405761871338, 9.375865621948241], 'val_acc': [0.042, 0.1058, 0.1291, 0.1523, 0.184, 0.2039, 0.2167, 0.223, 0.2193, 0.2267, 0.2196, 0.2113, 0.2043, 0.1993, 0.1957, 0.2015, 0.1955, 0.1956, 0.1975, 0.1921, 0.1961, 0.1987, 0.1871, 0.1978, 0.1902, 0.1979, 0.2011, 0.2002, 0.1948, 0.1999, 0.1974, 0.1957, 0.1885, 0.1954, 0.1925, 0.1951, 0.1932, 0.1986, 0.1991, 0.195, 0.1922, 0.1957, 0.1966, 0.1974, 0.1971, 0.2035, 0.1995, 0.1983, 0.2052, 0.2028, 0.2081, 0.2011, 0.2007, 0.1992, 0.1994, 0.2059, 0.2023, 0.2039, 0.2004, 0.202, 0.2059, 0.2012, 0.2048, 0.1996, 0.1974, 0.2023, 0.1992, 0.2002, 0.2027, 0.2014, 0.2019, 0.2056, 0.2075, 0.2065, 0.2106, 0.213, 0.2101, 0.2151, 0.2137, 0.2123, 0.2148, 0.2129, 0.2123, 0.2136, 0.2142, 0.2165, 0.214, 0.2165, 0.2152, 0.2163, 0.2159, 0.2159, 0.217, 0.2161, 0.2171, 0.2163, 0.2166, 0.2172, 0.217, 0.2152], 'loss': [5.075758567352295, 4.593401223602295, 4.22422464630127, 3.938196146850586, 3.7007226013183594, 3.474204048919678, 3.258599016036987, 3.0444135913085937, 2.813059559173584, 2.563416614532471, 2.2854572367858887, 1.9812760704040526, 1.6648841067886353, 1.3447824597549438, 1.0478790085029601, 0.8043436193084716, 0.6296934103488923, 0.5042697248220444, 0.3939908056092262, 0.34061421845674517, 0.27747701643943784, 0.21896229578375817, 0.2104725858402252, 0.1790001479268074, 0.16217333526730537, 0.13805615215986966, 0.1144381666150689, 0.11744352877721191, 0.11191298767745494, 0.09282185139507056, 0.08902576298579573, 0.07729428730338812, 0.0854906813068688, 0.0775593568020314, 0.07788485086429864, 0.06754930951304734, 0.06297754393421114, 0.057836639408320185, 0.052244412696138026, 0.04230088942810893, 0.054999443468339744, 0.04891917718347162, 0.05883176794132218, 0.05346210230588913, 0.04788124699560926, 0.05703344714924693, 0.053303545360658316, 0.03933000901636668, 0.03495459071546793, 0.03190957013054751, 0.03281510962607339, 0.031013732014568522, 0.03732353344648145, 0.029901507779778912, 0.02767801710343454, 0.03093218148441054, 0.034510584481786934, 0.0342073775225319, 0.03490657624313142, 0.04567240046709776, 0.02811583883595653, 0.02235576527444646, 0.029672018453408965, 0.023239133801846765, 0.04402062857851852, 0.060157804626896974, 0.0481388316372782, 0.03477473131387494, 0.026040178213315085, 0.022844937176057137, 0.019224423418205697, 0.013693672906963621, 0.009242413651761016, 0.007231142305743415, 0.004813121825114358, 0.0036891359073645434, 0.0032424002948807902, 0.0031081082195820635, 0.002967538569372191, 0.0028933370022768213, 0.0028659848687649357, 0.002906989223689379, 0.0029404824502598783, 0.0028642631077373517, 0.002844290844791394, 0.002706718689201225, 0.002790732197968173, 0.0027035182431376596, 0.0028271186724249855, 0.002719440587353165, 0.0027388531889130535, 0.0028027556716268007, 0.0027754249192580753, 0.0027367304921698814, 0.0027437501413497376, 0.0026183606273443728, 0.0028582748195528983, 0.002736756528407204, 0.002662797366545419, 0.00270083770802652], 'acc': [0.024, 0.07515, 0.12184, 0.16236, 0.19917, 0.23464, 0.27002, 0.30526, 0.34536, 0.39249, 0.4441, 0.50186, 0.56869, 0.63808, 0.70923, 0.76869, 0.8144, 0.85015, 0.88213, 0.8974, 0.91616, 0.93316, 0.936, 0.94619, 0.95123, 0.95757, 0.96532, 0.96518, 0.96617, 0.97284, 0.97364, 0.97645, 0.97458, 0.97697, 0.97695, 0.97976, 0.98144, 0.98344, 0.98487, 0.98792, 0.98397, 0.98567, 0.98234, 0.98451, 0.98594, 0.98378, 0.98392, 0.98867, 0.9903, 0.99059, 0.99063, 0.99144, 0.98959, 0.99163, 0.99191, 0.99107, 0.99043, 0.99034, 0.99033, 0.9868, 0.9923, 0.99379, 0.99127, 0.99341, 0.9873, 0.98326, 0.98645, 0.99002, 0.99257, 0.99395, 0.99459, 0.99627, 0.99795, 0.99882, 0.99939, 0.99968, 0.99974, 0.99976, 0.99978, 0.99978, 0.99978, 0.99977, 0.99976, 0.99977, 0.99977, 0.99979, 0.99977, 0.99979, 0.99976, 0.99977, 0.99977, 0.99975, 0.99976, 0.99976, 0.99976, 0.99978, 0.99975, 0.99976, 0.99977, 0.99975]}
Training time: 
2792.2521352767944
Evaluation results:  [9.375865652465821, 0.2152]
